/****************************************************************************
*   Generated by ACUITY 3.13.0
*   Match ovxlib 1.0.10
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_yolov388.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        _node->uid = (uint32_t)_uid; \
        if( NULL == _node ) {\
            goto error;\
        }\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (174)
#define NET_NORM_TENSOR_NUM     (4)
#define NET_CONST_TENSOR_NUM    (75)
#define NET_VIRTUAL_TENSOR_NUM  (174)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM + 32)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static void load_hw_config
    (
    vsi_nn_context_t ctx
    )
{
    ctx->config.evis.ver = VSI_NN_HW_EVIS_2;
    strncpy(ctx->config.target_name, "VIPNANOQI_PID0X88", VSI_NN_MAX_TARGET_NAME);
}

static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    VSILOGI("Read %d data.", ret);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateYolov388
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx
    )
{
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;



    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
        load_hw_config(ctx);
    }
    else
    {
        ctx = in_ctx;
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 3 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - trans_convolution_1
      var       - node[0]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[416, 416, 3, 1]]
      out_shape - [[416, 416, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_CONV_RELU, 2, 1, 1);
    node[0]->nn_param.conv2d.ksize[0] = 3;
    node[0]->nn_param.conv2d.ksize[1] = 3;
    node[0]->nn_param.conv2d.weights = 32;
    node[0]->nn_param.conv2d.stride[0] = 1;
    node[0]->nn_param.conv2d.stride[1] = 1;
    node[0]->nn_param.conv2d.pad[0] = 1;
    node[0]->nn_param.conv2d.pad[1] = 1;
    node[0]->nn_param.conv2d.pad[2] = 1;
    node[0]->nn_param.conv2d.pad[3] = 1;
    node[0]->nn_param.conv2d.group = 1;
    node[0]->nn_param.conv2d.dilation[0] = 1;
    node[0]->nn_param.conv2d.dilation[1] = 1;
    node[0]->nn_param.conv2d.multiplier = 0;
    node[0]->vx_param.has_relu = FALSE;
    node[0]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[0]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[0]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_3
      var       - node[1]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[416, 416, 32, 1]]
      out_shape - [[416, 416, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_LEAKY_RELU, 1, 1, 3);
    node[1]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_4
      var       - node[2]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[416, 416, 32, 1]]
      out_shape - [[208, 208, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_CONV_RELU, 2, 1, 4);
    node[2]->nn_param.conv2d.ksize[0] = 3;
    node[2]->nn_param.conv2d.ksize[1] = 3;
    node[2]->nn_param.conv2d.weights = 64;
    node[2]->nn_param.conv2d.stride[0] = 2;
    node[2]->nn_param.conv2d.stride[1] = 2;
    node[2]->nn_param.conv2d.pad[0] = 1;
    node[2]->nn_param.conv2d.pad[1] = 1;
    node[2]->nn_param.conv2d.pad[2] = 1;
    node[2]->nn_param.conv2d.pad[3] = 1;
    node[2]->nn_param.conv2d.group = 1;
    node[2]->nn_param.conv2d.dilation[0] = 1;
    node[2]->nn_param.conv2d.dilation[1] = 1;
    node[2]->nn_param.conv2d.multiplier = 0;
    node[2]->vx_param.has_relu = FALSE;
    node[2]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[2]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[2]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_6
      var       - node[3]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[208, 208, 64, 1]]
      out_shape - [[208, 208, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_LEAKY_RELU, 1, 1, 6);
    node[3]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_7
      var       - node[4]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[208, 208, 64, 1]]
      out_shape - [[208, 208, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV_RELU, 2, 1, 7);
    node[4]->nn_param.conv2d.ksize[0] = 1;
    node[4]->nn_param.conv2d.ksize[1] = 1;
    node[4]->nn_param.conv2d.weights = 32;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 0;
    node[4]->nn_param.conv2d.pad[1] = 0;
    node[4]->nn_param.conv2d.pad[2] = 0;
    node[4]->nn_param.conv2d.pad[3] = 0;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->nn_param.conv2d.multiplier = 0;
    node[4]->vx_param.has_relu = FALSE;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_9
      var       - node[5]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[208, 208, 32, 1]]
      out_shape - [[208, 208, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_LEAKY_RELU, 1, 1, 9);
    node[5]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_10
      var       - node[6]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[208, 208, 32, 1]]
      out_shape - [[208, 208, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_CONV_RELU, 2, 1, 10);
    node[6]->nn_param.conv2d.ksize[0] = 3;
    node[6]->nn_param.conv2d.ksize[1] = 3;
    node[6]->nn_param.conv2d.weights = 64;
    node[6]->nn_param.conv2d.stride[0] = 1;
    node[6]->nn_param.conv2d.stride[1] = 1;
    node[6]->nn_param.conv2d.pad[0] = 1;
    node[6]->nn_param.conv2d.pad[1] = 1;
    node[6]->nn_param.conv2d.pad[2] = 1;
    node[6]->nn_param.conv2d.pad[3] = 1;
    node[6]->nn_param.conv2d.group = 1;
    node[6]->nn_param.conv2d.dilation[0] = 1;
    node[6]->nn_param.conv2d.dilation[1] = 1;
    node[6]->nn_param.conv2d.multiplier = 0;
    node[6]->vx_param.has_relu = FALSE;
    node[6]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[6]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_12
      var       - node[7]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[208, 208, 64, 1]]
      out_shape - [[208, 208, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_LEAKY_RELU, 1, 1, 12);
    node[7]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_13
      var       - node[8]
      name      - add
      operation - add
      in_shape  - [[208, 208, 64, 1]]
                  [[208, 208, 64, 1]]
      out_shape - [[208, 208, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_ADD, 2, 1, 13);

    /*-----------------------------------------
      lid       - trans_convolution_14
      var       - node[9]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[208, 208, 64, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_CONV_RELU, 2, 1, 14);
    node[9]->nn_param.conv2d.ksize[0] = 3;
    node[9]->nn_param.conv2d.ksize[1] = 3;
    node[9]->nn_param.conv2d.weights = 128;
    node[9]->nn_param.conv2d.stride[0] = 2;
    node[9]->nn_param.conv2d.stride[1] = 2;
    node[9]->nn_param.conv2d.pad[0] = 1;
    node[9]->nn_param.conv2d.pad[1] = 1;
    node[9]->nn_param.conv2d.pad[2] = 1;
    node[9]->nn_param.conv2d.pad[3] = 1;
    node[9]->nn_param.conv2d.group = 1;
    node[9]->nn_param.conv2d.dilation[0] = 1;
    node[9]->nn_param.conv2d.dilation[1] = 1;
    node[9]->nn_param.conv2d.multiplier = 0;
    node[9]->vx_param.has_relu = FALSE;
    node[9]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[9]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[9]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_16
      var       - node[10]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_LEAKY_RELU, 1, 1, 16);
    node[10]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_17
      var       - node[11]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_CONV_RELU, 2, 1, 17);
    node[11]->nn_param.conv2d.ksize[0] = 1;
    node[11]->nn_param.conv2d.ksize[1] = 1;
    node[11]->nn_param.conv2d.weights = 64;
    node[11]->nn_param.conv2d.stride[0] = 1;
    node[11]->nn_param.conv2d.stride[1] = 1;
    node[11]->nn_param.conv2d.pad[0] = 0;
    node[11]->nn_param.conv2d.pad[1] = 0;
    node[11]->nn_param.conv2d.pad[2] = 0;
    node[11]->nn_param.conv2d.pad[3] = 0;
    node[11]->nn_param.conv2d.group = 1;
    node[11]->nn_param.conv2d.dilation[0] = 1;
    node[11]->nn_param.conv2d.dilation[1] = 1;
    node[11]->nn_param.conv2d.multiplier = 0;
    node[11]->vx_param.has_relu = FALSE;
    node[11]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[11]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[11]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_19
      var       - node[12]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_LEAKY_RELU, 1, 1, 19);
    node[12]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_20
      var       - node[13]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_CONV_RELU, 2, 1, 20);
    node[13]->nn_param.conv2d.ksize[0] = 3;
    node[13]->nn_param.conv2d.ksize[1] = 3;
    node[13]->nn_param.conv2d.weights = 128;
    node[13]->nn_param.conv2d.stride[0] = 1;
    node[13]->nn_param.conv2d.stride[1] = 1;
    node[13]->nn_param.conv2d.pad[0] = 1;
    node[13]->nn_param.conv2d.pad[1] = 1;
    node[13]->nn_param.conv2d.pad[2] = 1;
    node[13]->nn_param.conv2d.pad[3] = 1;
    node[13]->nn_param.conv2d.group = 1;
    node[13]->nn_param.conv2d.dilation[0] = 1;
    node[13]->nn_param.conv2d.dilation[1] = 1;
    node[13]->nn_param.conv2d.multiplier = 0;
    node[13]->vx_param.has_relu = FALSE;
    node[13]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[13]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[13]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_22
      var       - node[14]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_LEAKY_RELU, 1, 1, 22);
    node[14]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_23
      var       - node[15]
      name      - add
      operation - add
      in_shape  - [[104, 104, 128, 1]]
                  [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_ADD, 2, 1, 23);

    /*-----------------------------------------
      lid       - trans_convolution_24
      var       - node[16]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_CONV_RELU, 2, 1, 24);
    node[16]->nn_param.conv2d.ksize[0] = 1;
    node[16]->nn_param.conv2d.ksize[1] = 1;
    node[16]->nn_param.conv2d.weights = 64;
    node[16]->nn_param.conv2d.stride[0] = 1;
    node[16]->nn_param.conv2d.stride[1] = 1;
    node[16]->nn_param.conv2d.pad[0] = 0;
    node[16]->nn_param.conv2d.pad[1] = 0;
    node[16]->nn_param.conv2d.pad[2] = 0;
    node[16]->nn_param.conv2d.pad[3] = 0;
    node[16]->nn_param.conv2d.group = 1;
    node[16]->nn_param.conv2d.dilation[0] = 1;
    node[16]->nn_param.conv2d.dilation[1] = 1;
    node[16]->nn_param.conv2d.multiplier = 0;
    node[16]->vx_param.has_relu = FALSE;
    node[16]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[16]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[16]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_26
      var       - node[17]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_LEAKY_RELU, 1, 1, 26);
    node[17]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_27
      var       - node[18]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_CONV_RELU, 2, 1, 27);
    node[18]->nn_param.conv2d.ksize[0] = 3;
    node[18]->nn_param.conv2d.ksize[1] = 3;
    node[18]->nn_param.conv2d.weights = 128;
    node[18]->nn_param.conv2d.stride[0] = 1;
    node[18]->nn_param.conv2d.stride[1] = 1;
    node[18]->nn_param.conv2d.pad[0] = 1;
    node[18]->nn_param.conv2d.pad[1] = 1;
    node[18]->nn_param.conv2d.pad[2] = 1;
    node[18]->nn_param.conv2d.pad[3] = 1;
    node[18]->nn_param.conv2d.group = 1;
    node[18]->nn_param.conv2d.dilation[0] = 1;
    node[18]->nn_param.conv2d.dilation[1] = 1;
    node[18]->nn_param.conv2d.multiplier = 0;
    node[18]->vx_param.has_relu = FALSE;
    node[18]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[18]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[18]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_29
      var       - node[19]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_LEAKY_RELU, 1, 1, 29);
    node[19]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_30
      var       - node[20]
      name      - add
      operation - add
      in_shape  - [[104, 104, 128, 1]]
                  [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_ADD, 2, 1, 30);

    /*-----------------------------------------
      lid       - trans_convolution_31
      var       - node[21]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_CONV_RELU, 2, 1, 31);
    node[21]->nn_param.conv2d.ksize[0] = 3;
    node[21]->nn_param.conv2d.ksize[1] = 3;
    node[21]->nn_param.conv2d.weights = 256;
    node[21]->nn_param.conv2d.stride[0] = 2;
    node[21]->nn_param.conv2d.stride[1] = 2;
    node[21]->nn_param.conv2d.pad[0] = 1;
    node[21]->nn_param.conv2d.pad[1] = 1;
    node[21]->nn_param.conv2d.pad[2] = 1;
    node[21]->nn_param.conv2d.pad[3] = 1;
    node[21]->nn_param.conv2d.group = 1;
    node[21]->nn_param.conv2d.dilation[0] = 1;
    node[21]->nn_param.conv2d.dilation[1] = 1;
    node[21]->nn_param.conv2d.multiplier = 0;
    node[21]->vx_param.has_relu = FALSE;
    node[21]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[21]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[21]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_33
      var       - node[22]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[22], VSI_NN_OP_LEAKY_RELU, 1, 1, 33);
    node[22]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_34
      var       - node[23]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[23], VSI_NN_OP_CONV_RELU, 2, 1, 34);
    node[23]->nn_param.conv2d.ksize[0] = 1;
    node[23]->nn_param.conv2d.ksize[1] = 1;
    node[23]->nn_param.conv2d.weights = 128;
    node[23]->nn_param.conv2d.stride[0] = 1;
    node[23]->nn_param.conv2d.stride[1] = 1;
    node[23]->nn_param.conv2d.pad[0] = 0;
    node[23]->nn_param.conv2d.pad[1] = 0;
    node[23]->nn_param.conv2d.pad[2] = 0;
    node[23]->nn_param.conv2d.pad[3] = 0;
    node[23]->nn_param.conv2d.group = 1;
    node[23]->nn_param.conv2d.dilation[0] = 1;
    node[23]->nn_param.conv2d.dilation[1] = 1;
    node[23]->nn_param.conv2d.multiplier = 0;
    node[23]->vx_param.has_relu = FALSE;
    node[23]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[23]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[23]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_36
      var       - node[24]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[24], VSI_NN_OP_LEAKY_RELU, 1, 1, 36);
    node[24]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_37
      var       - node[25]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[25], VSI_NN_OP_CONV_RELU, 2, 1, 37);
    node[25]->nn_param.conv2d.ksize[0] = 3;
    node[25]->nn_param.conv2d.ksize[1] = 3;
    node[25]->nn_param.conv2d.weights = 256;
    node[25]->nn_param.conv2d.stride[0] = 1;
    node[25]->nn_param.conv2d.stride[1] = 1;
    node[25]->nn_param.conv2d.pad[0] = 1;
    node[25]->nn_param.conv2d.pad[1] = 1;
    node[25]->nn_param.conv2d.pad[2] = 1;
    node[25]->nn_param.conv2d.pad[3] = 1;
    node[25]->nn_param.conv2d.group = 1;
    node[25]->nn_param.conv2d.dilation[0] = 1;
    node[25]->nn_param.conv2d.dilation[1] = 1;
    node[25]->nn_param.conv2d.multiplier = 0;
    node[25]->vx_param.has_relu = FALSE;
    node[25]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[25]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[25]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_39
      var       - node[26]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[26], VSI_NN_OP_LEAKY_RELU, 1, 1, 39);
    node[26]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_40
      var       - node[27]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[27], VSI_NN_OP_ADD, 2, 1, 40);

    /*-----------------------------------------
      lid       - trans_convolution_41
      var       - node[28]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[28], VSI_NN_OP_CONV_RELU, 2, 1, 41);
    node[28]->nn_param.conv2d.ksize[0] = 1;
    node[28]->nn_param.conv2d.ksize[1] = 1;
    node[28]->nn_param.conv2d.weights = 128;
    node[28]->nn_param.conv2d.stride[0] = 1;
    node[28]->nn_param.conv2d.stride[1] = 1;
    node[28]->nn_param.conv2d.pad[0] = 0;
    node[28]->nn_param.conv2d.pad[1] = 0;
    node[28]->nn_param.conv2d.pad[2] = 0;
    node[28]->nn_param.conv2d.pad[3] = 0;
    node[28]->nn_param.conv2d.group = 1;
    node[28]->nn_param.conv2d.dilation[0] = 1;
    node[28]->nn_param.conv2d.dilation[1] = 1;
    node[28]->nn_param.conv2d.multiplier = 0;
    node[28]->vx_param.has_relu = FALSE;
    node[28]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[28]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[28]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_43
      var       - node[29]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[29], VSI_NN_OP_LEAKY_RELU, 1, 1, 43);
    node[29]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_44
      var       - node[30]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[30], VSI_NN_OP_CONV_RELU, 2, 1, 44);
    node[30]->nn_param.conv2d.ksize[0] = 3;
    node[30]->nn_param.conv2d.ksize[1] = 3;
    node[30]->nn_param.conv2d.weights = 256;
    node[30]->nn_param.conv2d.stride[0] = 1;
    node[30]->nn_param.conv2d.stride[1] = 1;
    node[30]->nn_param.conv2d.pad[0] = 1;
    node[30]->nn_param.conv2d.pad[1] = 1;
    node[30]->nn_param.conv2d.pad[2] = 1;
    node[30]->nn_param.conv2d.pad[3] = 1;
    node[30]->nn_param.conv2d.group = 1;
    node[30]->nn_param.conv2d.dilation[0] = 1;
    node[30]->nn_param.conv2d.dilation[1] = 1;
    node[30]->nn_param.conv2d.multiplier = 0;
    node[30]->vx_param.has_relu = FALSE;
    node[30]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[30]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[30]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_46
      var       - node[31]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[31], VSI_NN_OP_LEAKY_RELU, 1, 1, 46);
    node[31]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_47
      var       - node[32]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[32], VSI_NN_OP_ADD, 2, 1, 47);

    /*-----------------------------------------
      lid       - trans_convolution_48
      var       - node[33]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[33], VSI_NN_OP_CONV_RELU, 2, 1, 48);
    node[33]->nn_param.conv2d.ksize[0] = 1;
    node[33]->nn_param.conv2d.ksize[1] = 1;
    node[33]->nn_param.conv2d.weights = 128;
    node[33]->nn_param.conv2d.stride[0] = 1;
    node[33]->nn_param.conv2d.stride[1] = 1;
    node[33]->nn_param.conv2d.pad[0] = 0;
    node[33]->nn_param.conv2d.pad[1] = 0;
    node[33]->nn_param.conv2d.pad[2] = 0;
    node[33]->nn_param.conv2d.pad[3] = 0;
    node[33]->nn_param.conv2d.group = 1;
    node[33]->nn_param.conv2d.dilation[0] = 1;
    node[33]->nn_param.conv2d.dilation[1] = 1;
    node[33]->nn_param.conv2d.multiplier = 0;
    node[33]->vx_param.has_relu = FALSE;
    node[33]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[33]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[33]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_50
      var       - node[34]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[34], VSI_NN_OP_LEAKY_RELU, 1, 1, 50);
    node[34]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_51
      var       - node[35]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[35], VSI_NN_OP_CONV_RELU, 2, 1, 51);
    node[35]->nn_param.conv2d.ksize[0] = 3;
    node[35]->nn_param.conv2d.ksize[1] = 3;
    node[35]->nn_param.conv2d.weights = 256;
    node[35]->nn_param.conv2d.stride[0] = 1;
    node[35]->nn_param.conv2d.stride[1] = 1;
    node[35]->nn_param.conv2d.pad[0] = 1;
    node[35]->nn_param.conv2d.pad[1] = 1;
    node[35]->nn_param.conv2d.pad[2] = 1;
    node[35]->nn_param.conv2d.pad[3] = 1;
    node[35]->nn_param.conv2d.group = 1;
    node[35]->nn_param.conv2d.dilation[0] = 1;
    node[35]->nn_param.conv2d.dilation[1] = 1;
    node[35]->nn_param.conv2d.multiplier = 0;
    node[35]->vx_param.has_relu = FALSE;
    node[35]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[35]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[35]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_53
      var       - node[36]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[36], VSI_NN_OP_LEAKY_RELU, 1, 1, 53);
    node[36]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_54
      var       - node[37]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[37], VSI_NN_OP_ADD, 2, 1, 54);

    /*-----------------------------------------
      lid       - trans_convolution_55
      var       - node[38]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[38], VSI_NN_OP_CONV_RELU, 2, 1, 55);
    node[38]->nn_param.conv2d.ksize[0] = 1;
    node[38]->nn_param.conv2d.ksize[1] = 1;
    node[38]->nn_param.conv2d.weights = 128;
    node[38]->nn_param.conv2d.stride[0] = 1;
    node[38]->nn_param.conv2d.stride[1] = 1;
    node[38]->nn_param.conv2d.pad[0] = 0;
    node[38]->nn_param.conv2d.pad[1] = 0;
    node[38]->nn_param.conv2d.pad[2] = 0;
    node[38]->nn_param.conv2d.pad[3] = 0;
    node[38]->nn_param.conv2d.group = 1;
    node[38]->nn_param.conv2d.dilation[0] = 1;
    node[38]->nn_param.conv2d.dilation[1] = 1;
    node[38]->nn_param.conv2d.multiplier = 0;
    node[38]->vx_param.has_relu = FALSE;
    node[38]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[38]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[38]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_57
      var       - node[39]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[39], VSI_NN_OP_LEAKY_RELU, 1, 1, 57);
    node[39]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_58
      var       - node[40]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[40], VSI_NN_OP_CONV_RELU, 2, 1, 58);
    node[40]->nn_param.conv2d.ksize[0] = 3;
    node[40]->nn_param.conv2d.ksize[1] = 3;
    node[40]->nn_param.conv2d.weights = 256;
    node[40]->nn_param.conv2d.stride[0] = 1;
    node[40]->nn_param.conv2d.stride[1] = 1;
    node[40]->nn_param.conv2d.pad[0] = 1;
    node[40]->nn_param.conv2d.pad[1] = 1;
    node[40]->nn_param.conv2d.pad[2] = 1;
    node[40]->nn_param.conv2d.pad[3] = 1;
    node[40]->nn_param.conv2d.group = 1;
    node[40]->nn_param.conv2d.dilation[0] = 1;
    node[40]->nn_param.conv2d.dilation[1] = 1;
    node[40]->nn_param.conv2d.multiplier = 0;
    node[40]->vx_param.has_relu = FALSE;
    node[40]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[40]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[40]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_60
      var       - node[41]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[41], VSI_NN_OP_LEAKY_RELU, 1, 1, 60);
    node[41]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_61
      var       - node[42]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[42], VSI_NN_OP_ADD, 2, 1, 61);

    /*-----------------------------------------
      lid       - trans_convolution_62
      var       - node[43]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[43], VSI_NN_OP_CONV_RELU, 2, 1, 62);
    node[43]->nn_param.conv2d.ksize[0] = 1;
    node[43]->nn_param.conv2d.ksize[1] = 1;
    node[43]->nn_param.conv2d.weights = 128;
    node[43]->nn_param.conv2d.stride[0] = 1;
    node[43]->nn_param.conv2d.stride[1] = 1;
    node[43]->nn_param.conv2d.pad[0] = 0;
    node[43]->nn_param.conv2d.pad[1] = 0;
    node[43]->nn_param.conv2d.pad[2] = 0;
    node[43]->nn_param.conv2d.pad[3] = 0;
    node[43]->nn_param.conv2d.group = 1;
    node[43]->nn_param.conv2d.dilation[0] = 1;
    node[43]->nn_param.conv2d.dilation[1] = 1;
    node[43]->nn_param.conv2d.multiplier = 0;
    node[43]->vx_param.has_relu = FALSE;
    node[43]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[43]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[43]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_64
      var       - node[44]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[44], VSI_NN_OP_LEAKY_RELU, 1, 1, 64);
    node[44]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_65
      var       - node[45]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[45], VSI_NN_OP_CONV_RELU, 2, 1, 65);
    node[45]->nn_param.conv2d.ksize[0] = 3;
    node[45]->nn_param.conv2d.ksize[1] = 3;
    node[45]->nn_param.conv2d.weights = 256;
    node[45]->nn_param.conv2d.stride[0] = 1;
    node[45]->nn_param.conv2d.stride[1] = 1;
    node[45]->nn_param.conv2d.pad[0] = 1;
    node[45]->nn_param.conv2d.pad[1] = 1;
    node[45]->nn_param.conv2d.pad[2] = 1;
    node[45]->nn_param.conv2d.pad[3] = 1;
    node[45]->nn_param.conv2d.group = 1;
    node[45]->nn_param.conv2d.dilation[0] = 1;
    node[45]->nn_param.conv2d.dilation[1] = 1;
    node[45]->nn_param.conv2d.multiplier = 0;
    node[45]->vx_param.has_relu = FALSE;
    node[45]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[45]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[45]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_67
      var       - node[46]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[46], VSI_NN_OP_LEAKY_RELU, 1, 1, 67);
    node[46]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_68
      var       - node[47]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[47], VSI_NN_OP_ADD, 2, 1, 68);

    /*-----------------------------------------
      lid       - trans_convolution_69
      var       - node[48]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[48], VSI_NN_OP_CONV_RELU, 2, 1, 69);
    node[48]->nn_param.conv2d.ksize[0] = 1;
    node[48]->nn_param.conv2d.ksize[1] = 1;
    node[48]->nn_param.conv2d.weights = 128;
    node[48]->nn_param.conv2d.stride[0] = 1;
    node[48]->nn_param.conv2d.stride[1] = 1;
    node[48]->nn_param.conv2d.pad[0] = 0;
    node[48]->nn_param.conv2d.pad[1] = 0;
    node[48]->nn_param.conv2d.pad[2] = 0;
    node[48]->nn_param.conv2d.pad[3] = 0;
    node[48]->nn_param.conv2d.group = 1;
    node[48]->nn_param.conv2d.dilation[0] = 1;
    node[48]->nn_param.conv2d.dilation[1] = 1;
    node[48]->nn_param.conv2d.multiplier = 0;
    node[48]->vx_param.has_relu = FALSE;
    node[48]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[48]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[48]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_71
      var       - node[49]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[49], VSI_NN_OP_LEAKY_RELU, 1, 1, 71);
    node[49]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_72
      var       - node[50]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[50], VSI_NN_OP_CONV_RELU, 2, 1, 72);
    node[50]->nn_param.conv2d.ksize[0] = 3;
    node[50]->nn_param.conv2d.ksize[1] = 3;
    node[50]->nn_param.conv2d.weights = 256;
    node[50]->nn_param.conv2d.stride[0] = 1;
    node[50]->nn_param.conv2d.stride[1] = 1;
    node[50]->nn_param.conv2d.pad[0] = 1;
    node[50]->nn_param.conv2d.pad[1] = 1;
    node[50]->nn_param.conv2d.pad[2] = 1;
    node[50]->nn_param.conv2d.pad[3] = 1;
    node[50]->nn_param.conv2d.group = 1;
    node[50]->nn_param.conv2d.dilation[0] = 1;
    node[50]->nn_param.conv2d.dilation[1] = 1;
    node[50]->nn_param.conv2d.multiplier = 0;
    node[50]->vx_param.has_relu = FALSE;
    node[50]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[50]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[50]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_74
      var       - node[51]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[51], VSI_NN_OP_LEAKY_RELU, 1, 1, 74);
    node[51]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_75
      var       - node[52]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[52], VSI_NN_OP_ADD, 2, 1, 75);

    /*-----------------------------------------
      lid       - trans_convolution_76
      var       - node[53]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[53], VSI_NN_OP_CONV_RELU, 2, 1, 76);
    node[53]->nn_param.conv2d.ksize[0] = 1;
    node[53]->nn_param.conv2d.ksize[1] = 1;
    node[53]->nn_param.conv2d.weights = 128;
    node[53]->nn_param.conv2d.stride[0] = 1;
    node[53]->nn_param.conv2d.stride[1] = 1;
    node[53]->nn_param.conv2d.pad[0] = 0;
    node[53]->nn_param.conv2d.pad[1] = 0;
    node[53]->nn_param.conv2d.pad[2] = 0;
    node[53]->nn_param.conv2d.pad[3] = 0;
    node[53]->nn_param.conv2d.group = 1;
    node[53]->nn_param.conv2d.dilation[0] = 1;
    node[53]->nn_param.conv2d.dilation[1] = 1;
    node[53]->nn_param.conv2d.multiplier = 0;
    node[53]->vx_param.has_relu = FALSE;
    node[53]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[53]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[53]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_78
      var       - node[54]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[54], VSI_NN_OP_LEAKY_RELU, 1, 1, 78);
    node[54]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_79
      var       - node[55]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[55], VSI_NN_OP_CONV_RELU, 2, 1, 79);
    node[55]->nn_param.conv2d.ksize[0] = 3;
    node[55]->nn_param.conv2d.ksize[1] = 3;
    node[55]->nn_param.conv2d.weights = 256;
    node[55]->nn_param.conv2d.stride[0] = 1;
    node[55]->nn_param.conv2d.stride[1] = 1;
    node[55]->nn_param.conv2d.pad[0] = 1;
    node[55]->nn_param.conv2d.pad[1] = 1;
    node[55]->nn_param.conv2d.pad[2] = 1;
    node[55]->nn_param.conv2d.pad[3] = 1;
    node[55]->nn_param.conv2d.group = 1;
    node[55]->nn_param.conv2d.dilation[0] = 1;
    node[55]->nn_param.conv2d.dilation[1] = 1;
    node[55]->nn_param.conv2d.multiplier = 0;
    node[55]->vx_param.has_relu = FALSE;
    node[55]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[55]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[55]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_81
      var       - node[56]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[56], VSI_NN_OP_LEAKY_RELU, 1, 1, 81);
    node[56]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_82
      var       - node[57]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[57], VSI_NN_OP_ADD, 2, 1, 82);

    /*-----------------------------------------
      lid       - trans_convolution_83
      var       - node[58]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[58], VSI_NN_OP_CONV_RELU, 2, 1, 83);
    node[58]->nn_param.conv2d.ksize[0] = 1;
    node[58]->nn_param.conv2d.ksize[1] = 1;
    node[58]->nn_param.conv2d.weights = 128;
    node[58]->nn_param.conv2d.stride[0] = 1;
    node[58]->nn_param.conv2d.stride[1] = 1;
    node[58]->nn_param.conv2d.pad[0] = 0;
    node[58]->nn_param.conv2d.pad[1] = 0;
    node[58]->nn_param.conv2d.pad[2] = 0;
    node[58]->nn_param.conv2d.pad[3] = 0;
    node[58]->nn_param.conv2d.group = 1;
    node[58]->nn_param.conv2d.dilation[0] = 1;
    node[58]->nn_param.conv2d.dilation[1] = 1;
    node[58]->nn_param.conv2d.multiplier = 0;
    node[58]->vx_param.has_relu = FALSE;
    node[58]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[58]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[58]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_85
      var       - node[59]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[59], VSI_NN_OP_LEAKY_RELU, 1, 1, 85);
    node[59]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_86
      var       - node[60]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[60], VSI_NN_OP_CONV_RELU, 2, 1, 86);
    node[60]->nn_param.conv2d.ksize[0] = 3;
    node[60]->nn_param.conv2d.ksize[1] = 3;
    node[60]->nn_param.conv2d.weights = 256;
    node[60]->nn_param.conv2d.stride[0] = 1;
    node[60]->nn_param.conv2d.stride[1] = 1;
    node[60]->nn_param.conv2d.pad[0] = 1;
    node[60]->nn_param.conv2d.pad[1] = 1;
    node[60]->nn_param.conv2d.pad[2] = 1;
    node[60]->nn_param.conv2d.pad[3] = 1;
    node[60]->nn_param.conv2d.group = 1;
    node[60]->nn_param.conv2d.dilation[0] = 1;
    node[60]->nn_param.conv2d.dilation[1] = 1;
    node[60]->nn_param.conv2d.multiplier = 0;
    node[60]->vx_param.has_relu = FALSE;
    node[60]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[60]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[60]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_88
      var       - node[61]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[61], VSI_NN_OP_LEAKY_RELU, 1, 1, 88);
    node[61]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_89
      var       - node[62]
      name      - add
      operation - add
      in_shape  - [[52, 52, 256, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[62], VSI_NN_OP_ADD, 2, 1, 89);

    /*-----------------------------------------
      lid       - trans_convolution_90
      var       - node[63]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[63], VSI_NN_OP_CONV_RELU, 2, 1, 90);
    node[63]->nn_param.conv2d.ksize[0] = 3;
    node[63]->nn_param.conv2d.ksize[1] = 3;
    node[63]->nn_param.conv2d.weights = 512;
    node[63]->nn_param.conv2d.stride[0] = 2;
    node[63]->nn_param.conv2d.stride[1] = 2;
    node[63]->nn_param.conv2d.pad[0] = 1;
    node[63]->nn_param.conv2d.pad[1] = 1;
    node[63]->nn_param.conv2d.pad[2] = 1;
    node[63]->nn_param.conv2d.pad[3] = 1;
    node[63]->nn_param.conv2d.group = 1;
    node[63]->nn_param.conv2d.dilation[0] = 1;
    node[63]->nn_param.conv2d.dilation[1] = 1;
    node[63]->nn_param.conv2d.multiplier = 0;
    node[63]->vx_param.has_relu = FALSE;
    node[63]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[63]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[63]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_92
      var       - node[64]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[64], VSI_NN_OP_LEAKY_RELU, 1, 1, 92);
    node[64]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_93
      var       - node[65]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[65], VSI_NN_OP_CONV_RELU, 2, 1, 93);
    node[65]->nn_param.conv2d.ksize[0] = 1;
    node[65]->nn_param.conv2d.ksize[1] = 1;
    node[65]->nn_param.conv2d.weights = 256;
    node[65]->nn_param.conv2d.stride[0] = 1;
    node[65]->nn_param.conv2d.stride[1] = 1;
    node[65]->nn_param.conv2d.pad[0] = 0;
    node[65]->nn_param.conv2d.pad[1] = 0;
    node[65]->nn_param.conv2d.pad[2] = 0;
    node[65]->nn_param.conv2d.pad[3] = 0;
    node[65]->nn_param.conv2d.group = 1;
    node[65]->nn_param.conv2d.dilation[0] = 1;
    node[65]->nn_param.conv2d.dilation[1] = 1;
    node[65]->nn_param.conv2d.multiplier = 0;
    node[65]->vx_param.has_relu = FALSE;
    node[65]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[65]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[65]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_95
      var       - node[66]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[66], VSI_NN_OP_LEAKY_RELU, 1, 1, 95);
    node[66]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_96
      var       - node[67]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[67], VSI_NN_OP_CONV_RELU, 2, 1, 96);
    node[67]->nn_param.conv2d.ksize[0] = 3;
    node[67]->nn_param.conv2d.ksize[1] = 3;
    node[67]->nn_param.conv2d.weights = 512;
    node[67]->nn_param.conv2d.stride[0] = 1;
    node[67]->nn_param.conv2d.stride[1] = 1;
    node[67]->nn_param.conv2d.pad[0] = 1;
    node[67]->nn_param.conv2d.pad[1] = 1;
    node[67]->nn_param.conv2d.pad[2] = 1;
    node[67]->nn_param.conv2d.pad[3] = 1;
    node[67]->nn_param.conv2d.group = 1;
    node[67]->nn_param.conv2d.dilation[0] = 1;
    node[67]->nn_param.conv2d.dilation[1] = 1;
    node[67]->nn_param.conv2d.multiplier = 0;
    node[67]->vx_param.has_relu = FALSE;
    node[67]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[67]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[67]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_98
      var       - node[68]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[68], VSI_NN_OP_LEAKY_RELU, 1, 1, 98);
    node[68]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_99
      var       - node[69]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[69], VSI_NN_OP_ADD, 2, 1, 99);

    /*-----------------------------------------
      lid       - trans_convolution_100
      var       - node[70]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[70], VSI_NN_OP_CONV_RELU, 2, 1, 100);
    node[70]->nn_param.conv2d.ksize[0] = 1;
    node[70]->nn_param.conv2d.ksize[1] = 1;
    node[70]->nn_param.conv2d.weights = 256;
    node[70]->nn_param.conv2d.stride[0] = 1;
    node[70]->nn_param.conv2d.stride[1] = 1;
    node[70]->nn_param.conv2d.pad[0] = 0;
    node[70]->nn_param.conv2d.pad[1] = 0;
    node[70]->nn_param.conv2d.pad[2] = 0;
    node[70]->nn_param.conv2d.pad[3] = 0;
    node[70]->nn_param.conv2d.group = 1;
    node[70]->nn_param.conv2d.dilation[0] = 1;
    node[70]->nn_param.conv2d.dilation[1] = 1;
    node[70]->nn_param.conv2d.multiplier = 0;
    node[70]->vx_param.has_relu = FALSE;
    node[70]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[70]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[70]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_102
      var       - node[71]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[71], VSI_NN_OP_LEAKY_RELU, 1, 1, 102);
    node[71]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_103
      var       - node[72]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[72], VSI_NN_OP_CONV_RELU, 2, 1, 103);
    node[72]->nn_param.conv2d.ksize[0] = 3;
    node[72]->nn_param.conv2d.ksize[1] = 3;
    node[72]->nn_param.conv2d.weights = 512;
    node[72]->nn_param.conv2d.stride[0] = 1;
    node[72]->nn_param.conv2d.stride[1] = 1;
    node[72]->nn_param.conv2d.pad[0] = 1;
    node[72]->nn_param.conv2d.pad[1] = 1;
    node[72]->nn_param.conv2d.pad[2] = 1;
    node[72]->nn_param.conv2d.pad[3] = 1;
    node[72]->nn_param.conv2d.group = 1;
    node[72]->nn_param.conv2d.dilation[0] = 1;
    node[72]->nn_param.conv2d.dilation[1] = 1;
    node[72]->nn_param.conv2d.multiplier = 0;
    node[72]->vx_param.has_relu = FALSE;
    node[72]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[72]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[72]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_105
      var       - node[73]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[73], VSI_NN_OP_LEAKY_RELU, 1, 1, 105);
    node[73]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_106
      var       - node[74]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[74], VSI_NN_OP_ADD, 2, 1, 106);

    /*-----------------------------------------
      lid       - trans_convolution_107
      var       - node[75]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[75], VSI_NN_OP_CONV_RELU, 2, 1, 107);
    node[75]->nn_param.conv2d.ksize[0] = 1;
    node[75]->nn_param.conv2d.ksize[1] = 1;
    node[75]->nn_param.conv2d.weights = 256;
    node[75]->nn_param.conv2d.stride[0] = 1;
    node[75]->nn_param.conv2d.stride[1] = 1;
    node[75]->nn_param.conv2d.pad[0] = 0;
    node[75]->nn_param.conv2d.pad[1] = 0;
    node[75]->nn_param.conv2d.pad[2] = 0;
    node[75]->nn_param.conv2d.pad[3] = 0;
    node[75]->nn_param.conv2d.group = 1;
    node[75]->nn_param.conv2d.dilation[0] = 1;
    node[75]->nn_param.conv2d.dilation[1] = 1;
    node[75]->nn_param.conv2d.multiplier = 0;
    node[75]->vx_param.has_relu = FALSE;
    node[75]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[75]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[75]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_109
      var       - node[76]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[76], VSI_NN_OP_LEAKY_RELU, 1, 1, 109);
    node[76]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_110
      var       - node[77]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[77], VSI_NN_OP_CONV_RELU, 2, 1, 110);
    node[77]->nn_param.conv2d.ksize[0] = 3;
    node[77]->nn_param.conv2d.ksize[1] = 3;
    node[77]->nn_param.conv2d.weights = 512;
    node[77]->nn_param.conv2d.stride[0] = 1;
    node[77]->nn_param.conv2d.stride[1] = 1;
    node[77]->nn_param.conv2d.pad[0] = 1;
    node[77]->nn_param.conv2d.pad[1] = 1;
    node[77]->nn_param.conv2d.pad[2] = 1;
    node[77]->nn_param.conv2d.pad[3] = 1;
    node[77]->nn_param.conv2d.group = 1;
    node[77]->nn_param.conv2d.dilation[0] = 1;
    node[77]->nn_param.conv2d.dilation[1] = 1;
    node[77]->nn_param.conv2d.multiplier = 0;
    node[77]->vx_param.has_relu = FALSE;
    node[77]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[77]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[77]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_112
      var       - node[78]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[78], VSI_NN_OP_LEAKY_RELU, 1, 1, 112);
    node[78]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_113
      var       - node[79]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[79], VSI_NN_OP_ADD, 2, 1, 113);

    /*-----------------------------------------
      lid       - trans_convolution_114
      var       - node[80]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[80], VSI_NN_OP_CONV_RELU, 2, 1, 114);
    node[80]->nn_param.conv2d.ksize[0] = 1;
    node[80]->nn_param.conv2d.ksize[1] = 1;
    node[80]->nn_param.conv2d.weights = 256;
    node[80]->nn_param.conv2d.stride[0] = 1;
    node[80]->nn_param.conv2d.stride[1] = 1;
    node[80]->nn_param.conv2d.pad[0] = 0;
    node[80]->nn_param.conv2d.pad[1] = 0;
    node[80]->nn_param.conv2d.pad[2] = 0;
    node[80]->nn_param.conv2d.pad[3] = 0;
    node[80]->nn_param.conv2d.group = 1;
    node[80]->nn_param.conv2d.dilation[0] = 1;
    node[80]->nn_param.conv2d.dilation[1] = 1;
    node[80]->nn_param.conv2d.multiplier = 0;
    node[80]->vx_param.has_relu = FALSE;
    node[80]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[80]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[80]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_116
      var       - node[81]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[81], VSI_NN_OP_LEAKY_RELU, 1, 1, 116);
    node[81]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_117
      var       - node[82]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[82], VSI_NN_OP_CONV_RELU, 2, 1, 117);
    node[82]->nn_param.conv2d.ksize[0] = 3;
    node[82]->nn_param.conv2d.ksize[1] = 3;
    node[82]->nn_param.conv2d.weights = 512;
    node[82]->nn_param.conv2d.stride[0] = 1;
    node[82]->nn_param.conv2d.stride[1] = 1;
    node[82]->nn_param.conv2d.pad[0] = 1;
    node[82]->nn_param.conv2d.pad[1] = 1;
    node[82]->nn_param.conv2d.pad[2] = 1;
    node[82]->nn_param.conv2d.pad[3] = 1;
    node[82]->nn_param.conv2d.group = 1;
    node[82]->nn_param.conv2d.dilation[0] = 1;
    node[82]->nn_param.conv2d.dilation[1] = 1;
    node[82]->nn_param.conv2d.multiplier = 0;
    node[82]->vx_param.has_relu = FALSE;
    node[82]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[82]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[82]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_119
      var       - node[83]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[83], VSI_NN_OP_LEAKY_RELU, 1, 1, 119);
    node[83]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_120
      var       - node[84]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[84], VSI_NN_OP_ADD, 2, 1, 120);

    /*-----------------------------------------
      lid       - trans_convolution_121
      var       - node[85]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[85], VSI_NN_OP_CONV_RELU, 2, 1, 121);
    node[85]->nn_param.conv2d.ksize[0] = 1;
    node[85]->nn_param.conv2d.ksize[1] = 1;
    node[85]->nn_param.conv2d.weights = 256;
    node[85]->nn_param.conv2d.stride[0] = 1;
    node[85]->nn_param.conv2d.stride[1] = 1;
    node[85]->nn_param.conv2d.pad[0] = 0;
    node[85]->nn_param.conv2d.pad[1] = 0;
    node[85]->nn_param.conv2d.pad[2] = 0;
    node[85]->nn_param.conv2d.pad[3] = 0;
    node[85]->nn_param.conv2d.group = 1;
    node[85]->nn_param.conv2d.dilation[0] = 1;
    node[85]->nn_param.conv2d.dilation[1] = 1;
    node[85]->nn_param.conv2d.multiplier = 0;
    node[85]->vx_param.has_relu = FALSE;
    node[85]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[85]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[85]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_123
      var       - node[86]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[86], VSI_NN_OP_LEAKY_RELU, 1, 1, 123);
    node[86]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_124
      var       - node[87]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[87], VSI_NN_OP_CONV_RELU, 2, 1, 124);
    node[87]->nn_param.conv2d.ksize[0] = 3;
    node[87]->nn_param.conv2d.ksize[1] = 3;
    node[87]->nn_param.conv2d.weights = 512;
    node[87]->nn_param.conv2d.stride[0] = 1;
    node[87]->nn_param.conv2d.stride[1] = 1;
    node[87]->nn_param.conv2d.pad[0] = 1;
    node[87]->nn_param.conv2d.pad[1] = 1;
    node[87]->nn_param.conv2d.pad[2] = 1;
    node[87]->nn_param.conv2d.pad[3] = 1;
    node[87]->nn_param.conv2d.group = 1;
    node[87]->nn_param.conv2d.dilation[0] = 1;
    node[87]->nn_param.conv2d.dilation[1] = 1;
    node[87]->nn_param.conv2d.multiplier = 0;
    node[87]->vx_param.has_relu = FALSE;
    node[87]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[87]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[87]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_126
      var       - node[88]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[88], VSI_NN_OP_LEAKY_RELU, 1, 1, 126);
    node[88]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_127
      var       - node[89]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[89], VSI_NN_OP_ADD, 2, 1, 127);

    /*-----------------------------------------
      lid       - trans_convolution_128
      var       - node[90]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[90], VSI_NN_OP_CONV_RELU, 2, 1, 128);
    node[90]->nn_param.conv2d.ksize[0] = 1;
    node[90]->nn_param.conv2d.ksize[1] = 1;
    node[90]->nn_param.conv2d.weights = 256;
    node[90]->nn_param.conv2d.stride[0] = 1;
    node[90]->nn_param.conv2d.stride[1] = 1;
    node[90]->nn_param.conv2d.pad[0] = 0;
    node[90]->nn_param.conv2d.pad[1] = 0;
    node[90]->nn_param.conv2d.pad[2] = 0;
    node[90]->nn_param.conv2d.pad[3] = 0;
    node[90]->nn_param.conv2d.group = 1;
    node[90]->nn_param.conv2d.dilation[0] = 1;
    node[90]->nn_param.conv2d.dilation[1] = 1;
    node[90]->nn_param.conv2d.multiplier = 0;
    node[90]->vx_param.has_relu = FALSE;
    node[90]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[90]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[90]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_130
      var       - node[91]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[91], VSI_NN_OP_LEAKY_RELU, 1, 1, 130);
    node[91]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_131
      var       - node[92]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[92], VSI_NN_OP_CONV_RELU, 2, 1, 131);
    node[92]->nn_param.conv2d.ksize[0] = 3;
    node[92]->nn_param.conv2d.ksize[1] = 3;
    node[92]->nn_param.conv2d.weights = 512;
    node[92]->nn_param.conv2d.stride[0] = 1;
    node[92]->nn_param.conv2d.stride[1] = 1;
    node[92]->nn_param.conv2d.pad[0] = 1;
    node[92]->nn_param.conv2d.pad[1] = 1;
    node[92]->nn_param.conv2d.pad[2] = 1;
    node[92]->nn_param.conv2d.pad[3] = 1;
    node[92]->nn_param.conv2d.group = 1;
    node[92]->nn_param.conv2d.dilation[0] = 1;
    node[92]->nn_param.conv2d.dilation[1] = 1;
    node[92]->nn_param.conv2d.multiplier = 0;
    node[92]->vx_param.has_relu = FALSE;
    node[92]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[92]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[92]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_133
      var       - node[93]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[93], VSI_NN_OP_LEAKY_RELU, 1, 1, 133);
    node[93]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_134
      var       - node[94]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[94], VSI_NN_OP_ADD, 2, 1, 134);

    /*-----------------------------------------
      lid       - trans_convolution_135
      var       - node[95]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[95], VSI_NN_OP_CONV_RELU, 2, 1, 135);
    node[95]->nn_param.conv2d.ksize[0] = 1;
    node[95]->nn_param.conv2d.ksize[1] = 1;
    node[95]->nn_param.conv2d.weights = 256;
    node[95]->nn_param.conv2d.stride[0] = 1;
    node[95]->nn_param.conv2d.stride[1] = 1;
    node[95]->nn_param.conv2d.pad[0] = 0;
    node[95]->nn_param.conv2d.pad[1] = 0;
    node[95]->nn_param.conv2d.pad[2] = 0;
    node[95]->nn_param.conv2d.pad[3] = 0;
    node[95]->nn_param.conv2d.group = 1;
    node[95]->nn_param.conv2d.dilation[0] = 1;
    node[95]->nn_param.conv2d.dilation[1] = 1;
    node[95]->nn_param.conv2d.multiplier = 0;
    node[95]->vx_param.has_relu = FALSE;
    node[95]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[95]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[95]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_137
      var       - node[96]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[96], VSI_NN_OP_LEAKY_RELU, 1, 1, 137);
    node[96]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_138
      var       - node[97]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[97], VSI_NN_OP_CONV_RELU, 2, 1, 138);
    node[97]->nn_param.conv2d.ksize[0] = 3;
    node[97]->nn_param.conv2d.ksize[1] = 3;
    node[97]->nn_param.conv2d.weights = 512;
    node[97]->nn_param.conv2d.stride[0] = 1;
    node[97]->nn_param.conv2d.stride[1] = 1;
    node[97]->nn_param.conv2d.pad[0] = 1;
    node[97]->nn_param.conv2d.pad[1] = 1;
    node[97]->nn_param.conv2d.pad[2] = 1;
    node[97]->nn_param.conv2d.pad[3] = 1;
    node[97]->nn_param.conv2d.group = 1;
    node[97]->nn_param.conv2d.dilation[0] = 1;
    node[97]->nn_param.conv2d.dilation[1] = 1;
    node[97]->nn_param.conv2d.multiplier = 0;
    node[97]->vx_param.has_relu = FALSE;
    node[97]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[97]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[97]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_140
      var       - node[98]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[98], VSI_NN_OP_LEAKY_RELU, 1, 1, 140);
    node[98]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_141
      var       - node[99]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[99], VSI_NN_OP_ADD, 2, 1, 141);

    /*-----------------------------------------
      lid       - trans_convolution_142
      var       - node[100]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[100], VSI_NN_OP_CONV_RELU, 2, 1, 142);
    node[100]->nn_param.conv2d.ksize[0] = 1;
    node[100]->nn_param.conv2d.ksize[1] = 1;
    node[100]->nn_param.conv2d.weights = 256;
    node[100]->nn_param.conv2d.stride[0] = 1;
    node[100]->nn_param.conv2d.stride[1] = 1;
    node[100]->nn_param.conv2d.pad[0] = 0;
    node[100]->nn_param.conv2d.pad[1] = 0;
    node[100]->nn_param.conv2d.pad[2] = 0;
    node[100]->nn_param.conv2d.pad[3] = 0;
    node[100]->nn_param.conv2d.group = 1;
    node[100]->nn_param.conv2d.dilation[0] = 1;
    node[100]->nn_param.conv2d.dilation[1] = 1;
    node[100]->nn_param.conv2d.multiplier = 0;
    node[100]->vx_param.has_relu = FALSE;
    node[100]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[100]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[100]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_144
      var       - node[101]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[101], VSI_NN_OP_LEAKY_RELU, 1, 1, 144);
    node[101]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_145
      var       - node[102]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[102], VSI_NN_OP_CONV_RELU, 2, 1, 145);
    node[102]->nn_param.conv2d.ksize[0] = 3;
    node[102]->nn_param.conv2d.ksize[1] = 3;
    node[102]->nn_param.conv2d.weights = 512;
    node[102]->nn_param.conv2d.stride[0] = 1;
    node[102]->nn_param.conv2d.stride[1] = 1;
    node[102]->nn_param.conv2d.pad[0] = 1;
    node[102]->nn_param.conv2d.pad[1] = 1;
    node[102]->nn_param.conv2d.pad[2] = 1;
    node[102]->nn_param.conv2d.pad[3] = 1;
    node[102]->nn_param.conv2d.group = 1;
    node[102]->nn_param.conv2d.dilation[0] = 1;
    node[102]->nn_param.conv2d.dilation[1] = 1;
    node[102]->nn_param.conv2d.multiplier = 0;
    node[102]->vx_param.has_relu = FALSE;
    node[102]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[102]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[102]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_147
      var       - node[103]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[103], VSI_NN_OP_LEAKY_RELU, 1, 1, 147);
    node[103]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_148
      var       - node[104]
      name      - add
      operation - add
      in_shape  - [[26, 26, 512, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[104], VSI_NN_OP_ADD, 2, 1, 148);

    /*-----------------------------------------
      lid       - trans_convolution_149
      var       - node[105]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[105], VSI_NN_OP_CONV_RELU, 2, 1, 149);
    node[105]->nn_param.conv2d.ksize[0] = 3;
    node[105]->nn_param.conv2d.ksize[1] = 3;
    node[105]->nn_param.conv2d.weights = 1024;
    node[105]->nn_param.conv2d.stride[0] = 2;
    node[105]->nn_param.conv2d.stride[1] = 2;
    node[105]->nn_param.conv2d.pad[0] = 1;
    node[105]->nn_param.conv2d.pad[1] = 1;
    node[105]->nn_param.conv2d.pad[2] = 1;
    node[105]->nn_param.conv2d.pad[3] = 1;
    node[105]->nn_param.conv2d.group = 1;
    node[105]->nn_param.conv2d.dilation[0] = 1;
    node[105]->nn_param.conv2d.dilation[1] = 1;
    node[105]->nn_param.conv2d.multiplier = 0;
    node[105]->vx_param.has_relu = FALSE;
    node[105]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[105]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[105]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_151
      var       - node[106]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[106], VSI_NN_OP_LEAKY_RELU, 1, 1, 151);
    node[106]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_152
      var       - node[107]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[107], VSI_NN_OP_CONV_RELU, 2, 1, 152);
    node[107]->nn_param.conv2d.ksize[0] = 1;
    node[107]->nn_param.conv2d.ksize[1] = 1;
    node[107]->nn_param.conv2d.weights = 512;
    node[107]->nn_param.conv2d.stride[0] = 1;
    node[107]->nn_param.conv2d.stride[1] = 1;
    node[107]->nn_param.conv2d.pad[0] = 0;
    node[107]->nn_param.conv2d.pad[1] = 0;
    node[107]->nn_param.conv2d.pad[2] = 0;
    node[107]->nn_param.conv2d.pad[3] = 0;
    node[107]->nn_param.conv2d.group = 1;
    node[107]->nn_param.conv2d.dilation[0] = 1;
    node[107]->nn_param.conv2d.dilation[1] = 1;
    node[107]->nn_param.conv2d.multiplier = 0;
    node[107]->vx_param.has_relu = FALSE;
    node[107]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[107]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[107]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_154
      var       - node[108]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[108], VSI_NN_OP_LEAKY_RELU, 1, 1, 154);
    node[108]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_155
      var       - node[109]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[109], VSI_NN_OP_CONV_RELU, 2, 1, 155);
    node[109]->nn_param.conv2d.ksize[0] = 3;
    node[109]->nn_param.conv2d.ksize[1] = 3;
    node[109]->nn_param.conv2d.weights = 1024;
    node[109]->nn_param.conv2d.stride[0] = 1;
    node[109]->nn_param.conv2d.stride[1] = 1;
    node[109]->nn_param.conv2d.pad[0] = 1;
    node[109]->nn_param.conv2d.pad[1] = 1;
    node[109]->nn_param.conv2d.pad[2] = 1;
    node[109]->nn_param.conv2d.pad[3] = 1;
    node[109]->nn_param.conv2d.group = 1;
    node[109]->nn_param.conv2d.dilation[0] = 1;
    node[109]->nn_param.conv2d.dilation[1] = 1;
    node[109]->nn_param.conv2d.multiplier = 0;
    node[109]->vx_param.has_relu = FALSE;
    node[109]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[109]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[109]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_157
      var       - node[110]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[110], VSI_NN_OP_LEAKY_RELU, 1, 1, 157);
    node[110]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_158
      var       - node[111]
      name      - add
      operation - add
      in_shape  - [[13, 13, 1024, 1]]
                  [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[111], VSI_NN_OP_ADD, 2, 1, 158);

    /*-----------------------------------------
      lid       - trans_convolution_159
      var       - node[112]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[112], VSI_NN_OP_CONV_RELU, 2, 1, 159);
    node[112]->nn_param.conv2d.ksize[0] = 1;
    node[112]->nn_param.conv2d.ksize[1] = 1;
    node[112]->nn_param.conv2d.weights = 512;
    node[112]->nn_param.conv2d.stride[0] = 1;
    node[112]->nn_param.conv2d.stride[1] = 1;
    node[112]->nn_param.conv2d.pad[0] = 0;
    node[112]->nn_param.conv2d.pad[1] = 0;
    node[112]->nn_param.conv2d.pad[2] = 0;
    node[112]->nn_param.conv2d.pad[3] = 0;
    node[112]->nn_param.conv2d.group = 1;
    node[112]->nn_param.conv2d.dilation[0] = 1;
    node[112]->nn_param.conv2d.dilation[1] = 1;
    node[112]->nn_param.conv2d.multiplier = 0;
    node[112]->vx_param.has_relu = FALSE;
    node[112]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[112]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[112]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_161
      var       - node[113]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[113], VSI_NN_OP_LEAKY_RELU, 1, 1, 161);
    node[113]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_162
      var       - node[114]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[114], VSI_NN_OP_CONV_RELU, 2, 1, 162);
    node[114]->nn_param.conv2d.ksize[0] = 3;
    node[114]->nn_param.conv2d.ksize[1] = 3;
    node[114]->nn_param.conv2d.weights = 1024;
    node[114]->nn_param.conv2d.stride[0] = 1;
    node[114]->nn_param.conv2d.stride[1] = 1;
    node[114]->nn_param.conv2d.pad[0] = 1;
    node[114]->nn_param.conv2d.pad[1] = 1;
    node[114]->nn_param.conv2d.pad[2] = 1;
    node[114]->nn_param.conv2d.pad[3] = 1;
    node[114]->nn_param.conv2d.group = 1;
    node[114]->nn_param.conv2d.dilation[0] = 1;
    node[114]->nn_param.conv2d.dilation[1] = 1;
    node[114]->nn_param.conv2d.multiplier = 0;
    node[114]->vx_param.has_relu = FALSE;
    node[114]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[114]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[114]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_164
      var       - node[115]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[115], VSI_NN_OP_LEAKY_RELU, 1, 1, 164);
    node[115]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_165
      var       - node[116]
      name      - add
      operation - add
      in_shape  - [[13, 13, 1024, 1]]
                  [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[116], VSI_NN_OP_ADD, 2, 1, 165);

    /*-----------------------------------------
      lid       - trans_convolution_166
      var       - node[117]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[117], VSI_NN_OP_CONV_RELU, 2, 1, 166);
    node[117]->nn_param.conv2d.ksize[0] = 1;
    node[117]->nn_param.conv2d.ksize[1] = 1;
    node[117]->nn_param.conv2d.weights = 512;
    node[117]->nn_param.conv2d.stride[0] = 1;
    node[117]->nn_param.conv2d.stride[1] = 1;
    node[117]->nn_param.conv2d.pad[0] = 0;
    node[117]->nn_param.conv2d.pad[1] = 0;
    node[117]->nn_param.conv2d.pad[2] = 0;
    node[117]->nn_param.conv2d.pad[3] = 0;
    node[117]->nn_param.conv2d.group = 1;
    node[117]->nn_param.conv2d.dilation[0] = 1;
    node[117]->nn_param.conv2d.dilation[1] = 1;
    node[117]->nn_param.conv2d.multiplier = 0;
    node[117]->vx_param.has_relu = FALSE;
    node[117]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[117]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[117]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_168
      var       - node[118]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[118], VSI_NN_OP_LEAKY_RELU, 1, 1, 168);
    node[118]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_169
      var       - node[119]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[119], VSI_NN_OP_CONV_RELU, 2, 1, 169);
    node[119]->nn_param.conv2d.ksize[0] = 3;
    node[119]->nn_param.conv2d.ksize[1] = 3;
    node[119]->nn_param.conv2d.weights = 1024;
    node[119]->nn_param.conv2d.stride[0] = 1;
    node[119]->nn_param.conv2d.stride[1] = 1;
    node[119]->nn_param.conv2d.pad[0] = 1;
    node[119]->nn_param.conv2d.pad[1] = 1;
    node[119]->nn_param.conv2d.pad[2] = 1;
    node[119]->nn_param.conv2d.pad[3] = 1;
    node[119]->nn_param.conv2d.group = 1;
    node[119]->nn_param.conv2d.dilation[0] = 1;
    node[119]->nn_param.conv2d.dilation[1] = 1;
    node[119]->nn_param.conv2d.multiplier = 0;
    node[119]->vx_param.has_relu = FALSE;
    node[119]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[119]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[119]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_171
      var       - node[120]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[120], VSI_NN_OP_LEAKY_RELU, 1, 1, 171);
    node[120]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_172
      var       - node[121]
      name      - add
      operation - add
      in_shape  - [[13, 13, 1024, 1]]
                  [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[121], VSI_NN_OP_ADD, 2, 1, 172);

    /*-----------------------------------------
      lid       - trans_convolution_173
      var       - node[122]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[122], VSI_NN_OP_CONV_RELU, 2, 1, 173);
    node[122]->nn_param.conv2d.ksize[0] = 1;
    node[122]->nn_param.conv2d.ksize[1] = 1;
    node[122]->nn_param.conv2d.weights = 512;
    node[122]->nn_param.conv2d.stride[0] = 1;
    node[122]->nn_param.conv2d.stride[1] = 1;
    node[122]->nn_param.conv2d.pad[0] = 0;
    node[122]->nn_param.conv2d.pad[1] = 0;
    node[122]->nn_param.conv2d.pad[2] = 0;
    node[122]->nn_param.conv2d.pad[3] = 0;
    node[122]->nn_param.conv2d.group = 1;
    node[122]->nn_param.conv2d.dilation[0] = 1;
    node[122]->nn_param.conv2d.dilation[1] = 1;
    node[122]->nn_param.conv2d.multiplier = 0;
    node[122]->vx_param.has_relu = FALSE;
    node[122]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[122]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[122]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_175
      var       - node[123]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[123], VSI_NN_OP_LEAKY_RELU, 1, 1, 175);
    node[123]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_176
      var       - node[124]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[124], VSI_NN_OP_CONV_RELU, 2, 1, 176);
    node[124]->nn_param.conv2d.ksize[0] = 3;
    node[124]->nn_param.conv2d.ksize[1] = 3;
    node[124]->nn_param.conv2d.weights = 1024;
    node[124]->nn_param.conv2d.stride[0] = 1;
    node[124]->nn_param.conv2d.stride[1] = 1;
    node[124]->nn_param.conv2d.pad[0] = 1;
    node[124]->nn_param.conv2d.pad[1] = 1;
    node[124]->nn_param.conv2d.pad[2] = 1;
    node[124]->nn_param.conv2d.pad[3] = 1;
    node[124]->nn_param.conv2d.group = 1;
    node[124]->nn_param.conv2d.dilation[0] = 1;
    node[124]->nn_param.conv2d.dilation[1] = 1;
    node[124]->nn_param.conv2d.multiplier = 0;
    node[124]->vx_param.has_relu = FALSE;
    node[124]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[124]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[124]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_178
      var       - node[125]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[125], VSI_NN_OP_LEAKY_RELU, 1, 1, 178);
    node[125]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - add_179
      var       - node[126]
      name      - add
      operation - add
      in_shape  - [[13, 13, 1024, 1]]
                  [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[126], VSI_NN_OP_ADD, 2, 1, 179);

    /*-----------------------------------------
      lid       - trans_convolution_180
      var       - node[127]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[127], VSI_NN_OP_CONV_RELU, 2, 1, 180);
    node[127]->nn_param.conv2d.ksize[0] = 1;
    node[127]->nn_param.conv2d.ksize[1] = 1;
    node[127]->nn_param.conv2d.weights = 512;
    node[127]->nn_param.conv2d.stride[0] = 1;
    node[127]->nn_param.conv2d.stride[1] = 1;
    node[127]->nn_param.conv2d.pad[0] = 0;
    node[127]->nn_param.conv2d.pad[1] = 0;
    node[127]->nn_param.conv2d.pad[2] = 0;
    node[127]->nn_param.conv2d.pad[3] = 0;
    node[127]->nn_param.conv2d.group = 1;
    node[127]->nn_param.conv2d.dilation[0] = 1;
    node[127]->nn_param.conv2d.dilation[1] = 1;
    node[127]->nn_param.conv2d.multiplier = 0;
    node[127]->vx_param.has_relu = FALSE;
    node[127]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[127]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[127]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_182
      var       - node[128]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[128], VSI_NN_OP_LEAKY_RELU, 1, 1, 182);
    node[128]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_183
      var       - node[129]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[129], VSI_NN_OP_CONV_RELU, 2, 1, 183);
    node[129]->nn_param.conv2d.ksize[0] = 3;
    node[129]->nn_param.conv2d.ksize[1] = 3;
    node[129]->nn_param.conv2d.weights = 1024;
    node[129]->nn_param.conv2d.stride[0] = 1;
    node[129]->nn_param.conv2d.stride[1] = 1;
    node[129]->nn_param.conv2d.pad[0] = 1;
    node[129]->nn_param.conv2d.pad[1] = 1;
    node[129]->nn_param.conv2d.pad[2] = 1;
    node[129]->nn_param.conv2d.pad[3] = 1;
    node[129]->nn_param.conv2d.group = 1;
    node[129]->nn_param.conv2d.dilation[0] = 1;
    node[129]->nn_param.conv2d.dilation[1] = 1;
    node[129]->nn_param.conv2d.multiplier = 0;
    node[129]->vx_param.has_relu = FALSE;
    node[129]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[129]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[129]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_185
      var       - node[130]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[130], VSI_NN_OP_LEAKY_RELU, 1, 1, 185);
    node[130]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_186
      var       - node[131]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[131], VSI_NN_OP_CONV_RELU, 2, 1, 186);
    node[131]->nn_param.conv2d.ksize[0] = 1;
    node[131]->nn_param.conv2d.ksize[1] = 1;
    node[131]->nn_param.conv2d.weights = 512;
    node[131]->nn_param.conv2d.stride[0] = 1;
    node[131]->nn_param.conv2d.stride[1] = 1;
    node[131]->nn_param.conv2d.pad[0] = 0;
    node[131]->nn_param.conv2d.pad[1] = 0;
    node[131]->nn_param.conv2d.pad[2] = 0;
    node[131]->nn_param.conv2d.pad[3] = 0;
    node[131]->nn_param.conv2d.group = 1;
    node[131]->nn_param.conv2d.dilation[0] = 1;
    node[131]->nn_param.conv2d.dilation[1] = 1;
    node[131]->nn_param.conv2d.multiplier = 0;
    node[131]->vx_param.has_relu = FALSE;
    node[131]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[131]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[131]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_188
      var       - node[132]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[132], VSI_NN_OP_LEAKY_RELU, 1, 1, 188);
    node[132]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_189
      var       - node[133]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[133], VSI_NN_OP_CONV_RELU, 2, 1, 189);
    node[133]->nn_param.conv2d.ksize[0] = 3;
    node[133]->nn_param.conv2d.ksize[1] = 3;
    node[133]->nn_param.conv2d.weights = 1024;
    node[133]->nn_param.conv2d.stride[0] = 1;
    node[133]->nn_param.conv2d.stride[1] = 1;
    node[133]->nn_param.conv2d.pad[0] = 1;
    node[133]->nn_param.conv2d.pad[1] = 1;
    node[133]->nn_param.conv2d.pad[2] = 1;
    node[133]->nn_param.conv2d.pad[3] = 1;
    node[133]->nn_param.conv2d.group = 1;
    node[133]->nn_param.conv2d.dilation[0] = 1;
    node[133]->nn_param.conv2d.dilation[1] = 1;
    node[133]->nn_param.conv2d.multiplier = 0;
    node[133]->vx_param.has_relu = FALSE;
    node[133]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[133]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[133]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_191
      var       - node[134]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[134], VSI_NN_OP_LEAKY_RELU, 1, 1, 191);
    node[134]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_192
      var       - node[135]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[135], VSI_NN_OP_CONV_RELU, 2, 1, 192);
    node[135]->nn_param.conv2d.ksize[0] = 1;
    node[135]->nn_param.conv2d.ksize[1] = 1;
    node[135]->nn_param.conv2d.weights = 512;
    node[135]->nn_param.conv2d.stride[0] = 1;
    node[135]->nn_param.conv2d.stride[1] = 1;
    node[135]->nn_param.conv2d.pad[0] = 0;
    node[135]->nn_param.conv2d.pad[1] = 0;
    node[135]->nn_param.conv2d.pad[2] = 0;
    node[135]->nn_param.conv2d.pad[3] = 0;
    node[135]->nn_param.conv2d.group = 1;
    node[135]->nn_param.conv2d.dilation[0] = 1;
    node[135]->nn_param.conv2d.dilation[1] = 1;
    node[135]->nn_param.conv2d.multiplier = 0;
    node[135]->vx_param.has_relu = FALSE;
    node[135]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[135]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[135]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_194
      var       - node[136]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[136], VSI_NN_OP_LEAKY_RELU, 1, 1, 194);
    node[136]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_195
      var       - node[137]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[137], VSI_NN_OP_CONV_RELU, 2, 1, 195);
    node[137]->nn_param.conv2d.ksize[0] = 3;
    node[137]->nn_param.conv2d.ksize[1] = 3;
    node[137]->nn_param.conv2d.weights = 1024;
    node[137]->nn_param.conv2d.stride[0] = 1;
    node[137]->nn_param.conv2d.stride[1] = 1;
    node[137]->nn_param.conv2d.pad[0] = 1;
    node[137]->nn_param.conv2d.pad[1] = 1;
    node[137]->nn_param.conv2d.pad[2] = 1;
    node[137]->nn_param.conv2d.pad[3] = 1;
    node[137]->nn_param.conv2d.group = 1;
    node[137]->nn_param.conv2d.dilation[0] = 1;
    node[137]->nn_param.conv2d.dilation[1] = 1;
    node[137]->nn_param.conv2d.multiplier = 0;
    node[137]->vx_param.has_relu = FALSE;
    node[137]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[137]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[137]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - trans_convolution_201
      var       - node[138]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[138], VSI_NN_OP_CONV_RELU, 2, 1, 201);
    node[138]->nn_param.conv2d.ksize[0] = 1;
    node[138]->nn_param.conv2d.ksize[1] = 1;
    node[138]->nn_param.conv2d.weights = 256;
    node[138]->nn_param.conv2d.stride[0] = 1;
    node[138]->nn_param.conv2d.stride[1] = 1;
    node[138]->nn_param.conv2d.pad[0] = 0;
    node[138]->nn_param.conv2d.pad[1] = 0;
    node[138]->nn_param.conv2d.pad[2] = 0;
    node[138]->nn_param.conv2d.pad[3] = 0;
    node[138]->nn_param.conv2d.group = 1;
    node[138]->nn_param.conv2d.dilation[0] = 1;
    node[138]->nn_param.conv2d.dilation[1] = 1;
    node[138]->nn_param.conv2d.multiplier = 0;
    node[138]->vx_param.has_relu = FALSE;
    node[138]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[138]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[138]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_197
      var       - node[139]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[139], VSI_NN_OP_LEAKY_RELU, 1, 1, 197);
    node[139]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - leakyrelu_203
      var       - node[140]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[13, 13, 256, 1]]
      out_shape - [[13, 13, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[140], VSI_NN_OP_LEAKY_RELU, 1, 1, 203);
    node[140]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_198
      var       - node[141]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 255, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[141], VSI_NN_OP_CONV_RELU, 2, 1, 198);
    node[141]->nn_param.conv2d.ksize[0] = 1;
    node[141]->nn_param.conv2d.ksize[1] = 1;
    node[141]->nn_param.conv2d.weights = 255;
    node[141]->nn_param.conv2d.stride[0] = 1;
    node[141]->nn_param.conv2d.stride[1] = 1;
    node[141]->nn_param.conv2d.pad[0] = 0;
    node[141]->nn_param.conv2d.pad[1] = 0;
    node[141]->nn_param.conv2d.pad[2] = 0;
    node[141]->nn_param.conv2d.pad[3] = 0;
    node[141]->nn_param.conv2d.group = 1;
    node[141]->nn_param.conv2d.dilation[0] = 1;
    node[141]->nn_param.conv2d.dilation[1] = 1;
    node[141]->nn_param.conv2d.multiplier = 0;
    node[141]->vx_param.has_relu = FALSE;
    node[141]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[141]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[141]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - upsampling_204
      var       - node[142]
      name      - upsampling
      operation - upsampling
      in_shape  - [[13, 13, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[142], VSI_NN_OP_RESIZE, 1, 1, 204);
    node[142]->nn_param.resize.type = VSI_NN_INTERPOLATION_NEAREST_NEIGHBOR;
    node[142]->nn_param.resize.factor = 2.0;

    /*-----------------------------------------
      lid       - concat_205
      var       - node[143]
      name      - concat
      operation - concat
      in_shape  - [[26, 26, 256, 1]]
                  [[26, 26, 512, 1]]
      out_shape - [[26, 26, 768, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[143], VSI_NN_OP_CONCAT, 2, 1, 205);
    node[143]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_convolution_206
      var       - node[144]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 768, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[144], VSI_NN_OP_CONV_RELU, 2, 1, 206);
    node[144]->nn_param.conv2d.ksize[0] = 1;
    node[144]->nn_param.conv2d.ksize[1] = 1;
    node[144]->nn_param.conv2d.weights = 256;
    node[144]->nn_param.conv2d.stride[0] = 1;
    node[144]->nn_param.conv2d.stride[1] = 1;
    node[144]->nn_param.conv2d.pad[0] = 0;
    node[144]->nn_param.conv2d.pad[1] = 0;
    node[144]->nn_param.conv2d.pad[2] = 0;
    node[144]->nn_param.conv2d.pad[3] = 0;
    node[144]->nn_param.conv2d.group = 1;
    node[144]->nn_param.conv2d.dilation[0] = 1;
    node[144]->nn_param.conv2d.dilation[1] = 1;
    node[144]->nn_param.conv2d.multiplier = 0;
    node[144]->vx_param.has_relu = FALSE;
    node[144]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[144]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[144]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_208
      var       - node[145]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[145], VSI_NN_OP_LEAKY_RELU, 1, 1, 208);
    node[145]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_209
      var       - node[146]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[146], VSI_NN_OP_CONV_RELU, 2, 1, 209);
    node[146]->nn_param.conv2d.ksize[0] = 3;
    node[146]->nn_param.conv2d.ksize[1] = 3;
    node[146]->nn_param.conv2d.weights = 512;
    node[146]->nn_param.conv2d.stride[0] = 1;
    node[146]->nn_param.conv2d.stride[1] = 1;
    node[146]->nn_param.conv2d.pad[0] = 1;
    node[146]->nn_param.conv2d.pad[1] = 1;
    node[146]->nn_param.conv2d.pad[2] = 1;
    node[146]->nn_param.conv2d.pad[3] = 1;
    node[146]->nn_param.conv2d.group = 1;
    node[146]->nn_param.conv2d.dilation[0] = 1;
    node[146]->nn_param.conv2d.dilation[1] = 1;
    node[146]->nn_param.conv2d.multiplier = 0;
    node[146]->vx_param.has_relu = FALSE;
    node[146]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[146]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[146]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_211
      var       - node[147]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[147], VSI_NN_OP_LEAKY_RELU, 1, 1, 211);
    node[147]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_212
      var       - node[148]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[148], VSI_NN_OP_CONV_RELU, 2, 1, 212);
    node[148]->nn_param.conv2d.ksize[0] = 1;
    node[148]->nn_param.conv2d.ksize[1] = 1;
    node[148]->nn_param.conv2d.weights = 256;
    node[148]->nn_param.conv2d.stride[0] = 1;
    node[148]->nn_param.conv2d.stride[1] = 1;
    node[148]->nn_param.conv2d.pad[0] = 0;
    node[148]->nn_param.conv2d.pad[1] = 0;
    node[148]->nn_param.conv2d.pad[2] = 0;
    node[148]->nn_param.conv2d.pad[3] = 0;
    node[148]->nn_param.conv2d.group = 1;
    node[148]->nn_param.conv2d.dilation[0] = 1;
    node[148]->nn_param.conv2d.dilation[1] = 1;
    node[148]->nn_param.conv2d.multiplier = 0;
    node[148]->vx_param.has_relu = FALSE;
    node[148]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[148]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[148]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_214
      var       - node[149]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[149], VSI_NN_OP_LEAKY_RELU, 1, 1, 214);
    node[149]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_215
      var       - node[150]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[150], VSI_NN_OP_CONV_RELU, 2, 1, 215);
    node[150]->nn_param.conv2d.ksize[0] = 3;
    node[150]->nn_param.conv2d.ksize[1] = 3;
    node[150]->nn_param.conv2d.weights = 512;
    node[150]->nn_param.conv2d.stride[0] = 1;
    node[150]->nn_param.conv2d.stride[1] = 1;
    node[150]->nn_param.conv2d.pad[0] = 1;
    node[150]->nn_param.conv2d.pad[1] = 1;
    node[150]->nn_param.conv2d.pad[2] = 1;
    node[150]->nn_param.conv2d.pad[3] = 1;
    node[150]->nn_param.conv2d.group = 1;
    node[150]->nn_param.conv2d.dilation[0] = 1;
    node[150]->nn_param.conv2d.dilation[1] = 1;
    node[150]->nn_param.conv2d.multiplier = 0;
    node[150]->vx_param.has_relu = FALSE;
    node[150]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[150]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[150]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_217
      var       - node[151]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[151], VSI_NN_OP_LEAKY_RELU, 1, 1, 217);
    node[151]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_218
      var       - node[152]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[152], VSI_NN_OP_CONV_RELU, 2, 1, 218);
    node[152]->nn_param.conv2d.ksize[0] = 1;
    node[152]->nn_param.conv2d.ksize[1] = 1;
    node[152]->nn_param.conv2d.weights = 256;
    node[152]->nn_param.conv2d.stride[0] = 1;
    node[152]->nn_param.conv2d.stride[1] = 1;
    node[152]->nn_param.conv2d.pad[0] = 0;
    node[152]->nn_param.conv2d.pad[1] = 0;
    node[152]->nn_param.conv2d.pad[2] = 0;
    node[152]->nn_param.conv2d.pad[3] = 0;
    node[152]->nn_param.conv2d.group = 1;
    node[152]->nn_param.conv2d.dilation[0] = 1;
    node[152]->nn_param.conv2d.dilation[1] = 1;
    node[152]->nn_param.conv2d.multiplier = 0;
    node[152]->vx_param.has_relu = FALSE;
    node[152]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[152]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[152]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_220
      var       - node[153]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[153], VSI_NN_OP_LEAKY_RELU, 1, 1, 220);
    node[153]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_221
      var       - node[154]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[154], VSI_NN_OP_CONV_RELU, 2, 1, 221);
    node[154]->nn_param.conv2d.ksize[0] = 3;
    node[154]->nn_param.conv2d.ksize[1] = 3;
    node[154]->nn_param.conv2d.weights = 512;
    node[154]->nn_param.conv2d.stride[0] = 1;
    node[154]->nn_param.conv2d.stride[1] = 1;
    node[154]->nn_param.conv2d.pad[0] = 1;
    node[154]->nn_param.conv2d.pad[1] = 1;
    node[154]->nn_param.conv2d.pad[2] = 1;
    node[154]->nn_param.conv2d.pad[3] = 1;
    node[154]->nn_param.conv2d.group = 1;
    node[154]->nn_param.conv2d.dilation[0] = 1;
    node[154]->nn_param.conv2d.dilation[1] = 1;
    node[154]->nn_param.conv2d.multiplier = 0;
    node[154]->vx_param.has_relu = FALSE;
    node[154]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[154]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[154]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - trans_convolution_227
      var       - node[155]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[155], VSI_NN_OP_CONV_RELU, 2, 1, 227);
    node[155]->nn_param.conv2d.ksize[0] = 1;
    node[155]->nn_param.conv2d.ksize[1] = 1;
    node[155]->nn_param.conv2d.weights = 128;
    node[155]->nn_param.conv2d.stride[0] = 1;
    node[155]->nn_param.conv2d.stride[1] = 1;
    node[155]->nn_param.conv2d.pad[0] = 0;
    node[155]->nn_param.conv2d.pad[1] = 0;
    node[155]->nn_param.conv2d.pad[2] = 0;
    node[155]->nn_param.conv2d.pad[3] = 0;
    node[155]->nn_param.conv2d.group = 1;
    node[155]->nn_param.conv2d.dilation[0] = 1;
    node[155]->nn_param.conv2d.dilation[1] = 1;
    node[155]->nn_param.conv2d.multiplier = 0;
    node[155]->vx_param.has_relu = FALSE;
    node[155]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[155]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[155]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_223
      var       - node[156]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[156], VSI_NN_OP_LEAKY_RELU, 1, 1, 223);
    node[156]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - leakyrelu_229
      var       - node[157]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[26, 26, 128, 1]]
      out_shape - [[26, 26, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[157], VSI_NN_OP_LEAKY_RELU, 1, 1, 229);
    node[157]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_224
      var       - node[158]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 255, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[158], VSI_NN_OP_CONV_RELU, 2, 1, 224);
    node[158]->nn_param.conv2d.ksize[0] = 1;
    node[158]->nn_param.conv2d.ksize[1] = 1;
    node[158]->nn_param.conv2d.weights = 255;
    node[158]->nn_param.conv2d.stride[0] = 1;
    node[158]->nn_param.conv2d.stride[1] = 1;
    node[158]->nn_param.conv2d.pad[0] = 0;
    node[158]->nn_param.conv2d.pad[1] = 0;
    node[158]->nn_param.conv2d.pad[2] = 0;
    node[158]->nn_param.conv2d.pad[3] = 0;
    node[158]->nn_param.conv2d.group = 1;
    node[158]->nn_param.conv2d.dilation[0] = 1;
    node[158]->nn_param.conv2d.dilation[1] = 1;
    node[158]->nn_param.conv2d.multiplier = 0;
    node[158]->vx_param.has_relu = FALSE;
    node[158]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[158]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[158]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - upsampling_230
      var       - node[159]
      name      - upsampling
      operation - upsampling
      in_shape  - [[26, 26, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[159], VSI_NN_OP_RESIZE, 1, 1, 230);
    node[159]->nn_param.resize.type = VSI_NN_INTERPOLATION_NEAREST_NEIGHBOR;
    node[159]->nn_param.resize.factor = 2.0;

    /*-----------------------------------------
      lid       - concat_231
      var       - node[160]
      name      - concat
      operation - concat
      in_shape  - [[52, 52, 128, 1]]
                  [[52, 52, 256, 1]]
      out_shape - [[52, 52, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[160], VSI_NN_OP_CONCAT, 2, 1, 231);
    node[160]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_convolution_232
      var       - node[161]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 384, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[161], VSI_NN_OP_CONV_RELU, 2, 1, 232);
    node[161]->nn_param.conv2d.ksize[0] = 1;
    node[161]->nn_param.conv2d.ksize[1] = 1;
    node[161]->nn_param.conv2d.weights = 128;
    node[161]->nn_param.conv2d.stride[0] = 1;
    node[161]->nn_param.conv2d.stride[1] = 1;
    node[161]->nn_param.conv2d.pad[0] = 0;
    node[161]->nn_param.conv2d.pad[1] = 0;
    node[161]->nn_param.conv2d.pad[2] = 0;
    node[161]->nn_param.conv2d.pad[3] = 0;
    node[161]->nn_param.conv2d.group = 1;
    node[161]->nn_param.conv2d.dilation[0] = 1;
    node[161]->nn_param.conv2d.dilation[1] = 1;
    node[161]->nn_param.conv2d.multiplier = 0;
    node[161]->vx_param.has_relu = FALSE;
    node[161]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[161]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[161]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_234
      var       - node[162]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[162], VSI_NN_OP_LEAKY_RELU, 1, 1, 234);
    node[162]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_235
      var       - node[163]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[163], VSI_NN_OP_CONV_RELU, 2, 1, 235);
    node[163]->nn_param.conv2d.ksize[0] = 3;
    node[163]->nn_param.conv2d.ksize[1] = 3;
    node[163]->nn_param.conv2d.weights = 256;
    node[163]->nn_param.conv2d.stride[0] = 1;
    node[163]->nn_param.conv2d.stride[1] = 1;
    node[163]->nn_param.conv2d.pad[0] = 1;
    node[163]->nn_param.conv2d.pad[1] = 1;
    node[163]->nn_param.conv2d.pad[2] = 1;
    node[163]->nn_param.conv2d.pad[3] = 1;
    node[163]->nn_param.conv2d.group = 1;
    node[163]->nn_param.conv2d.dilation[0] = 1;
    node[163]->nn_param.conv2d.dilation[1] = 1;
    node[163]->nn_param.conv2d.multiplier = 0;
    node[163]->vx_param.has_relu = FALSE;
    node[163]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[163]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[163]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_237
      var       - node[164]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[164], VSI_NN_OP_LEAKY_RELU, 1, 1, 237);
    node[164]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_238
      var       - node[165]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[165], VSI_NN_OP_CONV_RELU, 2, 1, 238);
    node[165]->nn_param.conv2d.ksize[0] = 1;
    node[165]->nn_param.conv2d.ksize[1] = 1;
    node[165]->nn_param.conv2d.weights = 128;
    node[165]->nn_param.conv2d.stride[0] = 1;
    node[165]->nn_param.conv2d.stride[1] = 1;
    node[165]->nn_param.conv2d.pad[0] = 0;
    node[165]->nn_param.conv2d.pad[1] = 0;
    node[165]->nn_param.conv2d.pad[2] = 0;
    node[165]->nn_param.conv2d.pad[3] = 0;
    node[165]->nn_param.conv2d.group = 1;
    node[165]->nn_param.conv2d.dilation[0] = 1;
    node[165]->nn_param.conv2d.dilation[1] = 1;
    node[165]->nn_param.conv2d.multiplier = 0;
    node[165]->vx_param.has_relu = FALSE;
    node[165]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[165]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[165]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_240
      var       - node[166]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[166], VSI_NN_OP_LEAKY_RELU, 1, 1, 240);
    node[166]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_241
      var       - node[167]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[167], VSI_NN_OP_CONV_RELU, 2, 1, 241);
    node[167]->nn_param.conv2d.ksize[0] = 3;
    node[167]->nn_param.conv2d.ksize[1] = 3;
    node[167]->nn_param.conv2d.weights = 256;
    node[167]->nn_param.conv2d.stride[0] = 1;
    node[167]->nn_param.conv2d.stride[1] = 1;
    node[167]->nn_param.conv2d.pad[0] = 1;
    node[167]->nn_param.conv2d.pad[1] = 1;
    node[167]->nn_param.conv2d.pad[2] = 1;
    node[167]->nn_param.conv2d.pad[3] = 1;
    node[167]->nn_param.conv2d.group = 1;
    node[167]->nn_param.conv2d.dilation[0] = 1;
    node[167]->nn_param.conv2d.dilation[1] = 1;
    node[167]->nn_param.conv2d.multiplier = 0;
    node[167]->vx_param.has_relu = FALSE;
    node[167]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[167]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[167]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_243
      var       - node[168]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[168], VSI_NN_OP_LEAKY_RELU, 1, 1, 243);
    node[168]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_244
      var       - node[169]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[169], VSI_NN_OP_CONV_RELU, 2, 1, 244);
    node[169]->nn_param.conv2d.ksize[0] = 1;
    node[169]->nn_param.conv2d.ksize[1] = 1;
    node[169]->nn_param.conv2d.weights = 128;
    node[169]->nn_param.conv2d.stride[0] = 1;
    node[169]->nn_param.conv2d.stride[1] = 1;
    node[169]->nn_param.conv2d.pad[0] = 0;
    node[169]->nn_param.conv2d.pad[1] = 0;
    node[169]->nn_param.conv2d.pad[2] = 0;
    node[169]->nn_param.conv2d.pad[3] = 0;
    node[169]->nn_param.conv2d.group = 1;
    node[169]->nn_param.conv2d.dilation[0] = 1;
    node[169]->nn_param.conv2d.dilation[1] = 1;
    node[169]->nn_param.conv2d.multiplier = 0;
    node[169]->vx_param.has_relu = FALSE;
    node[169]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[169]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[169]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_246
      var       - node[170]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[170], VSI_NN_OP_LEAKY_RELU, 1, 1, 246);
    node[170]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_247
      var       - node[171]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[171], VSI_NN_OP_CONV_RELU, 2, 1, 247);
    node[171]->nn_param.conv2d.ksize[0] = 3;
    node[171]->nn_param.conv2d.ksize[1] = 3;
    node[171]->nn_param.conv2d.weights = 256;
    node[171]->nn_param.conv2d.stride[0] = 1;
    node[171]->nn_param.conv2d.stride[1] = 1;
    node[171]->nn_param.conv2d.pad[0] = 1;
    node[171]->nn_param.conv2d.pad[1] = 1;
    node[171]->nn_param.conv2d.pad[2] = 1;
    node[171]->nn_param.conv2d.pad[3] = 1;
    node[171]->nn_param.conv2d.group = 1;
    node[171]->nn_param.conv2d.dilation[0] = 1;
    node[171]->nn_param.conv2d.dilation[1] = 1;
    node[171]->nn_param.conv2d.multiplier = 0;
    node[171]->vx_param.has_relu = FALSE;
    node[171]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[171]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[171]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - leakyrelu_249
      var       - node[172]
      name      - leakyrelu
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[172], VSI_NN_OP_LEAKY_RELU, 1, 1, 249);
    node[172]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_convolution_250
      var       - node[173]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 255, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[173], VSI_NN_OP_CONV_RELU, 2, 1, 250);
    node[173]->nn_param.conv2d.ksize[0] = 1;
    node[173]->nn_param.conv2d.ksize[1] = 1;
    node[173]->nn_param.conv2d.weights = 255;
    node[173]->nn_param.conv2d.stride[0] = 1;
    node[173]->nn_param.conv2d.stride[1] = 1;
    node[173]->nn_param.conv2d.pad[0] = 0;
    node[173]->nn_param.conv2d.pad[1] = 0;
    node[173]->nn_param.conv2d.pad[2] = 0;
    node[173]->nn_param.conv2d.pad[3] = 0;
    node[173]->nn_param.conv2d.group = 1;
    node[173]->nn_param.conv2d.dilation[0] = 1;
    node[173]->nn_param.conv2d.dilation[1] = 1;
    node[173]->nn_param.conv2d.multiplier = 0;
    node[173]->vx_param.has_relu = FALSE;
    node[173]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[173]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[173]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @input_0:out0 */
    attr.size[0] = 416;
    attr.size[1] = 416;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_INT8);

    /* @output_199:out0 */
    attr.size[0] = 13;
    attr.size[1] = 13;
    attr.size[2] = 255;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_INT8);

    /* @output_225:out0 */
    attr.size[0] = 26;
    attr.size[1] = 26;
    attr.size[2] = 255;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[2], attr, VSI_NN_TYPE_INT8);

    /* @output_251:out0 */
    attr.size[0] = 52;
    attr.size[1] = 52;
    attr.size[2] = 255;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[3], attr, VSI_NN_TYPE_INT8);



    /* @trans_convolution_1:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4036;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_VDATA, 0, 4036);

    /* @trans_convolution_4:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 44356;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_VDATA, 57053572, 44356);

    /* @trans_convolution_7:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 3344;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_VDATA, 58437848, 3344);

    /* @trans_convolution_10:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 37188;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_VDATA, 4036, 37188);

    /* @trans_convolution_14:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 160836;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_VDATA, 7815688, 160836);

    /* @trans_convolution_17:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10000;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_VDATA, 29519900, 10000);

    /* @trans_convolution_20:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 143940;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_VDATA, 50749500, 143940);

    /* @trans_convolution_24:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10000;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_VDATA, 55605504, 10000);

    /* @trans_convolution_27:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 146756;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_VDATA, 56311696, 146756);

    /* @trans_convolution_31:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 325072;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_VDATA, 56458452, 325072);

    /* @trans_convolution_34:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_VDATA, 56783524, 35088);

    /* @trans_convolution_37:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 234960;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_VDATA, 56818612, 234960);

    /* @trans_convolution_41:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_VDATA, 57097928, 35088);

    /* @trans_convolution_44:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 294416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_VDATA, 57133016, 294416);

    /* @trans_convolution_48:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_VDATA, 57427432, 35088);

    /* @trans_convolution_51:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 277840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_VDATA, 57462520, 277840);

    /* @trans_convolution_55:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_VDATA, 57740360, 35088);

    /* @trans_convolution_58:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 295056;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_VDATA, 57775448, 295056);

    /* @trans_convolution_62:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_VDATA, 58070504, 35088);

    /* @trans_convolution_65:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 297168;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[19], attr, VSI_NN_TYPE_VDATA, 58105592, 297168);

    /* @trans_convolution_69:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[20], attr, VSI_NN_TYPE_VDATA, 58402760, 35088);

    /* @trans_convolution_72:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 293968;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[21], attr, VSI_NN_TYPE_VDATA, 58441192, 293968);

    /* @trans_convolution_76:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[22], attr, VSI_NN_TYPE_VDATA, 58735160, 35088);

    /* @trans_convolution_79:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 294992;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[23], attr, VSI_NN_TYPE_VDATA, 58770248, 294992);

    /* @trans_convolution_83:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[24], attr, VSI_NN_TYPE_VDATA, 59065240, 35088);

    /* @trans_convolution_86:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 298256;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[25], attr, VSI_NN_TYPE_VDATA, 59100328, 298256);

    /* @trans_convolution_90:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1341968;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[26], attr, VSI_NN_TYPE_VDATA, 59398584, 1341968);

    /* @trans_convolution_93:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[27], attr, VSI_NN_TYPE_VDATA, 60740552, 134416);

    /* @trans_convolution_96:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 717200;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[28], attr, VSI_NN_TYPE_VDATA, 60874968, 717200);

    /* @trans_convolution_100:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[29], attr, VSI_NN_TYPE_VDATA, 41224, 134416);

    /* @trans_convolution_103:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1144016;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[30], attr, VSI_NN_TYPE_VDATA, 175640, 1144016);

    /* @trans_convolution_107:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[31], attr, VSI_NN_TYPE_VDATA, 1319656, 134416);

    /* @trans_convolution_110:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[32], attr, VSI_NN_TYPE_VDATA, 1454072, 1185040);

    /* @trans_convolution_114:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[33], attr, VSI_NN_TYPE_VDATA, 2639112, 134416);

    /* @trans_convolution_117:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1150864;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[34], attr, VSI_NN_TYPE_VDATA, 2773528, 1150864);

    /* @trans_convolution_121:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[35], attr, VSI_NN_TYPE_VDATA, 3924392, 134416);

    /* @trans_convolution_124:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1137936;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[36], attr, VSI_NN_TYPE_VDATA, 4058808, 1137936);

    /* @trans_convolution_128:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[37], attr, VSI_NN_TYPE_VDATA, 5196744, 134416);

    /* @trans_convolution_131:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[38], attr, VSI_NN_TYPE_VDATA, 5331160, 1185040);

    /* @trans_convolution_135:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[39], attr, VSI_NN_TYPE_VDATA, 6516200, 134416);

    /* @trans_convolution_138:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1165072;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[40], attr, VSI_NN_TYPE_VDATA, 6650616, 1165072);

    /* @trans_convolution_142:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[41], attr, VSI_NN_TYPE_VDATA, 7976524, 134416);

    /* @trans_convolution_145:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[42], attr, VSI_NN_TYPE_VDATA, 8110940, 1185040);

    /* @trans_convolution_149:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 5559888;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[43], attr, VSI_NN_TYPE_VDATA, 9295980, 5559888);

    /* @trans_convolution_152:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[44], attr, VSI_NN_TYPE_VDATA, 14855868, 529680);

    /* @trans_convolution_155:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4173392;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[45], attr, VSI_NN_TYPE_VDATA, 15385548, 4173392);

    /* @trans_convolution_159:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[46], attr, VSI_NN_TYPE_VDATA, 19558940, 529680);

    /* @trans_convolution_162:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4679952;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[47], attr, VSI_NN_TYPE_VDATA, 20088620, 4679952);

    /* @trans_convolution_166:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[48], attr, VSI_NN_TYPE_VDATA, 24768572, 529680);

    /* @trans_convolution_169:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4221648;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[49], attr, VSI_NN_TYPE_VDATA, 25298252, 4221648);

    /* @trans_convolution_173:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[50], attr, VSI_NN_TYPE_VDATA, 29529900, 529680);

    /* @trans_convolution_176:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4652240;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[51], attr, VSI_NN_TYPE_VDATA, 30059580, 4652240);

    /* @trans_convolution_180:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[52], attr, VSI_NN_TYPE_VDATA, 34711820, 529680);

    /* @trans_convolution_183:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4728080;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[53], attr, VSI_NN_TYPE_VDATA, 35241500, 4728080);

    /* @trans_convolution_186:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[54], attr, VSI_NN_TYPE_VDATA, 39969580, 529680);

    /* @trans_convolution_189:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4728080;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[55], attr, VSI_NN_TYPE_VDATA, 40499260, 4728080);

    /* @trans_convolution_192:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[56], attr, VSI_NN_TYPE_VDATA, 45227340, 529680);

    /* @trans_convolution_195:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4728080;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[57], attr, VSI_NN_TYPE_VDATA, 45757020, 4728080);

    /* @trans_convolution_201:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[58], attr, VSI_NN_TYPE_VDATA, 50893440, 134416);

    /* @trans_convolution_198:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 264400;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[59], attr, VSI_NN_TYPE_VDATA, 50485100, 264400);

    /* @trans_convolution_206:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 199952;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[60], attr, VSI_NN_TYPE_VDATA, 51027856, 199952);

    /* @trans_convolution_209:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[61], attr, VSI_NN_TYPE_VDATA, 51227808, 1185040);

    /* @trans_convolution_212:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[62], attr, VSI_NN_TYPE_VDATA, 52412848, 134416);

    /* @trans_convolution_215:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[63], attr, VSI_NN_TYPE_VDATA, 52547264, 1185040);

    /* @trans_convolution_218:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[64], attr, VSI_NN_TYPE_VDATA, 53732304, 134416);

    /* @trans_convolution_221:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[65], attr, VSI_NN_TYPE_VDATA, 53866720, 1185040);

    /* @trans_convolution_227:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[66], attr, VSI_NN_TYPE_VDATA, 55185600, 35088);

    /* @trans_convolution_224:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 133840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[67], attr, VSI_NN_TYPE_VDATA, 55051760, 133840);

    /* @trans_convolution_232:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 51472;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[68], attr, VSI_NN_TYPE_VDATA, 55220688, 51472);

    /* @trans_convolution_235:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 298256;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[69], attr, VSI_NN_TYPE_VDATA, 55272160, 298256);

    /* @trans_convolution_238:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[70], attr, VSI_NN_TYPE_VDATA, 55570416, 35088);

    /* @trans_convolution_241:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 298256;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[71], attr, VSI_NN_TYPE_VDATA, 55615504, 298256);

    /* @trans_convolution_244:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[72], attr, VSI_NN_TYPE_VDATA, 55913760, 35088);

    /* @trans_convolution_247:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 294288;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[73], attr, VSI_NN_TYPE_VDATA, 55948848, 294288);

    /* @trans_convolution_250:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 68560;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[74], attr, VSI_NN_TYPE_VDATA, 56243136, 68560);



    /* @trans_convolution_1:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_3:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_4:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_6:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_7:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_9:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_10:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_12:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_13:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_14:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_16:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_17:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_19:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_20:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_22:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_23:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_24:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_26:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_27:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_29:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_30:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_31:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[21]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_33:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[22]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_34:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[23]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_36:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[24]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_37:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[25]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_39:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[26]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_40:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[27]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_41:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[28]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_43:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[29]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_44:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[30]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_46:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[31]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_47:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[32]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_48:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[33]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_50:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[34]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_51:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[35]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_53:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[36]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_54:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[37]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_55:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[38]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_57:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[39]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_58:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[40]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_60:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[41]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_61:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[42]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_62:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[43]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_64:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[44]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_65:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[45]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_67:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[46]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_68:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[47]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_69:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[48]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_71:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[49]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_72:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[50]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_74:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[51]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_75:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[52]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_76:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[53]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_78:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[54]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_79:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[55]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_81:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[56]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_82:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[57]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_83:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[58]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_85:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[59]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_86:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[60]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_88:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[61]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_89:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[62]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_90:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[63]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_92:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[64]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_93:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[65]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_95:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[66]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_96:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[67]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_98:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[68]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_99:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[69]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_100:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[70]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_102:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[71]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_103:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[72]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_105:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[73]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_106:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[74]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_107:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[75]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_109:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[76]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_110:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[77]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_112:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[78]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_113:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[79]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_114:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[80]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_116:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[81]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_117:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[82]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_119:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[83]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_120:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[84]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_121:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[85]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_123:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[86]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_124:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[87]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_126:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[88]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_127:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[89]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_128:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[90]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_130:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[91]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_131:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[92]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_133:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[93]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_134:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[94]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_135:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[95]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_137:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[96]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_138:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[97]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_140:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[98]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_141:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[99]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_142:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[100]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_144:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[101]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_145:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[102]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_147:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[103]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_148:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[104]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_149:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[105]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_151:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[106]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_152:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[107]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_154:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[108]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_155:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[109]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_157:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[110]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_158:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[111]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_159:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[112]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_161:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[113]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_162:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[114]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_164:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[115]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_165:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[116]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_166:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[117]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_168:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[118]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_169:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[119]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_171:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[120]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_172:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[121]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_173:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[122]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_175:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[123]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_176:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[124]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_178:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[125]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @add_179:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[126]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_180:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[127]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_182:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[128]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_183:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[129]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_185:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[130]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_186:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[131]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_188:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[132]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_189:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[133]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_191:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[134]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_192:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[135]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_194:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[136]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_195:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[137]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_201:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[138]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_197:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[139]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_203:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[140]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @upsampling_204:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[142]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @concat_205:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[143]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_206:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[144]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_208:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[145]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_209:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[146]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_211:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[147]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_212:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[148]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_214:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[149]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_215:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[150]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_217:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[151]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_218:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[152]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_220:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[153]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_221:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[154]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_227:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[155]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_223:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[156]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_229:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[157]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @upsampling_230:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[159]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @concat_231:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[160]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_232:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[161]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_234:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[162]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_235:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[163]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_237:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[164]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_238:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[165]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_240:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[166]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_241:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[167]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_243:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[168]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_244:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[169]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_246:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[170]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_convolution_247:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[171]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @leakyrelu_249:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[172]->output.tensors[0], attr, VSI_NN_TYPE_INT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[0]->input.tensors[0] = norm_tensor[0];
    node[141]->output.tensors[0] = norm_tensor[1];
    node[158]->output.tensors[0] = norm_tensor[2];
    node[173]->output.tensors[0] = norm_tensor[3];

    /* trans_convolution_1 */
    node[0]->input.tensors[1] = const_tensor[0]; /* data_vdata */

    /* leakyrelu_3 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];

    /* trans_convolution_4 */
    node[2]->input.tensors[0] = node[1]->output.tensors[0];
    node[2]->input.tensors[1] = const_tensor[1]; /* data_vdata */

    /* leakyrelu_6 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];

    /* trans_convolution_7 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[2]; /* data_vdata */

    /* leakyrelu_9 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];

    /* trans_convolution_10 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];
    node[6]->input.tensors[1] = const_tensor[3]; /* data_vdata */

    /* leakyrelu_12 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];

    /* add_13 */
    node[8]->input.tensors[0] = node[3]->output.tensors[0];
    node[8]->input.tensors[1] = node[7]->output.tensors[0];

    /* trans_convolution_14 */
    node[9]->input.tensors[0] = node[8]->output.tensors[0];
    node[9]->input.tensors[1] = const_tensor[4]; /* data_vdata */

    /* leakyrelu_16 */
    node[10]->input.tensors[0] = node[9]->output.tensors[0];

    /* trans_convolution_17 */
    node[11]->input.tensors[0] = node[10]->output.tensors[0];
    node[11]->input.tensors[1] = const_tensor[5]; /* data_vdata */

    /* leakyrelu_19 */
    node[12]->input.tensors[0] = node[11]->output.tensors[0];

    /* trans_convolution_20 */
    node[13]->input.tensors[0] = node[12]->output.tensors[0];
    node[13]->input.tensors[1] = const_tensor[6]; /* data_vdata */

    /* leakyrelu_22 */
    node[14]->input.tensors[0] = node[13]->output.tensors[0];

    /* add_23 */
    node[15]->input.tensors[0] = node[10]->output.tensors[0];
    node[15]->input.tensors[1] = node[14]->output.tensors[0];

    /* trans_convolution_24 */
    node[16]->input.tensors[0] = node[15]->output.tensors[0];
    node[16]->input.tensors[1] = const_tensor[7]; /* data_vdata */

    /* leakyrelu_26 */
    node[17]->input.tensors[0] = node[16]->output.tensors[0];

    /* trans_convolution_27 */
    node[18]->input.tensors[0] = node[17]->output.tensors[0];
    node[18]->input.tensors[1] = const_tensor[8]; /* data_vdata */

    /* leakyrelu_29 */
    node[19]->input.tensors[0] = node[18]->output.tensors[0];

    /* add_30 */
    node[20]->input.tensors[0] = node[15]->output.tensors[0];
    node[20]->input.tensors[1] = node[19]->output.tensors[0];

    /* trans_convolution_31 */
    node[21]->input.tensors[0] = node[20]->output.tensors[0];
    node[21]->input.tensors[1] = const_tensor[9]; /* data_vdata */

    /* leakyrelu_33 */
    node[22]->input.tensors[0] = node[21]->output.tensors[0];

    /* trans_convolution_34 */
    node[23]->input.tensors[0] = node[22]->output.tensors[0];
    node[23]->input.tensors[1] = const_tensor[10]; /* data_vdata */

    /* leakyrelu_36 */
    node[24]->input.tensors[0] = node[23]->output.tensors[0];

    /* trans_convolution_37 */
    node[25]->input.tensors[0] = node[24]->output.tensors[0];
    node[25]->input.tensors[1] = const_tensor[11]; /* data_vdata */

    /* leakyrelu_39 */
    node[26]->input.tensors[0] = node[25]->output.tensors[0];

    /* add_40 */
    node[27]->input.tensors[0] = node[22]->output.tensors[0];
    node[27]->input.tensors[1] = node[26]->output.tensors[0];

    /* trans_convolution_41 */
    node[28]->input.tensors[0] = node[27]->output.tensors[0];
    node[28]->input.tensors[1] = const_tensor[12]; /* data_vdata */

    /* leakyrelu_43 */
    node[29]->input.tensors[0] = node[28]->output.tensors[0];

    /* trans_convolution_44 */
    node[30]->input.tensors[0] = node[29]->output.tensors[0];
    node[30]->input.tensors[1] = const_tensor[13]; /* data_vdata */

    /* leakyrelu_46 */
    node[31]->input.tensors[0] = node[30]->output.tensors[0];

    /* add_47 */
    node[32]->input.tensors[0] = node[27]->output.tensors[0];
    node[32]->input.tensors[1] = node[31]->output.tensors[0];

    /* trans_convolution_48 */
    node[33]->input.tensors[0] = node[32]->output.tensors[0];
    node[33]->input.tensors[1] = const_tensor[14]; /* data_vdata */

    /* leakyrelu_50 */
    node[34]->input.tensors[0] = node[33]->output.tensors[0];

    /* trans_convolution_51 */
    node[35]->input.tensors[0] = node[34]->output.tensors[0];
    node[35]->input.tensors[1] = const_tensor[15]; /* data_vdata */

    /* leakyrelu_53 */
    node[36]->input.tensors[0] = node[35]->output.tensors[0];

    /* add_54 */
    node[37]->input.tensors[0] = node[32]->output.tensors[0];
    node[37]->input.tensors[1] = node[36]->output.tensors[0];

    /* trans_convolution_55 */
    node[38]->input.tensors[0] = node[37]->output.tensors[0];
    node[38]->input.tensors[1] = const_tensor[16]; /* data_vdata */

    /* leakyrelu_57 */
    node[39]->input.tensors[0] = node[38]->output.tensors[0];

    /* trans_convolution_58 */
    node[40]->input.tensors[0] = node[39]->output.tensors[0];
    node[40]->input.tensors[1] = const_tensor[17]; /* data_vdata */

    /* leakyrelu_60 */
    node[41]->input.tensors[0] = node[40]->output.tensors[0];

    /* add_61 */
    node[42]->input.tensors[0] = node[37]->output.tensors[0];
    node[42]->input.tensors[1] = node[41]->output.tensors[0];

    /* trans_convolution_62 */
    node[43]->input.tensors[0] = node[42]->output.tensors[0];
    node[43]->input.tensors[1] = const_tensor[18]; /* data_vdata */

    /* leakyrelu_64 */
    node[44]->input.tensors[0] = node[43]->output.tensors[0];

    /* trans_convolution_65 */
    node[45]->input.tensors[0] = node[44]->output.tensors[0];
    node[45]->input.tensors[1] = const_tensor[19]; /* data_vdata */

    /* leakyrelu_67 */
    node[46]->input.tensors[0] = node[45]->output.tensors[0];

    /* add_68 */
    node[47]->input.tensors[0] = node[42]->output.tensors[0];
    node[47]->input.tensors[1] = node[46]->output.tensors[0];

    /* trans_convolution_69 */
    node[48]->input.tensors[0] = node[47]->output.tensors[0];
    node[48]->input.tensors[1] = const_tensor[20]; /* data_vdata */

    /* leakyrelu_71 */
    node[49]->input.tensors[0] = node[48]->output.tensors[0];

    /* trans_convolution_72 */
    node[50]->input.tensors[0] = node[49]->output.tensors[0];
    node[50]->input.tensors[1] = const_tensor[21]; /* data_vdata */

    /* leakyrelu_74 */
    node[51]->input.tensors[0] = node[50]->output.tensors[0];

    /* add_75 */
    node[52]->input.tensors[0] = node[47]->output.tensors[0];
    node[52]->input.tensors[1] = node[51]->output.tensors[0];

    /* trans_convolution_76 */
    node[53]->input.tensors[0] = node[52]->output.tensors[0];
    node[53]->input.tensors[1] = const_tensor[22]; /* data_vdata */

    /* leakyrelu_78 */
    node[54]->input.tensors[0] = node[53]->output.tensors[0];

    /* trans_convolution_79 */
    node[55]->input.tensors[0] = node[54]->output.tensors[0];
    node[55]->input.tensors[1] = const_tensor[23]; /* data_vdata */

    /* leakyrelu_81 */
    node[56]->input.tensors[0] = node[55]->output.tensors[0];

    /* add_82 */
    node[57]->input.tensors[0] = node[52]->output.tensors[0];
    node[57]->input.tensors[1] = node[56]->output.tensors[0];

    /* trans_convolution_83 */
    node[58]->input.tensors[0] = node[57]->output.tensors[0];
    node[58]->input.tensors[1] = const_tensor[24]; /* data_vdata */

    /* leakyrelu_85 */
    node[59]->input.tensors[0] = node[58]->output.tensors[0];

    /* trans_convolution_86 */
    node[60]->input.tensors[0] = node[59]->output.tensors[0];
    node[60]->input.tensors[1] = const_tensor[25]; /* data_vdata */

    /* leakyrelu_88 */
    node[61]->input.tensors[0] = node[60]->output.tensors[0];

    /* add_89 */
    node[62]->input.tensors[0] = node[57]->output.tensors[0];
    node[62]->input.tensors[1] = node[61]->output.tensors[0];

    /* trans_convolution_90 */
    node[63]->input.tensors[0] = node[62]->output.tensors[0];
    node[63]->input.tensors[1] = const_tensor[26]; /* data_vdata */

    /* leakyrelu_92 */
    node[64]->input.tensors[0] = node[63]->output.tensors[0];

    /* trans_convolution_93 */
    node[65]->input.tensors[0] = node[64]->output.tensors[0];
    node[65]->input.tensors[1] = const_tensor[27]; /* data_vdata */

    /* leakyrelu_95 */
    node[66]->input.tensors[0] = node[65]->output.tensors[0];

    /* trans_convolution_96 */
    node[67]->input.tensors[0] = node[66]->output.tensors[0];
    node[67]->input.tensors[1] = const_tensor[28]; /* data_vdata */

    /* leakyrelu_98 */
    node[68]->input.tensors[0] = node[67]->output.tensors[0];

    /* add_99 */
    node[69]->input.tensors[0] = node[64]->output.tensors[0];
    node[69]->input.tensors[1] = node[68]->output.tensors[0];

    /* trans_convolution_100 */
    node[70]->input.tensors[0] = node[69]->output.tensors[0];
    node[70]->input.tensors[1] = const_tensor[29]; /* data_vdata */

    /* leakyrelu_102 */
    node[71]->input.tensors[0] = node[70]->output.tensors[0];

    /* trans_convolution_103 */
    node[72]->input.tensors[0] = node[71]->output.tensors[0];
    node[72]->input.tensors[1] = const_tensor[30]; /* data_vdata */

    /* leakyrelu_105 */
    node[73]->input.tensors[0] = node[72]->output.tensors[0];

    /* add_106 */
    node[74]->input.tensors[0] = node[69]->output.tensors[0];
    node[74]->input.tensors[1] = node[73]->output.tensors[0];

    /* trans_convolution_107 */
    node[75]->input.tensors[0] = node[74]->output.tensors[0];
    node[75]->input.tensors[1] = const_tensor[31]; /* data_vdata */

    /* leakyrelu_109 */
    node[76]->input.tensors[0] = node[75]->output.tensors[0];

    /* trans_convolution_110 */
    node[77]->input.tensors[0] = node[76]->output.tensors[0];
    node[77]->input.tensors[1] = const_tensor[32]; /* data_vdata */

    /* leakyrelu_112 */
    node[78]->input.tensors[0] = node[77]->output.tensors[0];

    /* add_113 */
    node[79]->input.tensors[0] = node[74]->output.tensors[0];
    node[79]->input.tensors[1] = node[78]->output.tensors[0];

    /* trans_convolution_114 */
    node[80]->input.tensors[0] = node[79]->output.tensors[0];
    node[80]->input.tensors[1] = const_tensor[33]; /* data_vdata */

    /* leakyrelu_116 */
    node[81]->input.tensors[0] = node[80]->output.tensors[0];

    /* trans_convolution_117 */
    node[82]->input.tensors[0] = node[81]->output.tensors[0];
    node[82]->input.tensors[1] = const_tensor[34]; /* data_vdata */

    /* leakyrelu_119 */
    node[83]->input.tensors[0] = node[82]->output.tensors[0];

    /* add_120 */
    node[84]->input.tensors[0] = node[79]->output.tensors[0];
    node[84]->input.tensors[1] = node[83]->output.tensors[0];

    /* trans_convolution_121 */
    node[85]->input.tensors[0] = node[84]->output.tensors[0];
    node[85]->input.tensors[1] = const_tensor[35]; /* data_vdata */

    /* leakyrelu_123 */
    node[86]->input.tensors[0] = node[85]->output.tensors[0];

    /* trans_convolution_124 */
    node[87]->input.tensors[0] = node[86]->output.tensors[0];
    node[87]->input.tensors[1] = const_tensor[36]; /* data_vdata */

    /* leakyrelu_126 */
    node[88]->input.tensors[0] = node[87]->output.tensors[0];

    /* add_127 */
    node[89]->input.tensors[0] = node[84]->output.tensors[0];
    node[89]->input.tensors[1] = node[88]->output.tensors[0];

    /* trans_convolution_128 */
    node[90]->input.tensors[0] = node[89]->output.tensors[0];
    node[90]->input.tensors[1] = const_tensor[37]; /* data_vdata */

    /* leakyrelu_130 */
    node[91]->input.tensors[0] = node[90]->output.tensors[0];

    /* trans_convolution_131 */
    node[92]->input.tensors[0] = node[91]->output.tensors[0];
    node[92]->input.tensors[1] = const_tensor[38]; /* data_vdata */

    /* leakyrelu_133 */
    node[93]->input.tensors[0] = node[92]->output.tensors[0];

    /* add_134 */
    node[94]->input.tensors[0] = node[89]->output.tensors[0];
    node[94]->input.tensors[1] = node[93]->output.tensors[0];

    /* trans_convolution_135 */
    node[95]->input.tensors[0] = node[94]->output.tensors[0];
    node[95]->input.tensors[1] = const_tensor[39]; /* data_vdata */

    /* leakyrelu_137 */
    node[96]->input.tensors[0] = node[95]->output.tensors[0];

    /* trans_convolution_138 */
    node[97]->input.tensors[0] = node[96]->output.tensors[0];
    node[97]->input.tensors[1] = const_tensor[40]; /* data_vdata */

    /* leakyrelu_140 */
    node[98]->input.tensors[0] = node[97]->output.tensors[0];

    /* add_141 */
    node[99]->input.tensors[0] = node[94]->output.tensors[0];
    node[99]->input.tensors[1] = node[98]->output.tensors[0];

    /* trans_convolution_142 */
    node[100]->input.tensors[0] = node[99]->output.tensors[0];
    node[100]->input.tensors[1] = const_tensor[41]; /* data_vdata */

    /* leakyrelu_144 */
    node[101]->input.tensors[0] = node[100]->output.tensors[0];

    /* trans_convolution_145 */
    node[102]->input.tensors[0] = node[101]->output.tensors[0];
    node[102]->input.tensors[1] = const_tensor[42]; /* data_vdata */

    /* leakyrelu_147 */
    node[103]->input.tensors[0] = node[102]->output.tensors[0];

    /* add_148 */
    node[104]->input.tensors[0] = node[99]->output.tensors[0];
    node[104]->input.tensors[1] = node[103]->output.tensors[0];

    /* trans_convolution_149 */
    node[105]->input.tensors[0] = node[104]->output.tensors[0];
    node[105]->input.tensors[1] = const_tensor[43]; /* data_vdata */

    /* leakyrelu_151 */
    node[106]->input.tensors[0] = node[105]->output.tensors[0];

    /* trans_convolution_152 */
    node[107]->input.tensors[0] = node[106]->output.tensors[0];
    node[107]->input.tensors[1] = const_tensor[44]; /* data_vdata */

    /* leakyrelu_154 */
    node[108]->input.tensors[0] = node[107]->output.tensors[0];

    /* trans_convolution_155 */
    node[109]->input.tensors[0] = node[108]->output.tensors[0];
    node[109]->input.tensors[1] = const_tensor[45]; /* data_vdata */

    /* leakyrelu_157 */
    node[110]->input.tensors[0] = node[109]->output.tensors[0];

    /* add_158 */
    node[111]->input.tensors[0] = node[106]->output.tensors[0];
    node[111]->input.tensors[1] = node[110]->output.tensors[0];

    /* trans_convolution_159 */
    node[112]->input.tensors[0] = node[111]->output.tensors[0];
    node[112]->input.tensors[1] = const_tensor[46]; /* data_vdata */

    /* leakyrelu_161 */
    node[113]->input.tensors[0] = node[112]->output.tensors[0];

    /* trans_convolution_162 */
    node[114]->input.tensors[0] = node[113]->output.tensors[0];
    node[114]->input.tensors[1] = const_tensor[47]; /* data_vdata */

    /* leakyrelu_164 */
    node[115]->input.tensors[0] = node[114]->output.tensors[0];

    /* add_165 */
    node[116]->input.tensors[0] = node[111]->output.tensors[0];
    node[116]->input.tensors[1] = node[115]->output.tensors[0];

    /* trans_convolution_166 */
    node[117]->input.tensors[0] = node[116]->output.tensors[0];
    node[117]->input.tensors[1] = const_tensor[48]; /* data_vdata */

    /* leakyrelu_168 */
    node[118]->input.tensors[0] = node[117]->output.tensors[0];

    /* trans_convolution_169 */
    node[119]->input.tensors[0] = node[118]->output.tensors[0];
    node[119]->input.tensors[1] = const_tensor[49]; /* data_vdata */

    /* leakyrelu_171 */
    node[120]->input.tensors[0] = node[119]->output.tensors[0];

    /* add_172 */
    node[121]->input.tensors[0] = node[116]->output.tensors[0];
    node[121]->input.tensors[1] = node[120]->output.tensors[0];

    /* trans_convolution_173 */
    node[122]->input.tensors[0] = node[121]->output.tensors[0];
    node[122]->input.tensors[1] = const_tensor[50]; /* data_vdata */

    /* leakyrelu_175 */
    node[123]->input.tensors[0] = node[122]->output.tensors[0];

    /* trans_convolution_176 */
    node[124]->input.tensors[0] = node[123]->output.tensors[0];
    node[124]->input.tensors[1] = const_tensor[51]; /* data_vdata */

    /* leakyrelu_178 */
    node[125]->input.tensors[0] = node[124]->output.tensors[0];

    /* add_179 */
    node[126]->input.tensors[0] = node[121]->output.tensors[0];
    node[126]->input.tensors[1] = node[125]->output.tensors[0];

    /* trans_convolution_180 */
    node[127]->input.tensors[0] = node[126]->output.tensors[0];
    node[127]->input.tensors[1] = const_tensor[52]; /* data_vdata */

    /* leakyrelu_182 */
    node[128]->input.tensors[0] = node[127]->output.tensors[0];

    /* trans_convolution_183 */
    node[129]->input.tensors[0] = node[128]->output.tensors[0];
    node[129]->input.tensors[1] = const_tensor[53]; /* data_vdata */

    /* leakyrelu_185 */
    node[130]->input.tensors[0] = node[129]->output.tensors[0];

    /* trans_convolution_186 */
    node[131]->input.tensors[0] = node[130]->output.tensors[0];
    node[131]->input.tensors[1] = const_tensor[54]; /* data_vdata */

    /* leakyrelu_188 */
    node[132]->input.tensors[0] = node[131]->output.tensors[0];

    /* trans_convolution_189 */
    node[133]->input.tensors[0] = node[132]->output.tensors[0];
    node[133]->input.tensors[1] = const_tensor[55]; /* data_vdata */

    /* leakyrelu_191 */
    node[134]->input.tensors[0] = node[133]->output.tensors[0];

    /* trans_convolution_192 */
    node[135]->input.tensors[0] = node[134]->output.tensors[0];
    node[135]->input.tensors[1] = const_tensor[56]; /* data_vdata */

    /* leakyrelu_194 */
    node[136]->input.tensors[0] = node[135]->output.tensors[0];

    /* trans_convolution_195 */
    node[137]->input.tensors[0] = node[136]->output.tensors[0];
    node[137]->input.tensors[1] = const_tensor[57]; /* data_vdata */

    /* trans_convolution_201 */
    node[138]->input.tensors[0] = node[136]->output.tensors[0];
    node[138]->input.tensors[1] = const_tensor[58]; /* data_vdata */

    /* leakyrelu_197 */
    node[139]->input.tensors[0] = node[137]->output.tensors[0];

    /* leakyrelu_203 */
    node[140]->input.tensors[0] = node[138]->output.tensors[0];

    /* trans_convolution_198 */
    node[141]->input.tensors[0] = node[139]->output.tensors[0];
    node[141]->input.tensors[1] = const_tensor[59]; /* data_vdata */

    /* upsampling_204 */
    node[142]->input.tensors[0] = node[140]->output.tensors[0];

    /* concat_205 */
    node[143]->input.tensors[0] = node[142]->output.tensors[0];
    node[143]->input.tensors[1] = node[104]->output.tensors[0];

    /* trans_convolution_206 */
    node[144]->input.tensors[0] = node[143]->output.tensors[0];
    node[144]->input.tensors[1] = const_tensor[60]; /* data_vdata */

    /* leakyrelu_208 */
    node[145]->input.tensors[0] = node[144]->output.tensors[0];

    /* trans_convolution_209 */
    node[146]->input.tensors[0] = node[145]->output.tensors[0];
    node[146]->input.tensors[1] = const_tensor[61]; /* data_vdata */

    /* leakyrelu_211 */
    node[147]->input.tensors[0] = node[146]->output.tensors[0];

    /* trans_convolution_212 */
    node[148]->input.tensors[0] = node[147]->output.tensors[0];
    node[148]->input.tensors[1] = const_tensor[62]; /* data_vdata */

    /* leakyrelu_214 */
    node[149]->input.tensors[0] = node[148]->output.tensors[0];

    /* trans_convolution_215 */
    node[150]->input.tensors[0] = node[149]->output.tensors[0];
    node[150]->input.tensors[1] = const_tensor[63]; /* data_vdata */

    /* leakyrelu_217 */
    node[151]->input.tensors[0] = node[150]->output.tensors[0];

    /* trans_convolution_218 */
    node[152]->input.tensors[0] = node[151]->output.tensors[0];
    node[152]->input.tensors[1] = const_tensor[64]; /* data_vdata */

    /* leakyrelu_220 */
    node[153]->input.tensors[0] = node[152]->output.tensors[0];

    /* trans_convolution_221 */
    node[154]->input.tensors[0] = node[153]->output.tensors[0];
    node[154]->input.tensors[1] = const_tensor[65]; /* data_vdata */

    /* trans_convolution_227 */
    node[155]->input.tensors[0] = node[153]->output.tensors[0];
    node[155]->input.tensors[1] = const_tensor[66]; /* data_vdata */

    /* leakyrelu_223 */
    node[156]->input.tensors[0] = node[154]->output.tensors[0];

    /* leakyrelu_229 */
    node[157]->input.tensors[0] = node[155]->output.tensors[0];

    /* trans_convolution_224 */
    node[158]->input.tensors[0] = node[156]->output.tensors[0];
    node[158]->input.tensors[1] = const_tensor[67]; /* data_vdata */

    /* upsampling_230 */
    node[159]->input.tensors[0] = node[157]->output.tensors[0];

    /* concat_231 */
    node[160]->input.tensors[0] = node[159]->output.tensors[0];
    node[160]->input.tensors[1] = node[62]->output.tensors[0];

    /* trans_convolution_232 */
    node[161]->input.tensors[0] = node[160]->output.tensors[0];
    node[161]->input.tensors[1] = const_tensor[68]; /* data_vdata */

    /* leakyrelu_234 */
    node[162]->input.tensors[0] = node[161]->output.tensors[0];

    /* trans_convolution_235 */
    node[163]->input.tensors[0] = node[162]->output.tensors[0];
    node[163]->input.tensors[1] = const_tensor[69]; /* data_vdata */

    /* leakyrelu_237 */
    node[164]->input.tensors[0] = node[163]->output.tensors[0];

    /* trans_convolution_238 */
    node[165]->input.tensors[0] = node[164]->output.tensors[0];
    node[165]->input.tensors[1] = const_tensor[70]; /* data_vdata */

    /* leakyrelu_240 */
    node[166]->input.tensors[0] = node[165]->output.tensors[0];

    /* trans_convolution_241 */
    node[167]->input.tensors[0] = node[166]->output.tensors[0];
    node[167]->input.tensors[1] = const_tensor[71]; /* data_vdata */

    /* leakyrelu_243 */
    node[168]->input.tensors[0] = node[167]->output.tensors[0];

    /* trans_convolution_244 */
    node[169]->input.tensors[0] = node[168]->output.tensors[0];
    node[169]->input.tensors[1] = const_tensor[72]; /* data_vdata */

    /* leakyrelu_246 */
    node[170]->input.tensors[0] = node[169]->output.tensors[0];

    /* trans_convolution_247 */
    node[171]->input.tensors[0] = node[170]->output.tensors[0];
    node[171]->input.tensors[1] = const_tensor[73]; /* data_vdata */

    /* leakyrelu_249 */
    node[172]->input.tensors[0] = node[171]->output.tensors[0];

    /* trans_convolution_250 */
    node[173]->input.tensors[0] = node[172]->output.tensors[0];
    node[173]->input.tensors[1] = const_tensor[74]; /* data_vdata */



    graph->input.tensors[0] = norm_tensor[0];
    graph->output.tensors[0] = norm_tensor[1];
    graph->output.tensors[1] = norm_tensor[2];
    graph->output.tensors[2] = norm_tensor[3];


    status = vsi_nn_SetupGraph( graph, FALSE );
    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vsi_nn_DumpGraphToJson(graph);
    vnn_ReleaseYolov388( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateYolov388() */

void vnn_ReleaseYolov388
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseYolov388() */

