/****************************************************************************
*   Generated by ACUITY 3.13.0
*   Match ovxlib 1.0.10
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_facenet7d.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        _node->uid = (uint32_t)_uid; \
        if( NULL == _node ) {\
            goto error;\
        }\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (247)
#define NET_NORM_TENSOR_NUM     (2)
#define NET_CONST_TENSOR_NUM    (155)
#define NET_VIRTUAL_TENSOR_NUM  (247)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM + 32)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static void load_hw_config
    (
    vsi_nn_context_t ctx
    )
{
    ctx->config.evis.ver = VSI_NN_HW_EVIS_2;
    strncpy(ctx->config.target_name, "VIPNANOQI_PID0X7D", VSI_NN_MAX_TARGET_NAME);
}

static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    VSILOGI("Read %d data.", ret);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateFacenet7d
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx
    )
{
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;

    uint32_t   perm_1[] = { 2, 0, 1, 3 };
    uint32_t   shape_1[] = { 64, 1156, 2, 1 };
    uint32_t   shape_2[] = { 17, 17, 256, 1 };
    uint32_t   shape_3[] = { 64, 1156, 2, 1 };
    uint32_t   shape_4[] = { 17, 17, 256, 1 };
    uint32_t   shape_5[] = { 64, 1156, 2, 1 };
    uint32_t   shape_6[] = { 17, 17, 256, 1 };
    uint32_t   shape_7[] = { 64, 1156, 2, 1 };
    uint32_t   shape_8[] = { 17, 17, 256, 1 };
    uint32_t   shape_9[] = { 64, 1156, 2, 1 };
    uint32_t   shape_10[] = { 17, 17, 256, 1 };
    uint32_t   shape_11[] = { 64, 896, 2, 1 };
    uint32_t   shape_12[] = { 8, 8, 896, 1 };
    uint32_t   shape_13[] = { 64, 896, 2, 1 };
    uint32_t   shape_14[] = { 8, 8, 896, 1 };
    uint32_t   shape_15[] = { 64, 896, 2, 1 };
    uint32_t   shape_16[] = { 8, 8, 896, 1 };
    uint32_t   shape_17[] = { 64, 896, 2, 1 };
    uint32_t   shape_18[] = { 8, 8, 896, 1 };
    uint32_t   shape_19[] = { 64, 896, 2, 1 };
    uint32_t   shape_20[] = { 8, 8, 896, 1 };
    uint32_t   shape_21[] = { 64, 896, 2, 1 };
    uint32_t   shape_22[] = { 8, 8, 896, 1 };
    uint32_t   shape_23[] = { 64, 896, 2, 1 };
    uint32_t   shape_24[] = { 8, 8, 896, 1 };
    uint32_t   shape_25[] = { 64, 896, 2, 1 };
    uint32_t   shape_26[] = { 8, 8, 896, 1 };
    uint32_t   shape_27[] = { 64, 896, 2, 1 };
    uint32_t   shape_28[] = { 8, 8, 896, 1 };
    uint32_t   shape_29[] = { 64, 896, 2, 1 };
    uint32_t   shape_30[] = { 8, 8, 896, 1 };
    uint32_t   shape_31[] = { 64, 252, 2, 1 };
    uint32_t   shape_32[] = { 3, 3, 1792, 1 };
    uint32_t   shape_33[] = { 64, 252, 2, 1 };
    uint32_t   shape_34[] = { 3, 3, 1792, 1 };
    uint32_t   shape_35[] = { 64, 252, 2, 1 };
    uint32_t   shape_36[] = { 3, 3, 1792, 1 };
    uint32_t   shape_37[] = { 64, 252, 2, 1 };
    uint32_t   shape_38[] = { 3, 3, 1792, 1 };
    uint32_t   shape_39[] = { 64, 252, 2, 1 };
    uint32_t   shape_40[] = { 3, 3, 1792, 1 };
    uint32_t   shape_41[] = { 64, 252, 2, 1 };
    uint32_t   shape_42[] = { 3, 3, 1792, 1 };
    uint32_t   shape_43[] = { 1, 1, 1792, 1 };
    uint32_t   shape_44[] = { 1792, -1 };


    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
        load_hw_config(ctx);
    }
    else
    {
        ctx = in_ctx;
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 1 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_1a_3x3/convolution_468_InceptionResnetV1/Conv2d_1a_3x3/Relu_466
      var       - node[0]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[160, 160, 3, 1]]
      out_shape - [[79, 79, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_CONV_RELU, 2, 1, 466);
    node[0]->nn_param.conv2d.ksize[0] = 3;
    node[0]->nn_param.conv2d.ksize[1] = 3;
    node[0]->nn_param.conv2d.weights = 32;
    node[0]->nn_param.conv2d.stride[0] = 2;
    node[0]->nn_param.conv2d.stride[1] = 2;
    node[0]->nn_param.conv2d.pad[0] = 0;
    node[0]->nn_param.conv2d.pad[1] = 0;
    node[0]->nn_param.conv2d.pad[2] = 0;
    node[0]->nn_param.conv2d.pad[3] = 0;
    node[0]->nn_param.conv2d.group = 1;
    node[0]->nn_param.conv2d.dilation[0] = 1;
    node[0]->nn_param.conv2d.dilation[1] = 1;
    node[0]->nn_param.conv2d.multiplier = 0;
    node[0]->vx_param.has_relu = TRUE;
    node[0]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[0]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[0]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_2a_3x3/convolution_465_InceptionResnetV1/Conv2d_2a_3x3/Relu_463
      var       - node[1]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[79, 79, 32, 1]]
      out_shape - [[77, 77, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_CONV_RELU, 2, 1, 463);
    node[1]->nn_param.conv2d.ksize[0] = 3;
    node[1]->nn_param.conv2d.ksize[1] = 3;
    node[1]->nn_param.conv2d.weights = 32;
    node[1]->nn_param.conv2d.stride[0] = 1;
    node[1]->nn_param.conv2d.stride[1] = 1;
    node[1]->nn_param.conv2d.pad[0] = 0;
    node[1]->nn_param.conv2d.pad[1] = 0;
    node[1]->nn_param.conv2d.pad[2] = 0;
    node[1]->nn_param.conv2d.pad[3] = 0;
    node[1]->nn_param.conv2d.group = 1;
    node[1]->nn_param.conv2d.dilation[0] = 1;
    node[1]->nn_param.conv2d.dilation[1] = 1;
    node[1]->nn_param.conv2d.multiplier = 0;
    node[1]->vx_param.has_relu = TRUE;
    node[1]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[1]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[1]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_2b_3x3/convolution_462_InceptionResnetV1/Conv2d_2b_3x3/Relu_459
      var       - node[2]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[77, 77, 32, 1]]
      out_shape - [[77, 77, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_CONV_RELU, 2, 1, 459);
    node[2]->nn_param.conv2d.ksize[0] = 3;
    node[2]->nn_param.conv2d.ksize[1] = 3;
    node[2]->nn_param.conv2d.weights = 64;
    node[2]->nn_param.conv2d.stride[0] = 1;
    node[2]->nn_param.conv2d.stride[1] = 1;
    node[2]->nn_param.conv2d.pad[0] = 1;
    node[2]->nn_param.conv2d.pad[1] = 1;
    node[2]->nn_param.conv2d.pad[2] = 1;
    node[2]->nn_param.conv2d.pad[3] = 1;
    node[2]->nn_param.conv2d.group = 1;
    node[2]->nn_param.conv2d.dilation[0] = 1;
    node[2]->nn_param.conv2d.dilation[1] = 1;
    node[2]->nn_param.conv2d.multiplier = 0;
    node[2]->vx_param.has_relu = TRUE;
    node[2]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[2]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[2]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/MaxPool_3a_3x3/MaxPool_457
      var       - node[3]
      name      - InceptionResnetV1/MaxPool_3a_3x3/MaxPool
      operation - pooling
      in_shape  - [[77, 77, 64, 1]]
      out_shape - [[38, 38, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_POOL, 1, 1, 457);
    node[3]->nn_param.pool.ksize[0] = 3;
    node[3]->nn_param.pool.ksize[1] = 3;
    node[3]->nn_param.pool.stride[0] = 2;
    node[3]->nn_param.pool.stride[1] = 2;
    node[3]->nn_param.pool.pad[0] = 0;
    node[3]->nn_param.pool.pad[1] = 0;
    node[3]->nn_param.pool.pad[2] = 0;
    node[3]->nn_param.pool.pad[3] = 0;
    node[3]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[3]->nn_param.pool.round_type = VSI_NN_ROUND_FLOOR;
    node[3]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_3b_1x1/convolution_454_InceptionResnetV1/Conv2d_3b_1x1/Relu_444
      var       - node[4]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[38, 38, 64, 1]]
      out_shape - [[38, 38, 80, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV_RELU, 2, 1, 444);
    node[4]->nn_param.conv2d.ksize[0] = 1;
    node[4]->nn_param.conv2d.ksize[1] = 1;
    node[4]->nn_param.conv2d.weights = 80;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 0;
    node[4]->nn_param.conv2d.pad[1] = 0;
    node[4]->nn_param.conv2d.pad[2] = 0;
    node[4]->nn_param.conv2d.pad[3] = 0;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->nn_param.conv2d.multiplier = 0;
    node[4]->vx_param.has_relu = TRUE;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_4a_3x3/convolution_438_InceptionResnetV1/Conv2d_4a_3x3/Relu_420
      var       - node[5]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[38, 38, 80, 1]]
      out_shape - [[36, 36, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_CONV_RELU, 2, 1, 420);
    node[5]->nn_param.conv2d.ksize[0] = 3;
    node[5]->nn_param.conv2d.ksize[1] = 3;
    node[5]->nn_param.conv2d.weights = 192;
    node[5]->nn_param.conv2d.stride[0] = 1;
    node[5]->nn_param.conv2d.stride[1] = 1;
    node[5]->nn_param.conv2d.pad[0] = 0;
    node[5]->nn_param.conv2d.pad[1] = 0;
    node[5]->nn_param.conv2d.pad[2] = 0;
    node[5]->nn_param.conv2d.pad[3] = 0;
    node[5]->nn_param.conv2d.group = 1;
    node[5]->nn_param.conv2d.dilation[0] = 1;
    node[5]->nn_param.conv2d.dilation[1] = 1;
    node[5]->nn_param.conv2d.multiplier = 0;
    node[5]->vx_param.has_relu = TRUE;
    node[5]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[5]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[5]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Conv2d_4b_3x3/convolution_409_InceptionResnetV1/Conv2d_4b_3x3/Relu_386
      var       - node[6]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[36, 36, 192, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_CONV_RELU, 2, 1, 386);
    node[6]->nn_param.conv2d.ksize[0] = 3;
    node[6]->nn_param.conv2d.ksize[1] = 3;
    node[6]->nn_param.conv2d.weights = 256;
    node[6]->nn_param.conv2d.stride[0] = 2;
    node[6]->nn_param.conv2d.stride[1] = 2;
    node[6]->nn_param.conv2d.pad[0] = 0;
    node[6]->nn_param.conv2d.pad[1] = 0;
    node[6]->nn_param.conv2d.pad[2] = 0;
    node[6]->nn_param.conv2d.pad[3] = 0;
    node[6]->nn_param.conv2d.group = 1;
    node[6]->nn_param.conv2d.dilation[0] = 1;
    node[6]->nn_param.conv2d.dilation[1] = 1;
    node[6]->nn_param.conv2d.multiplier = 0;
    node[6]->vx_param.has_relu = TRUE;
    node[6]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[6]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/convolution_436_InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/Relu_418
      var       - node[7]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_CONV_RELU, 2, 1, 418);
    node[7]->nn_param.conv2d.ksize[0] = 1;
    node[7]->nn_param.conv2d.ksize[1] = 1;
    node[7]->nn_param.conv2d.weights = 32;
    node[7]->nn_param.conv2d.stride[0] = 1;
    node[7]->nn_param.conv2d.stride[1] = 1;
    node[7]->nn_param.conv2d.pad[0] = 0;
    node[7]->nn_param.conv2d.pad[1] = 0;
    node[7]->nn_param.conv2d.pad[2] = 0;
    node[7]->nn_param.conv2d.pad[3] = 0;
    node[7]->nn_param.conv2d.group = 1;
    node[7]->nn_param.conv2d.dilation[0] = 1;
    node[7]->nn_param.conv2d.dilation[1] = 1;
    node[7]->nn_param.conv2d.multiplier = 0;
    node[7]->vx_param.has_relu = TRUE;
    node[7]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[7]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[7]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/convolution_452_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/Relu_442
      var       - node[8]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_CONV_RELU, 2, 1, 442);
    node[8]->nn_param.conv2d.ksize[0] = 1;
    node[8]->nn_param.conv2d.ksize[1] = 1;
    node[8]->nn_param.conv2d.weights = 32;
    node[8]->nn_param.conv2d.stride[0] = 1;
    node[8]->nn_param.conv2d.stride[1] = 1;
    node[8]->nn_param.conv2d.pad[0] = 0;
    node[8]->nn_param.conv2d.pad[1] = 0;
    node[8]->nn_param.conv2d.pad[2] = 0;
    node[8]->nn_param.conv2d.pad[3] = 0;
    node[8]->nn_param.conv2d.group = 1;
    node[8]->nn_param.conv2d.dilation[0] = 1;
    node[8]->nn_param.conv2d.dilation[1] = 1;
    node[8]->nn_param.conv2d.multiplier = 0;
    node[8]->vx_param.has_relu = TRUE;
    node[8]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[8]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[8]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/convolution_460_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/Relu_456
      var       - node[9]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_CONV_RELU, 2, 1, 456);
    node[9]->nn_param.conv2d.ksize[0] = 1;
    node[9]->nn_param.conv2d.ksize[1] = 1;
    node[9]->nn_param.conv2d.weights = 32;
    node[9]->nn_param.conv2d.stride[0] = 1;
    node[9]->nn_param.conv2d.stride[1] = 1;
    node[9]->nn_param.conv2d.pad[0] = 0;
    node[9]->nn_param.conv2d.pad[1] = 0;
    node[9]->nn_param.conv2d.pad[2] = 0;
    node[9]->nn_param.conv2d.pad[3] = 0;
    node[9]->nn_param.conv2d.group = 1;
    node[9]->nn_param.conv2d.dilation[0] = 1;
    node[9]->nn_param.conv2d.dilation[1] = 1;
    node[9]->nn_param.conv2d.multiplier = 0;
    node[9]->vx_param.has_relu = TRUE;
    node[9]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[9]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[9]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/convolution_435_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/Relu_417
      var       - node[10]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_CONV_RELU, 2, 1, 417);
    node[10]->nn_param.conv2d.ksize[0] = 3;
    node[10]->nn_param.conv2d.ksize[1] = 3;
    node[10]->nn_param.conv2d.weights = 32;
    node[10]->nn_param.conv2d.stride[0] = 1;
    node[10]->nn_param.conv2d.stride[1] = 1;
    node[10]->nn_param.conv2d.pad[0] = 1;
    node[10]->nn_param.conv2d.pad[1] = 1;
    node[10]->nn_param.conv2d.pad[2] = 1;
    node[10]->nn_param.conv2d.pad[3] = 1;
    node[10]->nn_param.conv2d.group = 1;
    node[10]->nn_param.conv2d.dilation[0] = 1;
    node[10]->nn_param.conv2d.dilation[1] = 1;
    node[10]->nn_param.conv2d.multiplier = 0;
    node[10]->vx_param.has_relu = TRUE;
    node[10]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[10]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[10]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/convolution_453_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/Relu_443
      var       - node[11]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_CONV_RELU, 2, 1, 443);
    node[11]->nn_param.conv2d.ksize[0] = 3;
    node[11]->nn_param.conv2d.ksize[1] = 3;
    node[11]->nn_param.conv2d.weights = 32;
    node[11]->nn_param.conv2d.stride[0] = 1;
    node[11]->nn_param.conv2d.stride[1] = 1;
    node[11]->nn_param.conv2d.pad[0] = 1;
    node[11]->nn_param.conv2d.pad[1] = 1;
    node[11]->nn_param.conv2d.pad[2] = 1;
    node[11]->nn_param.conv2d.pad[3] = 1;
    node[11]->nn_param.conv2d.group = 1;
    node[11]->nn_param.conv2d.dilation[0] = 1;
    node[11]->nn_param.conv2d.dilation[1] = 1;
    node[11]->nn_param.conv2d.multiplier = 0;
    node[11]->vx_param.has_relu = TRUE;
    node[11]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[11]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[11]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/convolution_437_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/Relu_419
      var       - node[12]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_CONV_RELU, 2, 1, 419);
    node[12]->nn_param.conv2d.ksize[0] = 3;
    node[12]->nn_param.conv2d.ksize[1] = 3;
    node[12]->nn_param.conv2d.weights = 32;
    node[12]->nn_param.conv2d.stride[0] = 1;
    node[12]->nn_param.conv2d.stride[1] = 1;
    node[12]->nn_param.conv2d.pad[0] = 1;
    node[12]->nn_param.conv2d.pad[1] = 1;
    node[12]->nn_param.conv2d.pad[2] = 1;
    node[12]->nn_param.conv2d.pad[3] = 1;
    node[12]->nn_param.conv2d.group = 1;
    node[12]->nn_param.conv2d.dilation[0] = 1;
    node[12]->nn_param.conv2d.dilation[1] = 1;
    node[12]->nn_param.conv2d.multiplier = 0;
    node[12]->vx_param.has_relu = TRUE;
    node[12]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[12]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[12]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/concat_408
      var       - node[13]
      name      - InceptionResnetV1/Repeat/block35_1/concat
      operation - concat
      in_shape  - [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
      out_shape - [[17, 17, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_CONCAT, 3, 1, 408);
    node[13]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat/block35_1/Conv2d_1x1/convolution_396
      var       - node[14]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 96, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_CONV_RELU, 2, 1, 396);
    node[14]->nn_param.conv2d.ksize[0] = 1;
    node[14]->nn_param.conv2d.ksize[1] = 1;
    node[14]->nn_param.conv2d.weights = 256;
    node[14]->nn_param.conv2d.stride[0] = 1;
    node[14]->nn_param.conv2d.stride[1] = 1;
    node[14]->nn_param.conv2d.pad[0] = 0;
    node[14]->nn_param.conv2d.pad[1] = 0;
    node[14]->nn_param.conv2d.pad[2] = 0;
    node[14]->nn_param.conv2d.pad[3] = 0;
    node[14]->nn_param.conv2d.group = 1;
    node[14]->nn_param.conv2d.dilation[0] = 1;
    node[14]->nn_param.conv2d.dilation[1] = 1;
    node[14]->nn_param.conv2d.multiplier = 0;
    node[14]->vx_param.has_relu = FALSE;
    node[14]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[14]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[14]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/add_375_concat_246
      var       - node[15]
      name      - InceptionResnetV1/Repeat/block35_1/add_375_concat
      operation - concat
      in_shape  - [[17, 17, 256, 1]]
                  [[17, 17, 256, 1]]
      out_shape - [[17, 17, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_CONCAT, 2, 1, 246);
    node[15]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/add_375_reshape_247
      var       - node[16]
      name      - InceptionResnetV1/Repeat/block35_1/add_375_reshape
      operation - reshape
      in_shape  - [[17, 17, 512, 1]]
      out_shape - [[64, 1156, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_RESHAPE, 1, 1, 247);
    node[16]->nn_param.reshape.size = shape_1;
    node[16]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/add_375_conv_250_InceptionResnetV1/Repeat/block35_1/Relu_365
      var       - node[17]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 1156, 2, 1]]
      out_shape - [[64, 1156, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_CONV_RELU, 2, 1, 365);
    node[17]->nn_param.conv2d.ksize[0] = 1;
    node[17]->nn_param.conv2d.ksize[1] = 1;
    node[17]->nn_param.conv2d.weights = 1;
    node[17]->nn_param.conv2d.stride[0] = 1;
    node[17]->nn_param.conv2d.stride[1] = 1;
    node[17]->nn_param.conv2d.pad[0] = 0;
    node[17]->nn_param.conv2d.pad[1] = 0;
    node[17]->nn_param.conv2d.pad[2] = 0;
    node[17]->nn_param.conv2d.pad[3] = 0;
    node[17]->nn_param.conv2d.group = 1;
    node[17]->nn_param.conv2d.dilation[0] = 1;
    node[17]->nn_param.conv2d.dilation[1] = 1;
    node[17]->nn_param.conv2d.multiplier = 0;
    node[17]->vx_param.has_relu = TRUE;
    node[17]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[17]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[17]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_1/add_375_reshape_252
      var       - node[18]
      name      - InceptionResnetV1/Repeat/block35_1/add_375_reshape
      operation - reshape
      in_shape  - [[64, 1156, 1, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_RESHAPE, 1, 1, 252);
    node[18]->nn_param.reshape.size = shape_2;
    node[18]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/convolution_414_InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/Relu_393
      var       - node[19]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_CONV_RELU, 2, 1, 393);
    node[19]->nn_param.conv2d.ksize[0] = 1;
    node[19]->nn_param.conv2d.ksize[1] = 1;
    node[19]->nn_param.conv2d.weights = 32;
    node[19]->nn_param.conv2d.stride[0] = 1;
    node[19]->nn_param.conv2d.stride[1] = 1;
    node[19]->nn_param.conv2d.pad[0] = 0;
    node[19]->nn_param.conv2d.pad[1] = 0;
    node[19]->nn_param.conv2d.pad[2] = 0;
    node[19]->nn_param.conv2d.pad[3] = 0;
    node[19]->nn_param.conv2d.group = 1;
    node[19]->nn_param.conv2d.dilation[0] = 1;
    node[19]->nn_param.conv2d.dilation[1] = 1;
    node[19]->nn_param.conv2d.multiplier = 0;
    node[19]->vx_param.has_relu = TRUE;
    node[19]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[19]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[19]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/convolution_440_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/Relu_424
      var       - node[20]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_CONV_RELU, 2, 1, 424);
    node[20]->nn_param.conv2d.ksize[0] = 1;
    node[20]->nn_param.conv2d.ksize[1] = 1;
    node[20]->nn_param.conv2d.weights = 32;
    node[20]->nn_param.conv2d.stride[0] = 1;
    node[20]->nn_param.conv2d.stride[1] = 1;
    node[20]->nn_param.conv2d.pad[0] = 0;
    node[20]->nn_param.conv2d.pad[1] = 0;
    node[20]->nn_param.conv2d.pad[2] = 0;
    node[20]->nn_param.conv2d.pad[3] = 0;
    node[20]->nn_param.conv2d.group = 1;
    node[20]->nn_param.conv2d.dilation[0] = 1;
    node[20]->nn_param.conv2d.dilation[1] = 1;
    node[20]->nn_param.conv2d.multiplier = 0;
    node[20]->vx_param.has_relu = TRUE;
    node[20]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[20]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[20]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/convolution_455_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/Relu_446
      var       - node[21]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_CONV_RELU, 2, 1, 446);
    node[21]->nn_param.conv2d.ksize[0] = 1;
    node[21]->nn_param.conv2d.ksize[1] = 1;
    node[21]->nn_param.conv2d.weights = 32;
    node[21]->nn_param.conv2d.stride[0] = 1;
    node[21]->nn_param.conv2d.stride[1] = 1;
    node[21]->nn_param.conv2d.pad[0] = 0;
    node[21]->nn_param.conv2d.pad[1] = 0;
    node[21]->nn_param.conv2d.pad[2] = 0;
    node[21]->nn_param.conv2d.pad[3] = 0;
    node[21]->nn_param.conv2d.group = 1;
    node[21]->nn_param.conv2d.dilation[0] = 1;
    node[21]->nn_param.conv2d.dilation[1] = 1;
    node[21]->nn_param.conv2d.multiplier = 0;
    node[21]->vx_param.has_relu = TRUE;
    node[21]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[21]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[21]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/convolution_415_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/Relu_394
      var       - node[22]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[22], VSI_NN_OP_CONV_RELU, 2, 1, 394);
    node[22]->nn_param.conv2d.ksize[0] = 3;
    node[22]->nn_param.conv2d.ksize[1] = 3;
    node[22]->nn_param.conv2d.weights = 32;
    node[22]->nn_param.conv2d.stride[0] = 1;
    node[22]->nn_param.conv2d.stride[1] = 1;
    node[22]->nn_param.conv2d.pad[0] = 1;
    node[22]->nn_param.conv2d.pad[1] = 1;
    node[22]->nn_param.conv2d.pad[2] = 1;
    node[22]->nn_param.conv2d.pad[3] = 1;
    node[22]->nn_param.conv2d.group = 1;
    node[22]->nn_param.conv2d.dilation[0] = 1;
    node[22]->nn_param.conv2d.dilation[1] = 1;
    node[22]->nn_param.conv2d.multiplier = 0;
    node[22]->vx_param.has_relu = TRUE;
    node[22]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[22]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[22]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/convolution_441_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/Relu_425
      var       - node[23]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[23], VSI_NN_OP_CONV_RELU, 2, 1, 425);
    node[23]->nn_param.conv2d.ksize[0] = 3;
    node[23]->nn_param.conv2d.ksize[1] = 3;
    node[23]->nn_param.conv2d.weights = 32;
    node[23]->nn_param.conv2d.stride[0] = 1;
    node[23]->nn_param.conv2d.stride[1] = 1;
    node[23]->nn_param.conv2d.pad[0] = 1;
    node[23]->nn_param.conv2d.pad[1] = 1;
    node[23]->nn_param.conv2d.pad[2] = 1;
    node[23]->nn_param.conv2d.pad[3] = 1;
    node[23]->nn_param.conv2d.group = 1;
    node[23]->nn_param.conv2d.dilation[0] = 1;
    node[23]->nn_param.conv2d.dilation[1] = 1;
    node[23]->nn_param.conv2d.multiplier = 0;
    node[23]->vx_param.has_relu = TRUE;
    node[23]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[23]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[23]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/convolution_416_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/Relu_395
      var       - node[24]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[24], VSI_NN_OP_CONV_RELU, 2, 1, 395);
    node[24]->nn_param.conv2d.ksize[0] = 3;
    node[24]->nn_param.conv2d.ksize[1] = 3;
    node[24]->nn_param.conv2d.weights = 32;
    node[24]->nn_param.conv2d.stride[0] = 1;
    node[24]->nn_param.conv2d.stride[1] = 1;
    node[24]->nn_param.conv2d.pad[0] = 1;
    node[24]->nn_param.conv2d.pad[1] = 1;
    node[24]->nn_param.conv2d.pad[2] = 1;
    node[24]->nn_param.conv2d.pad[3] = 1;
    node[24]->nn_param.conv2d.group = 1;
    node[24]->nn_param.conv2d.dilation[0] = 1;
    node[24]->nn_param.conv2d.dilation[1] = 1;
    node[24]->nn_param.conv2d.multiplier = 0;
    node[24]->vx_param.has_relu = TRUE;
    node[24]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[24]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[24]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/concat_384
      var       - node[25]
      name      - InceptionResnetV1/Repeat/block35_2/concat
      operation - concat
      in_shape  - [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
      out_shape - [[17, 17, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[25], VSI_NN_OP_CONCAT, 3, 1, 384);
    node[25]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat/block35_2/Conv2d_1x1/convolution_373
      var       - node[26]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 96, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[26], VSI_NN_OP_CONV_RELU, 2, 1, 373);
    node[26]->nn_param.conv2d.ksize[0] = 1;
    node[26]->nn_param.conv2d.ksize[1] = 1;
    node[26]->nn_param.conv2d.weights = 256;
    node[26]->nn_param.conv2d.stride[0] = 1;
    node[26]->nn_param.conv2d.stride[1] = 1;
    node[26]->nn_param.conv2d.pad[0] = 0;
    node[26]->nn_param.conv2d.pad[1] = 0;
    node[26]->nn_param.conv2d.pad[2] = 0;
    node[26]->nn_param.conv2d.pad[3] = 0;
    node[26]->nn_param.conv2d.group = 1;
    node[26]->nn_param.conv2d.dilation[0] = 1;
    node[26]->nn_param.conv2d.dilation[1] = 1;
    node[26]->nn_param.conv2d.multiplier = 0;
    node[26]->vx_param.has_relu = FALSE;
    node[26]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[26]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[26]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/add_355_concat_231
      var       - node[27]
      name      - InceptionResnetV1/Repeat/block35_2/add_355_concat
      operation - concat
      in_shape  - [[17, 17, 256, 1]]
                  [[17, 17, 256, 1]]
      out_shape - [[17, 17, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[27], VSI_NN_OP_CONCAT, 2, 1, 231);
    node[27]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/add_355_reshape_232
      var       - node[28]
      name      - InceptionResnetV1/Repeat/block35_2/add_355_reshape
      operation - reshape
      in_shape  - [[17, 17, 512, 1]]
      out_shape - [[64, 1156, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[28], VSI_NN_OP_RESHAPE, 1, 1, 232);
    node[28]->nn_param.reshape.size = shape_3;
    node[28]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/add_355_conv_233_InceptionResnetV1/Repeat/block35_2/Relu_344
      var       - node[29]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 1156, 2, 1]]
      out_shape - [[64, 1156, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[29], VSI_NN_OP_CONV_RELU, 2, 1, 344);
    node[29]->nn_param.conv2d.ksize[0] = 1;
    node[29]->nn_param.conv2d.ksize[1] = 1;
    node[29]->nn_param.conv2d.weights = 1;
    node[29]->nn_param.conv2d.stride[0] = 1;
    node[29]->nn_param.conv2d.stride[1] = 1;
    node[29]->nn_param.conv2d.pad[0] = 0;
    node[29]->nn_param.conv2d.pad[1] = 0;
    node[29]->nn_param.conv2d.pad[2] = 0;
    node[29]->nn_param.conv2d.pad[3] = 0;
    node[29]->nn_param.conv2d.group = 1;
    node[29]->nn_param.conv2d.dilation[0] = 1;
    node[29]->nn_param.conv2d.dilation[1] = 1;
    node[29]->nn_param.conv2d.multiplier = 0;
    node[29]->vx_param.has_relu = TRUE;
    node[29]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[29]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[29]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_2/add_355_reshape_242
      var       - node[30]
      name      - InceptionResnetV1/Repeat/block35_2/add_355_reshape
      operation - reshape
      in_shape  - [[64, 1156, 1, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[30], VSI_NN_OP_RESHAPE, 1, 1, 242);
    node[30]->nn_param.reshape.size = shape_4;
    node[30]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/convolution_401_InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/Relu_378
      var       - node[31]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[31], VSI_NN_OP_CONV_RELU, 2, 1, 378);
    node[31]->nn_param.conv2d.ksize[0] = 1;
    node[31]->nn_param.conv2d.ksize[1] = 1;
    node[31]->nn_param.conv2d.weights = 32;
    node[31]->nn_param.conv2d.stride[0] = 1;
    node[31]->nn_param.conv2d.stride[1] = 1;
    node[31]->nn_param.conv2d.pad[0] = 0;
    node[31]->nn_param.conv2d.pad[1] = 0;
    node[31]->nn_param.conv2d.pad[2] = 0;
    node[31]->nn_param.conv2d.pad[3] = 0;
    node[31]->nn_param.conv2d.group = 1;
    node[31]->nn_param.conv2d.dilation[0] = 1;
    node[31]->nn_param.conv2d.dilation[1] = 1;
    node[31]->nn_param.conv2d.multiplier = 0;
    node[31]->vx_param.has_relu = TRUE;
    node[31]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[31]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[31]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/convolution_431_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/Relu_411
      var       - node[32]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[32], VSI_NN_OP_CONV_RELU, 2, 1, 411);
    node[32]->nn_param.conv2d.ksize[0] = 1;
    node[32]->nn_param.conv2d.ksize[1] = 1;
    node[32]->nn_param.conv2d.weights = 32;
    node[32]->nn_param.conv2d.stride[0] = 1;
    node[32]->nn_param.conv2d.stride[1] = 1;
    node[32]->nn_param.conv2d.pad[0] = 0;
    node[32]->nn_param.conv2d.pad[1] = 0;
    node[32]->nn_param.conv2d.pad[2] = 0;
    node[32]->nn_param.conv2d.pad[3] = 0;
    node[32]->nn_param.conv2d.group = 1;
    node[32]->nn_param.conv2d.dilation[0] = 1;
    node[32]->nn_param.conv2d.dilation[1] = 1;
    node[32]->nn_param.conv2d.multiplier = 0;
    node[32]->vx_param.has_relu = TRUE;
    node[32]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[32]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[32]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/convolution_450_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/Relu_439
      var       - node[33]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[33], VSI_NN_OP_CONV_RELU, 2, 1, 439);
    node[33]->nn_param.conv2d.ksize[0] = 1;
    node[33]->nn_param.conv2d.ksize[1] = 1;
    node[33]->nn_param.conv2d.weights = 32;
    node[33]->nn_param.conv2d.stride[0] = 1;
    node[33]->nn_param.conv2d.stride[1] = 1;
    node[33]->nn_param.conv2d.pad[0] = 0;
    node[33]->nn_param.conv2d.pad[1] = 0;
    node[33]->nn_param.conv2d.pad[2] = 0;
    node[33]->nn_param.conv2d.pad[3] = 0;
    node[33]->nn_param.conv2d.group = 1;
    node[33]->nn_param.conv2d.dilation[0] = 1;
    node[33]->nn_param.conv2d.dilation[1] = 1;
    node[33]->nn_param.conv2d.multiplier = 0;
    node[33]->vx_param.has_relu = TRUE;
    node[33]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[33]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[33]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/convolution_400_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/Relu_377
      var       - node[34]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[34], VSI_NN_OP_CONV_RELU, 2, 1, 377);
    node[34]->nn_param.conv2d.ksize[0] = 3;
    node[34]->nn_param.conv2d.ksize[1] = 3;
    node[34]->nn_param.conv2d.weights = 32;
    node[34]->nn_param.conv2d.stride[0] = 1;
    node[34]->nn_param.conv2d.stride[1] = 1;
    node[34]->nn_param.conv2d.pad[0] = 1;
    node[34]->nn_param.conv2d.pad[1] = 1;
    node[34]->nn_param.conv2d.pad[2] = 1;
    node[34]->nn_param.conv2d.pad[3] = 1;
    node[34]->nn_param.conv2d.group = 1;
    node[34]->nn_param.conv2d.dilation[0] = 1;
    node[34]->nn_param.conv2d.dilation[1] = 1;
    node[34]->nn_param.conv2d.multiplier = 0;
    node[34]->vx_param.has_relu = TRUE;
    node[34]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[34]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[34]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/convolution_430_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/Relu_410
      var       - node[35]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[35], VSI_NN_OP_CONV_RELU, 2, 1, 410);
    node[35]->nn_param.conv2d.ksize[0] = 3;
    node[35]->nn_param.conv2d.ksize[1] = 3;
    node[35]->nn_param.conv2d.weights = 32;
    node[35]->nn_param.conv2d.stride[0] = 1;
    node[35]->nn_param.conv2d.stride[1] = 1;
    node[35]->nn_param.conv2d.pad[0] = 1;
    node[35]->nn_param.conv2d.pad[1] = 1;
    node[35]->nn_param.conv2d.pad[2] = 1;
    node[35]->nn_param.conv2d.pad[3] = 1;
    node[35]->nn_param.conv2d.group = 1;
    node[35]->nn_param.conv2d.dilation[0] = 1;
    node[35]->nn_param.conv2d.dilation[1] = 1;
    node[35]->nn_param.conv2d.multiplier = 0;
    node[35]->vx_param.has_relu = TRUE;
    node[35]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[35]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[35]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/convolution_399_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/Relu_376
      var       - node[36]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[36], VSI_NN_OP_CONV_RELU, 2, 1, 376);
    node[36]->nn_param.conv2d.ksize[0] = 3;
    node[36]->nn_param.conv2d.ksize[1] = 3;
    node[36]->nn_param.conv2d.weights = 32;
    node[36]->nn_param.conv2d.stride[0] = 1;
    node[36]->nn_param.conv2d.stride[1] = 1;
    node[36]->nn_param.conv2d.pad[0] = 1;
    node[36]->nn_param.conv2d.pad[1] = 1;
    node[36]->nn_param.conv2d.pad[2] = 1;
    node[36]->nn_param.conv2d.pad[3] = 1;
    node[36]->nn_param.conv2d.group = 1;
    node[36]->nn_param.conv2d.dilation[0] = 1;
    node[36]->nn_param.conv2d.dilation[1] = 1;
    node[36]->nn_param.conv2d.multiplier = 0;
    node[36]->vx_param.has_relu = TRUE;
    node[36]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[36]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[36]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/concat_366
      var       - node[37]
      name      - InceptionResnetV1/Repeat/block35_3/concat
      operation - concat
      in_shape  - [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
      out_shape - [[17, 17, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[37], VSI_NN_OP_CONCAT, 3, 1, 366);
    node[37]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat/block35_3/Conv2d_1x1/convolution_356
      var       - node[38]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 96, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[38], VSI_NN_OP_CONV_RELU, 2, 1, 356);
    node[38]->nn_param.conv2d.ksize[0] = 1;
    node[38]->nn_param.conv2d.ksize[1] = 1;
    node[38]->nn_param.conv2d.weights = 256;
    node[38]->nn_param.conv2d.stride[0] = 1;
    node[38]->nn_param.conv2d.stride[1] = 1;
    node[38]->nn_param.conv2d.pad[0] = 0;
    node[38]->nn_param.conv2d.pad[1] = 0;
    node[38]->nn_param.conv2d.pad[2] = 0;
    node[38]->nn_param.conv2d.pad[3] = 0;
    node[38]->nn_param.conv2d.group = 1;
    node[38]->nn_param.conv2d.dilation[0] = 1;
    node[38]->nn_param.conv2d.dilation[1] = 1;
    node[38]->nn_param.conv2d.multiplier = 0;
    node[38]->vx_param.has_relu = FALSE;
    node[38]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[38]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[38]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/add_336_concat_218
      var       - node[39]
      name      - InceptionResnetV1/Repeat/block35_3/add_336_concat
      operation - concat
      in_shape  - [[17, 17, 256, 1]]
                  [[17, 17, 256, 1]]
      out_shape - [[17, 17, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[39], VSI_NN_OP_CONCAT, 2, 1, 218);
    node[39]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/add_336_reshape_220
      var       - node[40]
      name      - InceptionResnetV1/Repeat/block35_3/add_336_reshape
      operation - reshape
      in_shape  - [[17, 17, 512, 1]]
      out_shape - [[64, 1156, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[40], VSI_NN_OP_RESHAPE, 1, 1, 220);
    node[40]->nn_param.reshape.size = shape_5;
    node[40]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/add_336_conv_224_InceptionResnetV1/Repeat/block35_3/Relu_326
      var       - node[41]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 1156, 2, 1]]
      out_shape - [[64, 1156, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[41], VSI_NN_OP_CONV_RELU, 2, 1, 326);
    node[41]->nn_param.conv2d.ksize[0] = 1;
    node[41]->nn_param.conv2d.ksize[1] = 1;
    node[41]->nn_param.conv2d.weights = 1;
    node[41]->nn_param.conv2d.stride[0] = 1;
    node[41]->nn_param.conv2d.stride[1] = 1;
    node[41]->nn_param.conv2d.pad[0] = 0;
    node[41]->nn_param.conv2d.pad[1] = 0;
    node[41]->nn_param.conv2d.pad[2] = 0;
    node[41]->nn_param.conv2d.pad[3] = 0;
    node[41]->nn_param.conv2d.group = 1;
    node[41]->nn_param.conv2d.dilation[0] = 1;
    node[41]->nn_param.conv2d.dilation[1] = 1;
    node[41]->nn_param.conv2d.multiplier = 0;
    node[41]->vx_param.has_relu = TRUE;
    node[41]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[41]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[41]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_3/add_336_reshape_229
      var       - node[42]
      name      - InceptionResnetV1/Repeat/block35_3/add_336_reshape
      operation - reshape
      in_shape  - [[64, 1156, 1, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[42], VSI_NN_OP_RESHAPE, 1, 1, 229);
    node[42]->nn_param.reshape.size = shape_6;
    node[42]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/convolution_371_InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/Relu_353
      var       - node[43]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[43], VSI_NN_OP_CONV_RELU, 2, 1, 353);
    node[43]->nn_param.conv2d.ksize[0] = 1;
    node[43]->nn_param.conv2d.ksize[1] = 1;
    node[43]->nn_param.conv2d.weights = 32;
    node[43]->nn_param.conv2d.stride[0] = 1;
    node[43]->nn_param.conv2d.stride[1] = 1;
    node[43]->nn_param.conv2d.pad[0] = 0;
    node[43]->nn_param.conv2d.pad[1] = 0;
    node[43]->nn_param.conv2d.pad[2] = 0;
    node[43]->nn_param.conv2d.pad[3] = 0;
    node[43]->nn_param.conv2d.group = 1;
    node[43]->nn_param.conv2d.dilation[0] = 1;
    node[43]->nn_param.conv2d.dilation[1] = 1;
    node[43]->nn_param.conv2d.multiplier = 0;
    node[43]->vx_param.has_relu = TRUE;
    node[43]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[43]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[43]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/convolution_403_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/Relu_382
      var       - node[44]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[44], VSI_NN_OP_CONV_RELU, 2, 1, 382);
    node[44]->nn_param.conv2d.ksize[0] = 1;
    node[44]->nn_param.conv2d.ksize[1] = 1;
    node[44]->nn_param.conv2d.weights = 32;
    node[44]->nn_param.conv2d.stride[0] = 1;
    node[44]->nn_param.conv2d.stride[1] = 1;
    node[44]->nn_param.conv2d.pad[0] = 0;
    node[44]->nn_param.conv2d.pad[1] = 0;
    node[44]->nn_param.conv2d.pad[2] = 0;
    node[44]->nn_param.conv2d.pad[3] = 0;
    node[44]->nn_param.conv2d.group = 1;
    node[44]->nn_param.conv2d.dilation[0] = 1;
    node[44]->nn_param.conv2d.dilation[1] = 1;
    node[44]->nn_param.conv2d.multiplier = 0;
    node[44]->vx_param.has_relu = TRUE;
    node[44]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[44]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[44]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/convolution_432_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/Relu_413
      var       - node[45]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[45], VSI_NN_OP_CONV_RELU, 2, 1, 413);
    node[45]->nn_param.conv2d.ksize[0] = 1;
    node[45]->nn_param.conv2d.ksize[1] = 1;
    node[45]->nn_param.conv2d.weights = 32;
    node[45]->nn_param.conv2d.stride[0] = 1;
    node[45]->nn_param.conv2d.stride[1] = 1;
    node[45]->nn_param.conv2d.pad[0] = 0;
    node[45]->nn_param.conv2d.pad[1] = 0;
    node[45]->nn_param.conv2d.pad[2] = 0;
    node[45]->nn_param.conv2d.pad[3] = 0;
    node[45]->nn_param.conv2d.group = 1;
    node[45]->nn_param.conv2d.dilation[0] = 1;
    node[45]->nn_param.conv2d.dilation[1] = 1;
    node[45]->nn_param.conv2d.multiplier = 0;
    node[45]->vx_param.has_relu = TRUE;
    node[45]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[45]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[45]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/convolution_370_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/Relu_352
      var       - node[46]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[46], VSI_NN_OP_CONV_RELU, 2, 1, 352);
    node[46]->nn_param.conv2d.ksize[0] = 3;
    node[46]->nn_param.conv2d.ksize[1] = 3;
    node[46]->nn_param.conv2d.weights = 32;
    node[46]->nn_param.conv2d.stride[0] = 1;
    node[46]->nn_param.conv2d.stride[1] = 1;
    node[46]->nn_param.conv2d.pad[0] = 1;
    node[46]->nn_param.conv2d.pad[1] = 1;
    node[46]->nn_param.conv2d.pad[2] = 1;
    node[46]->nn_param.conv2d.pad[3] = 1;
    node[46]->nn_param.conv2d.group = 1;
    node[46]->nn_param.conv2d.dilation[0] = 1;
    node[46]->nn_param.conv2d.dilation[1] = 1;
    node[46]->nn_param.conv2d.multiplier = 0;
    node[46]->vx_param.has_relu = TRUE;
    node[46]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[46]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[46]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/convolution_404_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/Relu_383
      var       - node[47]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[47], VSI_NN_OP_CONV_RELU, 2, 1, 383);
    node[47]->nn_param.conv2d.ksize[0] = 3;
    node[47]->nn_param.conv2d.ksize[1] = 3;
    node[47]->nn_param.conv2d.weights = 32;
    node[47]->nn_param.conv2d.stride[0] = 1;
    node[47]->nn_param.conv2d.stride[1] = 1;
    node[47]->nn_param.conv2d.pad[0] = 1;
    node[47]->nn_param.conv2d.pad[1] = 1;
    node[47]->nn_param.conv2d.pad[2] = 1;
    node[47]->nn_param.conv2d.pad[3] = 1;
    node[47]->nn_param.conv2d.group = 1;
    node[47]->nn_param.conv2d.dilation[0] = 1;
    node[47]->nn_param.conv2d.dilation[1] = 1;
    node[47]->nn_param.conv2d.multiplier = 0;
    node[47]->vx_param.has_relu = TRUE;
    node[47]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[47]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[47]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/convolution_372_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/Relu_354
      var       - node[48]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[48], VSI_NN_OP_CONV_RELU, 2, 1, 354);
    node[48]->nn_param.conv2d.ksize[0] = 3;
    node[48]->nn_param.conv2d.ksize[1] = 3;
    node[48]->nn_param.conv2d.weights = 32;
    node[48]->nn_param.conv2d.stride[0] = 1;
    node[48]->nn_param.conv2d.stride[1] = 1;
    node[48]->nn_param.conv2d.pad[0] = 1;
    node[48]->nn_param.conv2d.pad[1] = 1;
    node[48]->nn_param.conv2d.pad[2] = 1;
    node[48]->nn_param.conv2d.pad[3] = 1;
    node[48]->nn_param.conv2d.group = 1;
    node[48]->nn_param.conv2d.dilation[0] = 1;
    node[48]->nn_param.conv2d.dilation[1] = 1;
    node[48]->nn_param.conv2d.multiplier = 0;
    node[48]->vx_param.has_relu = TRUE;
    node[48]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[48]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[48]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/concat_343
      var       - node[49]
      name      - InceptionResnetV1/Repeat/block35_4/concat
      operation - concat
      in_shape  - [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
      out_shape - [[17, 17, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[49], VSI_NN_OP_CONCAT, 3, 1, 343);
    node[49]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat/block35_4/Conv2d_1x1/convolution_334
      var       - node[50]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 96, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[50], VSI_NN_OP_CONV_RELU, 2, 1, 334);
    node[50]->nn_param.conv2d.ksize[0] = 1;
    node[50]->nn_param.conv2d.ksize[1] = 1;
    node[50]->nn_param.conv2d.weights = 256;
    node[50]->nn_param.conv2d.stride[0] = 1;
    node[50]->nn_param.conv2d.stride[1] = 1;
    node[50]->nn_param.conv2d.pad[0] = 0;
    node[50]->nn_param.conv2d.pad[1] = 0;
    node[50]->nn_param.conv2d.pad[2] = 0;
    node[50]->nn_param.conv2d.pad[3] = 0;
    node[50]->nn_param.conv2d.group = 1;
    node[50]->nn_param.conv2d.dilation[0] = 1;
    node[50]->nn_param.conv2d.dilation[1] = 1;
    node[50]->nn_param.conv2d.multiplier = 0;
    node[50]->vx_param.has_relu = FALSE;
    node[50]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[50]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[50]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/add_317_concat_206
      var       - node[51]
      name      - InceptionResnetV1/Repeat/block35_4/add_317_concat
      operation - concat
      in_shape  - [[17, 17, 256, 1]]
                  [[17, 17, 256, 1]]
      out_shape - [[17, 17, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[51], VSI_NN_OP_CONCAT, 2, 1, 206);
    node[51]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/add_317_reshape_215
      var       - node[52]
      name      - InceptionResnetV1/Repeat/block35_4/add_317_reshape
      operation - reshape
      in_shape  - [[17, 17, 512, 1]]
      out_shape - [[64, 1156, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[52], VSI_NN_OP_RESHAPE, 1, 1, 215);
    node[52]->nn_param.reshape.size = shape_7;
    node[52]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/add_317_conv_216_InceptionResnetV1/Repeat/block35_4/Relu_308
      var       - node[53]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 1156, 2, 1]]
      out_shape - [[64, 1156, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[53], VSI_NN_OP_CONV_RELU, 2, 1, 308);
    node[53]->nn_param.conv2d.ksize[0] = 1;
    node[53]->nn_param.conv2d.ksize[1] = 1;
    node[53]->nn_param.conv2d.weights = 1;
    node[53]->nn_param.conv2d.stride[0] = 1;
    node[53]->nn_param.conv2d.stride[1] = 1;
    node[53]->nn_param.conv2d.pad[0] = 0;
    node[53]->nn_param.conv2d.pad[1] = 0;
    node[53]->nn_param.conv2d.pad[2] = 0;
    node[53]->nn_param.conv2d.pad[3] = 0;
    node[53]->nn_param.conv2d.group = 1;
    node[53]->nn_param.conv2d.dilation[0] = 1;
    node[53]->nn_param.conv2d.dilation[1] = 1;
    node[53]->nn_param.conv2d.multiplier = 0;
    node[53]->vx_param.has_relu = TRUE;
    node[53]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[53]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[53]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_4/add_317_reshape_217
      var       - node[54]
      name      - InceptionResnetV1/Repeat/block35_4/add_317_reshape
      operation - reshape
      in_shape  - [[64, 1156, 1, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[54], VSI_NN_OP_RESHAPE, 1, 1, 217);
    node[54]->nn_param.reshape.size = shape_8;
    node[54]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/convolution_350_InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/Relu_332
      var       - node[55]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[55], VSI_NN_OP_CONV_RELU, 2, 1, 332);
    node[55]->nn_param.conv2d.ksize[0] = 1;
    node[55]->nn_param.conv2d.ksize[1] = 1;
    node[55]->nn_param.conv2d.weights = 32;
    node[55]->nn_param.conv2d.stride[0] = 1;
    node[55]->nn_param.conv2d.stride[1] = 1;
    node[55]->nn_param.conv2d.pad[0] = 0;
    node[55]->nn_param.conv2d.pad[1] = 0;
    node[55]->nn_param.conv2d.pad[2] = 0;
    node[55]->nn_param.conv2d.pad[3] = 0;
    node[55]->nn_param.conv2d.group = 1;
    node[55]->nn_param.conv2d.dilation[0] = 1;
    node[55]->nn_param.conv2d.dilation[1] = 1;
    node[55]->nn_param.conv2d.multiplier = 0;
    node[55]->vx_param.has_relu = TRUE;
    node[55]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[55]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[55]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/convolution_380_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/Relu_359
      var       - node[56]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[56], VSI_NN_OP_CONV_RELU, 2, 1, 359);
    node[56]->nn_param.conv2d.ksize[0] = 1;
    node[56]->nn_param.conv2d.ksize[1] = 1;
    node[56]->nn_param.conv2d.weights = 32;
    node[56]->nn_param.conv2d.stride[0] = 1;
    node[56]->nn_param.conv2d.stride[1] = 1;
    node[56]->nn_param.conv2d.pad[0] = 0;
    node[56]->nn_param.conv2d.pad[1] = 0;
    node[56]->nn_param.conv2d.pad[2] = 0;
    node[56]->nn_param.conv2d.pad[3] = 0;
    node[56]->nn_param.conv2d.group = 1;
    node[56]->nn_param.conv2d.dilation[0] = 1;
    node[56]->nn_param.conv2d.dilation[1] = 1;
    node[56]->nn_param.conv2d.multiplier = 0;
    node[56]->vx_param.has_relu = TRUE;
    node[56]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[56]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[56]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/convolution_412_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/Relu_390
      var       - node[57]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[57], VSI_NN_OP_CONV_RELU, 2, 1, 390);
    node[57]->nn_param.conv2d.ksize[0] = 1;
    node[57]->nn_param.conv2d.ksize[1] = 1;
    node[57]->nn_param.conv2d.weights = 32;
    node[57]->nn_param.conv2d.stride[0] = 1;
    node[57]->nn_param.conv2d.stride[1] = 1;
    node[57]->nn_param.conv2d.pad[0] = 0;
    node[57]->nn_param.conv2d.pad[1] = 0;
    node[57]->nn_param.conv2d.pad[2] = 0;
    node[57]->nn_param.conv2d.pad[3] = 0;
    node[57]->nn_param.conv2d.group = 1;
    node[57]->nn_param.conv2d.dilation[0] = 1;
    node[57]->nn_param.conv2d.dilation[1] = 1;
    node[57]->nn_param.conv2d.multiplier = 0;
    node[57]->vx_param.has_relu = TRUE;
    node[57]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[57]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[57]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/convolution_349_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/Relu_331
      var       - node[58]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[58], VSI_NN_OP_CONV_RELU, 2, 1, 331);
    node[58]->nn_param.conv2d.ksize[0] = 3;
    node[58]->nn_param.conv2d.ksize[1] = 3;
    node[58]->nn_param.conv2d.weights = 32;
    node[58]->nn_param.conv2d.stride[0] = 1;
    node[58]->nn_param.conv2d.stride[1] = 1;
    node[58]->nn_param.conv2d.pad[0] = 1;
    node[58]->nn_param.conv2d.pad[1] = 1;
    node[58]->nn_param.conv2d.pad[2] = 1;
    node[58]->nn_param.conv2d.pad[3] = 1;
    node[58]->nn_param.conv2d.group = 1;
    node[58]->nn_param.conv2d.dilation[0] = 1;
    node[58]->nn_param.conv2d.dilation[1] = 1;
    node[58]->nn_param.conv2d.multiplier = 0;
    node[58]->vx_param.has_relu = TRUE;
    node[58]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[58]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[58]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/convolution_381_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/Relu_360
      var       - node[59]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[59], VSI_NN_OP_CONV_RELU, 2, 1, 360);
    node[59]->nn_param.conv2d.ksize[0] = 3;
    node[59]->nn_param.conv2d.ksize[1] = 3;
    node[59]->nn_param.conv2d.weights = 32;
    node[59]->nn_param.conv2d.stride[0] = 1;
    node[59]->nn_param.conv2d.stride[1] = 1;
    node[59]->nn_param.conv2d.pad[0] = 1;
    node[59]->nn_param.conv2d.pad[1] = 1;
    node[59]->nn_param.conv2d.pad[2] = 1;
    node[59]->nn_param.conv2d.pad[3] = 1;
    node[59]->nn_param.conv2d.group = 1;
    node[59]->nn_param.conv2d.dilation[0] = 1;
    node[59]->nn_param.conv2d.dilation[1] = 1;
    node[59]->nn_param.conv2d.multiplier = 0;
    node[59]->vx_param.has_relu = TRUE;
    node[59]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[59]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[59]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/convolution_351_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/Relu_333
      var       - node[60]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 32, 1]]
      out_shape - [[17, 17, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[60], VSI_NN_OP_CONV_RELU, 2, 1, 333);
    node[60]->nn_param.conv2d.ksize[0] = 3;
    node[60]->nn_param.conv2d.ksize[1] = 3;
    node[60]->nn_param.conv2d.weights = 32;
    node[60]->nn_param.conv2d.stride[0] = 1;
    node[60]->nn_param.conv2d.stride[1] = 1;
    node[60]->nn_param.conv2d.pad[0] = 1;
    node[60]->nn_param.conv2d.pad[1] = 1;
    node[60]->nn_param.conv2d.pad[2] = 1;
    node[60]->nn_param.conv2d.pad[3] = 1;
    node[60]->nn_param.conv2d.group = 1;
    node[60]->nn_param.conv2d.dilation[0] = 1;
    node[60]->nn_param.conv2d.dilation[1] = 1;
    node[60]->nn_param.conv2d.multiplier = 0;
    node[60]->vx_param.has_relu = TRUE;
    node[60]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[60]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[60]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/concat_324
      var       - node[61]
      name      - InceptionResnetV1/Repeat/block35_5/concat
      operation - concat
      in_shape  - [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
                  [[17, 17, 32, 1]]
      out_shape - [[17, 17, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[61], VSI_NN_OP_CONCAT, 3, 1, 324);
    node[61]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat/block35_5/Conv2d_1x1/convolution_316
      var       - node[62]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 96, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[62], VSI_NN_OP_CONV_RELU, 2, 1, 316);
    node[62]->nn_param.conv2d.ksize[0] = 1;
    node[62]->nn_param.conv2d.ksize[1] = 1;
    node[62]->nn_param.conv2d.weights = 256;
    node[62]->nn_param.conv2d.stride[0] = 1;
    node[62]->nn_param.conv2d.stride[1] = 1;
    node[62]->nn_param.conv2d.pad[0] = 0;
    node[62]->nn_param.conv2d.pad[1] = 0;
    node[62]->nn_param.conv2d.pad[2] = 0;
    node[62]->nn_param.conv2d.pad[3] = 0;
    node[62]->nn_param.conv2d.group = 1;
    node[62]->nn_param.conv2d.dilation[0] = 1;
    node[62]->nn_param.conv2d.dilation[1] = 1;
    node[62]->nn_param.conv2d.multiplier = 0;
    node[62]->vx_param.has_relu = FALSE;
    node[62]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[62]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[62]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/add_300_concat_195
      var       - node[63]
      name      - InceptionResnetV1/Repeat/block35_5/add_300_concat
      operation - concat
      in_shape  - [[17, 17, 256, 1]]
                  [[17, 17, 256, 1]]
      out_shape - [[17, 17, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[63], VSI_NN_OP_CONCAT, 2, 1, 195);
    node[63]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/add_300_reshape_196
      var       - node[64]
      name      - InceptionResnetV1/Repeat/block35_5/add_300_reshape
      operation - reshape
      in_shape  - [[17, 17, 512, 1]]
      out_shape - [[64, 1156, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[64], VSI_NN_OP_RESHAPE, 1, 1, 196);
    node[64]->nn_param.reshape.size = shape_9;
    node[64]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/add_300_conv_197_InceptionResnetV1/Repeat/block35_5/Relu_291
      var       - node[65]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 1156, 2, 1]]
      out_shape - [[64, 1156, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[65], VSI_NN_OP_CONV_RELU, 2, 1, 291);
    node[65]->nn_param.conv2d.ksize[0] = 1;
    node[65]->nn_param.conv2d.ksize[1] = 1;
    node[65]->nn_param.conv2d.weights = 1;
    node[65]->nn_param.conv2d.stride[0] = 1;
    node[65]->nn_param.conv2d.stride[1] = 1;
    node[65]->nn_param.conv2d.pad[0] = 0;
    node[65]->nn_param.conv2d.pad[1] = 0;
    node[65]->nn_param.conv2d.pad[2] = 0;
    node[65]->nn_param.conv2d.pad[3] = 0;
    node[65]->nn_param.conv2d.group = 1;
    node[65]->nn_param.conv2d.dilation[0] = 1;
    node[65]->nn_param.conv2d.dilation[1] = 1;
    node[65]->nn_param.conv2d.multiplier = 0;
    node[65]->vx_param.has_relu = TRUE;
    node[65]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[65]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[65]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat/block35_5/add_300_reshape_202
      var       - node[66]
      name      - InceptionResnetV1/Repeat/block35_5/add_300_reshape
      operation - reshape
      in_shape  - [[64, 1156, 1, 1]]
      out_shape - [[17, 17, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[66], VSI_NN_OP_RESHAPE, 1, 1, 202);
    node[66]->nn_param.reshape.size = shape_10;
    node[66]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool_280
      var       - node[67]
      name      - InceptionResnetV1/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool
      operation - pooling
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[67], VSI_NN_OP_POOL, 1, 1, 280);
    node[67]->nn_param.pool.ksize[0] = 3;
    node[67]->nn_param.pool.ksize[1] = 3;
    node[67]->nn_param.pool.stride[0] = 2;
    node[67]->nn_param.pool.stride[1] = 2;
    node[67]->nn_param.pool.pad[0] = 0;
    node[67]->nn_param.pool.pad[1] = 0;
    node[67]->nn_param.pool.pad[2] = 0;
    node[67]->nn_param.pool.pad[3] = 0;
    node[67]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[67]->nn_param.pool.round_type = VSI_NN_ROUND_FLOOR;
    node[67]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/convolution_299_InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/Relu_279
      var       - node[68]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[8, 8, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[68], VSI_NN_OP_CONV_RELU, 2, 1, 279);
    node[68]->nn_param.conv2d.ksize[0] = 3;
    node[68]->nn_param.conv2d.ksize[1] = 3;
    node[68]->nn_param.conv2d.weights = 384;
    node[68]->nn_param.conv2d.stride[0] = 2;
    node[68]->nn_param.conv2d.stride[1] = 2;
    node[68]->nn_param.conv2d.pad[0] = 0;
    node[68]->nn_param.conv2d.pad[1] = 0;
    node[68]->nn_param.conv2d.pad[2] = 0;
    node[68]->nn_param.conv2d.pad[3] = 0;
    node[68]->nn_param.conv2d.group = 1;
    node[68]->nn_param.conv2d.dilation[0] = 1;
    node[68]->nn_param.conv2d.dilation[1] = 1;
    node[68]->nn_param.conv2d.multiplier = 0;
    node[68]->vx_param.has_relu = TRUE;
    node[68]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[68]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[68]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/convolution_348_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/Relu_330
      var       - node[69]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 256, 1]]
      out_shape - [[17, 17, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[69], VSI_NN_OP_CONV_RELU, 2, 1, 330);
    node[69]->nn_param.conv2d.ksize[0] = 1;
    node[69]->nn_param.conv2d.ksize[1] = 1;
    node[69]->nn_param.conv2d.weights = 192;
    node[69]->nn_param.conv2d.stride[0] = 1;
    node[69]->nn_param.conv2d.stride[1] = 1;
    node[69]->nn_param.conv2d.pad[0] = 0;
    node[69]->nn_param.conv2d.pad[1] = 0;
    node[69]->nn_param.conv2d.pad[2] = 0;
    node[69]->nn_param.conv2d.pad[3] = 0;
    node[69]->nn_param.conv2d.group = 1;
    node[69]->nn_param.conv2d.dilation[0] = 1;
    node[69]->nn_param.conv2d.dilation[1] = 1;
    node[69]->nn_param.conv2d.multiplier = 0;
    node[69]->vx_param.has_relu = TRUE;
    node[69]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[69]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[69]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/convolution_323_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/Relu_306
      var       - node[70]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 192, 1]]
      out_shape - [[17, 17, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[70], VSI_NN_OP_CONV_RELU, 2, 1, 306);
    node[70]->nn_param.conv2d.ksize[0] = 3;
    node[70]->nn_param.conv2d.ksize[1] = 3;
    node[70]->nn_param.conv2d.weights = 192;
    node[70]->nn_param.conv2d.stride[0] = 1;
    node[70]->nn_param.conv2d.stride[1] = 1;
    node[70]->nn_param.conv2d.pad[0] = 1;
    node[70]->nn_param.conv2d.pad[1] = 1;
    node[70]->nn_param.conv2d.pad[2] = 1;
    node[70]->nn_param.conv2d.pad[3] = 1;
    node[70]->nn_param.conv2d.group = 1;
    node[70]->nn_param.conv2d.dilation[0] = 1;
    node[70]->nn_param.conv2d.dilation[1] = 1;
    node[70]->nn_param.conv2d.multiplier = 0;
    node[70]->vx_param.has_relu = TRUE;
    node[70]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[70]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[70]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/convolution_298_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/Relu_278
      var       - node[71]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[17, 17, 192, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[71], VSI_NN_OP_CONV_RELU, 2, 1, 278);
    node[71]->nn_param.conv2d.ksize[0] = 3;
    node[71]->nn_param.conv2d.ksize[1] = 3;
    node[71]->nn_param.conv2d.weights = 256;
    node[71]->nn_param.conv2d.stride[0] = 2;
    node[71]->nn_param.conv2d.stride[1] = 2;
    node[71]->nn_param.conv2d.pad[0] = 0;
    node[71]->nn_param.conv2d.pad[1] = 0;
    node[71]->nn_param.conv2d.pad[2] = 0;
    node[71]->nn_param.conv2d.pad[3] = 0;
    node[71]->nn_param.conv2d.group = 1;
    node[71]->nn_param.conv2d.dilation[0] = 1;
    node[71]->nn_param.conv2d.dilation[1] = 1;
    node[71]->nn_param.conv2d.multiplier = 0;
    node[71]->vx_param.has_relu = TRUE;
    node[71]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[71]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[71]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_6a/concat_269
      var       - node[72]
      name      - InceptionResnetV1/Mixed_6a/concat
      operation - concat
      in_shape  - [[8, 8, 384, 1]]
                  [[8, 8, 256, 1]]
                  [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[72], VSI_NN_OP_CONCAT, 3, 1, 269);
    node[72]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/convolution_319_InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/Relu_302
      var       - node[73]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[73], VSI_NN_OP_CONV_RELU, 2, 1, 302);
    node[73]->nn_param.conv2d.ksize[0] = 1;
    node[73]->nn_param.conv2d.ksize[1] = 1;
    node[73]->nn_param.conv2d.weights = 128;
    node[73]->nn_param.conv2d.stride[0] = 1;
    node[73]->nn_param.conv2d.stride[1] = 1;
    node[73]->nn_param.conv2d.pad[0] = 0;
    node[73]->nn_param.conv2d.pad[1] = 0;
    node[73]->nn_param.conv2d.pad[2] = 0;
    node[73]->nn_param.conv2d.pad[3] = 0;
    node[73]->nn_param.conv2d.group = 1;
    node[73]->nn_param.conv2d.dilation[0] = 1;
    node[73]->nn_param.conv2d.dilation[1] = 1;
    node[73]->nn_param.conv2d.multiplier = 0;
    node[73]->vx_param.has_relu = TRUE;
    node[73]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[73]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[73]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/convolution_379_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/Relu_358
      var       - node[74]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[74], VSI_NN_OP_CONV_RELU, 2, 1, 358);
    node[74]->nn_param.conv2d.ksize[0] = 1;
    node[74]->nn_param.conv2d.ksize[1] = 1;
    node[74]->nn_param.conv2d.weights = 128;
    node[74]->nn_param.conv2d.stride[0] = 1;
    node[74]->nn_param.conv2d.stride[1] = 1;
    node[74]->nn_param.conv2d.pad[0] = 0;
    node[74]->nn_param.conv2d.pad[1] = 0;
    node[74]->nn_param.conv2d.pad[2] = 0;
    node[74]->nn_param.conv2d.pad[3] = 0;
    node[74]->nn_param.conv2d.group = 1;
    node[74]->nn_param.conv2d.dilation[0] = 1;
    node[74]->nn_param.conv2d.dilation[1] = 1;
    node[74]->nn_param.conv2d.multiplier = 0;
    node[74]->vx_param.has_relu = TRUE;
    node[74]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[74]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[74]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/convolution_346_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/Relu_327
      var       - node[75]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[75], VSI_NN_OP_CONV_RELU, 2, 1, 327);
    node[75]->nn_param.conv2d.ksize[0] = 7;
    node[75]->nn_param.conv2d.ksize[1] = 7;
    node[75]->nn_param.conv2d.weights = 128;
    node[75]->nn_param.conv2d.stride[0] = 1;
    node[75]->nn_param.conv2d.stride[1] = 1;
    node[75]->nn_param.conv2d.pad[0] = 3;
    node[75]->nn_param.conv2d.pad[1] = 3;
    node[75]->nn_param.conv2d.pad[2] = 3;
    node[75]->nn_param.conv2d.pad[3] = 3;
    node[75]->nn_param.conv2d.group = 1;
    node[75]->nn_param.conv2d.dilation[0] = 1;
    node[75]->nn_param.conv2d.dilation[1] = 1;
    node[75]->nn_param.conv2d.multiplier = 0;
    node[75]->vx_param.has_relu = TRUE;
    node[75]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[75]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[75]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/convolution_318_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/Relu_301
      var       - node[76]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[76], VSI_NN_OP_CONV_RELU, 2, 1, 301);
    node[76]->nn_param.conv2d.ksize[0] = 1;
    node[76]->nn_param.conv2d.ksize[1] = 7;
    node[76]->nn_param.conv2d.weights = 128;
    node[76]->nn_param.conv2d.stride[0] = 1;
    node[76]->nn_param.conv2d.stride[1] = 1;
    node[76]->nn_param.conv2d.pad[0] = 0;
    node[76]->nn_param.conv2d.pad[1] = 0;
    node[76]->nn_param.conv2d.pad[2] = 3;
    node[76]->nn_param.conv2d.pad[3] = 3;
    node[76]->nn_param.conv2d.group = 1;
    node[76]->nn_param.conv2d.dilation[0] = 1;
    node[76]->nn_param.conv2d.dilation[1] = 1;
    node[76]->nn_param.conv2d.multiplier = 0;
    node[76]->vx_param.has_relu = TRUE;
    node[76]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[76]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[76]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/concat_292
      var       - node[77]
      name      - InceptionResnetV1/Repeat_1/block17_1/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[77], VSI_NN_OP_CONCAT, 2, 1, 292);
    node[77]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_1/Conv2d_1x1/convolution_281
      var       - node[78]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[78], VSI_NN_OP_CONV_RELU, 2, 1, 281);
    node[78]->nn_param.conv2d.ksize[0] = 1;
    node[78]->nn_param.conv2d.ksize[1] = 1;
    node[78]->nn_param.conv2d.weights = 896;
    node[78]->nn_param.conv2d.stride[0] = 1;
    node[78]->nn_param.conv2d.stride[1] = 1;
    node[78]->nn_param.conv2d.pad[0] = 0;
    node[78]->nn_param.conv2d.pad[1] = 0;
    node[78]->nn_param.conv2d.pad[2] = 0;
    node[78]->nn_param.conv2d.pad[3] = 0;
    node[78]->nn_param.conv2d.group = 1;
    node[78]->nn_param.conv2d.dilation[0] = 1;
    node[78]->nn_param.conv2d.dilation[1] = 1;
    node[78]->nn_param.conv2d.multiplier = 0;
    node[78]->vx_param.has_relu = FALSE;
    node[78]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[78]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[78]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/add_261_concat_182
      var       - node[79]
      name      - InceptionResnetV1/Repeat_1/block17_1/add_261_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[79], VSI_NN_OP_CONCAT, 2, 1, 182);
    node[79]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_190
      var       - node[80]
      name      - InceptionResnetV1/Repeat_1/block17_1/add_261_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[80], VSI_NN_OP_RESHAPE, 1, 1, 190);
    node[80]->nn_param.reshape.size = shape_11;
    node[80]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/add_261_conv_192_InceptionResnetV1/Repeat_1/block17_1/Relu_251
      var       - node[81]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[81], VSI_NN_OP_CONV_RELU, 2, 1, 251);
    node[81]->nn_param.conv2d.ksize[0] = 1;
    node[81]->nn_param.conv2d.ksize[1] = 1;
    node[81]->nn_param.conv2d.weights = 1;
    node[81]->nn_param.conv2d.stride[0] = 1;
    node[81]->nn_param.conv2d.stride[1] = 1;
    node[81]->nn_param.conv2d.pad[0] = 0;
    node[81]->nn_param.conv2d.pad[1] = 0;
    node[81]->nn_param.conv2d.pad[2] = 0;
    node[81]->nn_param.conv2d.pad[3] = 0;
    node[81]->nn_param.conv2d.group = 1;
    node[81]->nn_param.conv2d.dilation[0] = 1;
    node[81]->nn_param.conv2d.dilation[1] = 1;
    node[81]->nn_param.conv2d.multiplier = 0;
    node[81]->vx_param.has_relu = TRUE;
    node[81]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[81]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[81]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_193
      var       - node[82]
      name      - InceptionResnetV1/Repeat_1/block17_1/add_261_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[82], VSI_NN_OP_RESHAPE, 1, 1, 193);
    node[82]->nn_param.reshape.size = shape_12;
    node[82]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/convolution_296_InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/Relu_276
      var       - node[83]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[83], VSI_NN_OP_CONV_RELU, 2, 1, 276);
    node[83]->nn_param.conv2d.ksize[0] = 1;
    node[83]->nn_param.conv2d.ksize[1] = 1;
    node[83]->nn_param.conv2d.weights = 128;
    node[83]->nn_param.conv2d.stride[0] = 1;
    node[83]->nn_param.conv2d.stride[1] = 1;
    node[83]->nn_param.conv2d.pad[0] = 0;
    node[83]->nn_param.conv2d.pad[1] = 0;
    node[83]->nn_param.conv2d.pad[2] = 0;
    node[83]->nn_param.conv2d.pad[3] = 0;
    node[83]->nn_param.conv2d.group = 1;
    node[83]->nn_param.conv2d.dilation[0] = 1;
    node[83]->nn_param.conv2d.dilation[1] = 1;
    node[83]->nn_param.conv2d.multiplier = 0;
    node[83]->vx_param.has_relu = TRUE;
    node[83]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[83]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[83]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/convolution_347_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/Relu_329
      var       - node[84]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[84], VSI_NN_OP_CONV_RELU, 2, 1, 329);
    node[84]->nn_param.conv2d.ksize[0] = 1;
    node[84]->nn_param.conv2d.ksize[1] = 1;
    node[84]->nn_param.conv2d.weights = 128;
    node[84]->nn_param.conv2d.stride[0] = 1;
    node[84]->nn_param.conv2d.stride[1] = 1;
    node[84]->nn_param.conv2d.pad[0] = 0;
    node[84]->nn_param.conv2d.pad[1] = 0;
    node[84]->nn_param.conv2d.pad[2] = 0;
    node[84]->nn_param.conv2d.pad[3] = 0;
    node[84]->nn_param.conv2d.group = 1;
    node[84]->nn_param.conv2d.dilation[0] = 1;
    node[84]->nn_param.conv2d.dilation[1] = 1;
    node[84]->nn_param.conv2d.multiplier = 0;
    node[84]->vx_param.has_relu = TRUE;
    node[84]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[84]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[84]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/convolution_322_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/Relu_305
      var       - node[85]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[85], VSI_NN_OP_CONV_RELU, 2, 1, 305);
    node[85]->nn_param.conv2d.ksize[0] = 7;
    node[85]->nn_param.conv2d.ksize[1] = 7;
    node[85]->nn_param.conv2d.weights = 128;
    node[85]->nn_param.conv2d.stride[0] = 1;
    node[85]->nn_param.conv2d.stride[1] = 1;
    node[85]->nn_param.conv2d.pad[0] = 3;
    node[85]->nn_param.conv2d.pad[1] = 3;
    node[85]->nn_param.conv2d.pad[2] = 3;
    node[85]->nn_param.conv2d.pad[3] = 3;
    node[85]->nn_param.conv2d.group = 1;
    node[85]->nn_param.conv2d.dilation[0] = 1;
    node[85]->nn_param.conv2d.dilation[1] = 1;
    node[85]->nn_param.conv2d.multiplier = 0;
    node[85]->vx_param.has_relu = TRUE;
    node[85]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[85]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[85]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/convolution_297_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/Relu_277
      var       - node[86]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[86], VSI_NN_OP_CONV_RELU, 2, 1, 277);
    node[86]->nn_param.conv2d.ksize[0] = 1;
    node[86]->nn_param.conv2d.ksize[1] = 7;
    node[86]->nn_param.conv2d.weights = 128;
    node[86]->nn_param.conv2d.stride[0] = 1;
    node[86]->nn_param.conv2d.stride[1] = 1;
    node[86]->nn_param.conv2d.pad[0] = 0;
    node[86]->nn_param.conv2d.pad[1] = 0;
    node[86]->nn_param.conv2d.pad[2] = 3;
    node[86]->nn_param.conv2d.pad[3] = 3;
    node[86]->nn_param.conv2d.group = 1;
    node[86]->nn_param.conv2d.dilation[0] = 1;
    node[86]->nn_param.conv2d.dilation[1] = 1;
    node[86]->nn_param.conv2d.multiplier = 0;
    node[86]->vx_param.has_relu = TRUE;
    node[86]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[86]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[86]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/concat_268
      var       - node[87]
      name      - InceptionResnetV1/Repeat_1/block17_2/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[87], VSI_NN_OP_CONCAT, 2, 1, 268);
    node[87]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_2/Conv2d_1x1/convolution_260
      var       - node[88]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[88], VSI_NN_OP_CONV_RELU, 2, 1, 260);
    node[88]->nn_param.conv2d.ksize[0] = 1;
    node[88]->nn_param.conv2d.ksize[1] = 1;
    node[88]->nn_param.conv2d.weights = 896;
    node[88]->nn_param.conv2d.stride[0] = 1;
    node[88]->nn_param.conv2d.stride[1] = 1;
    node[88]->nn_param.conv2d.pad[0] = 0;
    node[88]->nn_param.conv2d.pad[1] = 0;
    node[88]->nn_param.conv2d.pad[2] = 0;
    node[88]->nn_param.conv2d.pad[3] = 0;
    node[88]->nn_param.conv2d.group = 1;
    node[88]->nn_param.conv2d.dilation[0] = 1;
    node[88]->nn_param.conv2d.dilation[1] = 1;
    node[88]->nn_param.conv2d.multiplier = 0;
    node[88]->vx_param.has_relu = FALSE;
    node[88]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[88]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[88]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/add_243_concat_169
      var       - node[89]
      name      - InceptionResnetV1/Repeat_1/block17_2/add_243_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[89], VSI_NN_OP_CONCAT, 2, 1, 169);
    node[89]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_173
      var       - node[90]
      name      - InceptionResnetV1/Repeat_1/block17_2/add_243_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[90], VSI_NN_OP_RESHAPE, 1, 1, 173);
    node[90]->nn_param.reshape.size = shape_13;
    node[90]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/add_243_conv_178_InceptionResnetV1/Repeat_1/block17_2/Relu_234
      var       - node[91]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[91], VSI_NN_OP_CONV_RELU, 2, 1, 234);
    node[91]->nn_param.conv2d.ksize[0] = 1;
    node[91]->nn_param.conv2d.ksize[1] = 1;
    node[91]->nn_param.conv2d.weights = 1;
    node[91]->nn_param.conv2d.stride[0] = 1;
    node[91]->nn_param.conv2d.stride[1] = 1;
    node[91]->nn_param.conv2d.pad[0] = 0;
    node[91]->nn_param.conv2d.pad[1] = 0;
    node[91]->nn_param.conv2d.pad[2] = 0;
    node[91]->nn_param.conv2d.pad[3] = 0;
    node[91]->nn_param.conv2d.group = 1;
    node[91]->nn_param.conv2d.dilation[0] = 1;
    node[91]->nn_param.conv2d.dilation[1] = 1;
    node[91]->nn_param.conv2d.multiplier = 0;
    node[91]->vx_param.has_relu = TRUE;
    node[91]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[91]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[91]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_179
      var       - node[92]
      name      - InceptionResnetV1/Repeat_1/block17_2/add_243_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[92], VSI_NN_OP_RESHAPE, 1, 1, 179);
    node[92]->nn_param.reshape.size = shape_14;
    node[92]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/convolution_275_InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/Relu_258
      var       - node[93]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[93], VSI_NN_OP_CONV_RELU, 2, 1, 258);
    node[93]->nn_param.conv2d.ksize[0] = 1;
    node[93]->nn_param.conv2d.ksize[1] = 1;
    node[93]->nn_param.conv2d.weights = 128;
    node[93]->nn_param.conv2d.stride[0] = 1;
    node[93]->nn_param.conv2d.stride[1] = 1;
    node[93]->nn_param.conv2d.pad[0] = 0;
    node[93]->nn_param.conv2d.pad[1] = 0;
    node[93]->nn_param.conv2d.pad[2] = 0;
    node[93]->nn_param.conv2d.pad[3] = 0;
    node[93]->nn_param.conv2d.group = 1;
    node[93]->nn_param.conv2d.dilation[0] = 1;
    node[93]->nn_param.conv2d.dilation[1] = 1;
    node[93]->nn_param.conv2d.multiplier = 0;
    node[93]->vx_param.has_relu = TRUE;
    node[93]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[93]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[93]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/convolution_328_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/Relu_312
      var       - node[94]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[94], VSI_NN_OP_CONV_RELU, 2, 1, 312);
    node[94]->nn_param.conv2d.ksize[0] = 1;
    node[94]->nn_param.conv2d.ksize[1] = 1;
    node[94]->nn_param.conv2d.weights = 128;
    node[94]->nn_param.conv2d.stride[0] = 1;
    node[94]->nn_param.conv2d.stride[1] = 1;
    node[94]->nn_param.conv2d.pad[0] = 0;
    node[94]->nn_param.conv2d.pad[1] = 0;
    node[94]->nn_param.conv2d.pad[2] = 0;
    node[94]->nn_param.conv2d.pad[3] = 0;
    node[94]->nn_param.conv2d.group = 1;
    node[94]->nn_param.conv2d.dilation[0] = 1;
    node[94]->nn_param.conv2d.dilation[1] = 1;
    node[94]->nn_param.conv2d.multiplier = 0;
    node[94]->vx_param.has_relu = TRUE;
    node[94]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[94]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[94]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/convolution_304_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/Relu_286
      var       - node[95]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[95], VSI_NN_OP_CONV_RELU, 2, 1, 286);
    node[95]->nn_param.conv2d.ksize[0] = 7;
    node[95]->nn_param.conv2d.ksize[1] = 7;
    node[95]->nn_param.conv2d.weights = 128;
    node[95]->nn_param.conv2d.stride[0] = 1;
    node[95]->nn_param.conv2d.stride[1] = 1;
    node[95]->nn_param.conv2d.pad[0] = 3;
    node[95]->nn_param.conv2d.pad[1] = 3;
    node[95]->nn_param.conv2d.pad[2] = 3;
    node[95]->nn_param.conv2d.pad[3] = 3;
    node[95]->nn_param.conv2d.group = 1;
    node[95]->nn_param.conv2d.dilation[0] = 1;
    node[95]->nn_param.conv2d.dilation[1] = 1;
    node[95]->nn_param.conv2d.multiplier = 0;
    node[95]->vx_param.has_relu = TRUE;
    node[95]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[95]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[95]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/convolution_274_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/Relu_257
      var       - node[96]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[96], VSI_NN_OP_CONV_RELU, 2, 1, 257);
    node[96]->nn_param.conv2d.ksize[0] = 1;
    node[96]->nn_param.conv2d.ksize[1] = 7;
    node[96]->nn_param.conv2d.weights = 128;
    node[96]->nn_param.conv2d.stride[0] = 1;
    node[96]->nn_param.conv2d.stride[1] = 1;
    node[96]->nn_param.conv2d.pad[0] = 0;
    node[96]->nn_param.conv2d.pad[1] = 0;
    node[96]->nn_param.conv2d.pad[2] = 3;
    node[96]->nn_param.conv2d.pad[3] = 3;
    node[96]->nn_param.conv2d.group = 1;
    node[96]->nn_param.conv2d.dilation[0] = 1;
    node[96]->nn_param.conv2d.dilation[1] = 1;
    node[96]->nn_param.conv2d.multiplier = 0;
    node[96]->vx_param.has_relu = TRUE;
    node[96]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[96]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[96]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/concat_249
      var       - node[97]
      name      - InceptionResnetV1/Repeat_1/block17_3/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[97], VSI_NN_OP_CONCAT, 2, 1, 249);
    node[97]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_3/Conv2d_1x1/convolution_241
      var       - node[98]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[98], VSI_NN_OP_CONV_RELU, 2, 1, 241);
    node[98]->nn_param.conv2d.ksize[0] = 1;
    node[98]->nn_param.conv2d.ksize[1] = 1;
    node[98]->nn_param.conv2d.weights = 896;
    node[98]->nn_param.conv2d.stride[0] = 1;
    node[98]->nn_param.conv2d.stride[1] = 1;
    node[98]->nn_param.conv2d.pad[0] = 0;
    node[98]->nn_param.conv2d.pad[1] = 0;
    node[98]->nn_param.conv2d.pad[2] = 0;
    node[98]->nn_param.conv2d.pad[3] = 0;
    node[98]->nn_param.conv2d.group = 1;
    node[98]->nn_param.conv2d.dilation[0] = 1;
    node[98]->nn_param.conv2d.dilation[1] = 1;
    node[98]->nn_param.conv2d.multiplier = 0;
    node[98]->vx_param.has_relu = FALSE;
    node[98]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[98]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[98]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/add_223_concat_161
      var       - node[99]
      name      - InceptionResnetV1/Repeat_1/block17_3/add_223_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[99], VSI_NN_OP_CONCAT, 2, 1, 161);
    node[99]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_164
      var       - node[100]
      name      - InceptionResnetV1/Repeat_1/block17_3/add_223_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[100], VSI_NN_OP_RESHAPE, 1, 1, 164);
    node[100]->nn_param.reshape.size = shape_15;
    node[100]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/add_223_conv_166_InceptionResnetV1/Repeat_1/block17_3/Relu_214
      var       - node[101]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[101], VSI_NN_OP_CONV_RELU, 2, 1, 214);
    node[101]->nn_param.conv2d.ksize[0] = 1;
    node[101]->nn_param.conv2d.ksize[1] = 1;
    node[101]->nn_param.conv2d.weights = 1;
    node[101]->nn_param.conv2d.stride[0] = 1;
    node[101]->nn_param.conv2d.stride[1] = 1;
    node[101]->nn_param.conv2d.pad[0] = 0;
    node[101]->nn_param.conv2d.pad[1] = 0;
    node[101]->nn_param.conv2d.pad[2] = 0;
    node[101]->nn_param.conv2d.pad[3] = 0;
    node[101]->nn_param.conv2d.group = 1;
    node[101]->nn_param.conv2d.dilation[0] = 1;
    node[101]->nn_param.conv2d.dilation[1] = 1;
    node[101]->nn_param.conv2d.multiplier = 0;
    node[101]->vx_param.has_relu = TRUE;
    node[101]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[101]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[101]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_167
      var       - node[102]
      name      - InceptionResnetV1/Repeat_1/block17_3/add_223_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[102], VSI_NN_OP_RESHAPE, 1, 1, 167);
    node[102]->nn_param.reshape.size = shape_16;
    node[102]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/convolution_263_InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/Relu_245
      var       - node[103]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[103], VSI_NN_OP_CONV_RELU, 2, 1, 245);
    node[103]->nn_param.conv2d.ksize[0] = 1;
    node[103]->nn_param.conv2d.ksize[1] = 1;
    node[103]->nn_param.conv2d.weights = 128;
    node[103]->nn_param.conv2d.stride[0] = 1;
    node[103]->nn_param.conv2d.stride[1] = 1;
    node[103]->nn_param.conv2d.pad[0] = 0;
    node[103]->nn_param.conv2d.pad[1] = 0;
    node[103]->nn_param.conv2d.pad[2] = 0;
    node[103]->nn_param.conv2d.pad[3] = 0;
    node[103]->nn_param.conv2d.group = 1;
    node[103]->nn_param.conv2d.dilation[0] = 1;
    node[103]->nn_param.conv2d.dilation[1] = 1;
    node[103]->nn_param.conv2d.multiplier = 0;
    node[103]->vx_param.has_relu = TRUE;
    node[103]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[103]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[103]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/convolution_320_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/Relu_303
      var       - node[104]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[104], VSI_NN_OP_CONV_RELU, 2, 1, 303);
    node[104]->nn_param.conv2d.ksize[0] = 1;
    node[104]->nn_param.conv2d.ksize[1] = 1;
    node[104]->nn_param.conv2d.weights = 128;
    node[104]->nn_param.conv2d.stride[0] = 1;
    node[104]->nn_param.conv2d.stride[1] = 1;
    node[104]->nn_param.conv2d.pad[0] = 0;
    node[104]->nn_param.conv2d.pad[1] = 0;
    node[104]->nn_param.conv2d.pad[2] = 0;
    node[104]->nn_param.conv2d.pad[3] = 0;
    node[104]->nn_param.conv2d.group = 1;
    node[104]->nn_param.conv2d.dilation[0] = 1;
    node[104]->nn_param.conv2d.dilation[1] = 1;
    node[104]->nn_param.conv2d.multiplier = 0;
    node[104]->vx_param.has_relu = TRUE;
    node[104]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[104]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[104]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/convolution_293_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/Relu_271
      var       - node[105]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[105], VSI_NN_OP_CONV_RELU, 2, 1, 271);
    node[105]->nn_param.conv2d.ksize[0] = 7;
    node[105]->nn_param.conv2d.ksize[1] = 7;
    node[105]->nn_param.conv2d.weights = 128;
    node[105]->nn_param.conv2d.stride[0] = 1;
    node[105]->nn_param.conv2d.stride[1] = 1;
    node[105]->nn_param.conv2d.pad[0] = 3;
    node[105]->nn_param.conv2d.pad[1] = 3;
    node[105]->nn_param.conv2d.pad[2] = 3;
    node[105]->nn_param.conv2d.pad[3] = 3;
    node[105]->nn_param.conv2d.group = 1;
    node[105]->nn_param.conv2d.dilation[0] = 1;
    node[105]->nn_param.conv2d.dilation[1] = 1;
    node[105]->nn_param.conv2d.multiplier = 0;
    node[105]->vx_param.has_relu = TRUE;
    node[105]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[105]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[105]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/convolution_262_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/Relu_244
      var       - node[106]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[106], VSI_NN_OP_CONV_RELU, 2, 1, 244);
    node[106]->nn_param.conv2d.ksize[0] = 1;
    node[106]->nn_param.conv2d.ksize[1] = 7;
    node[106]->nn_param.conv2d.weights = 128;
    node[106]->nn_param.conv2d.stride[0] = 1;
    node[106]->nn_param.conv2d.stride[1] = 1;
    node[106]->nn_param.conv2d.pad[0] = 0;
    node[106]->nn_param.conv2d.pad[1] = 0;
    node[106]->nn_param.conv2d.pad[2] = 3;
    node[106]->nn_param.conv2d.pad[3] = 3;
    node[106]->nn_param.conv2d.group = 1;
    node[106]->nn_param.conv2d.dilation[0] = 1;
    node[106]->nn_param.conv2d.dilation[1] = 1;
    node[106]->nn_param.conv2d.multiplier = 0;
    node[106]->vx_param.has_relu = TRUE;
    node[106]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[106]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[106]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/concat_235
      var       - node[107]
      name      - InceptionResnetV1/Repeat_1/block17_4/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[107], VSI_NN_OP_CONCAT, 2, 1, 235);
    node[107]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_4/Conv2d_1x1/convolution_225
      var       - node[108]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[108], VSI_NN_OP_CONV_RELU, 2, 1, 225);
    node[108]->nn_param.conv2d.ksize[0] = 1;
    node[108]->nn_param.conv2d.ksize[1] = 1;
    node[108]->nn_param.conv2d.weights = 896;
    node[108]->nn_param.conv2d.stride[0] = 1;
    node[108]->nn_param.conv2d.stride[1] = 1;
    node[108]->nn_param.conv2d.pad[0] = 0;
    node[108]->nn_param.conv2d.pad[1] = 0;
    node[108]->nn_param.conv2d.pad[2] = 0;
    node[108]->nn_param.conv2d.pad[3] = 0;
    node[108]->nn_param.conv2d.group = 1;
    node[108]->nn_param.conv2d.dilation[0] = 1;
    node[108]->nn_param.conv2d.dilation[1] = 1;
    node[108]->nn_param.conv2d.multiplier = 0;
    node[108]->vx_param.has_relu = FALSE;
    node[108]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[108]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[108]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/add_207_concat_146
      var       - node[109]
      name      - InceptionResnetV1/Repeat_1/block17_4/add_207_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[109], VSI_NN_OP_CONCAT, 2, 1, 146);
    node[109]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_147
      var       - node[110]
      name      - InceptionResnetV1/Repeat_1/block17_4/add_207_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[110], VSI_NN_OP_RESHAPE, 1, 1, 147);
    node[110]->nn_param.reshape.size = shape_17;
    node[110]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/add_207_conv_150_InceptionResnetV1/Repeat_1/block17_4/Relu_198
      var       - node[111]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[111], VSI_NN_OP_CONV_RELU, 2, 1, 198);
    node[111]->nn_param.conv2d.ksize[0] = 1;
    node[111]->nn_param.conv2d.ksize[1] = 1;
    node[111]->nn_param.conv2d.weights = 1;
    node[111]->nn_param.conv2d.stride[0] = 1;
    node[111]->nn_param.conv2d.stride[1] = 1;
    node[111]->nn_param.conv2d.pad[0] = 0;
    node[111]->nn_param.conv2d.pad[1] = 0;
    node[111]->nn_param.conv2d.pad[2] = 0;
    node[111]->nn_param.conv2d.pad[3] = 0;
    node[111]->nn_param.conv2d.group = 1;
    node[111]->nn_param.conv2d.dilation[0] = 1;
    node[111]->nn_param.conv2d.dilation[1] = 1;
    node[111]->nn_param.conv2d.multiplier = 0;
    node[111]->vx_param.has_relu = TRUE;
    node[111]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[111]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[111]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_156
      var       - node[112]
      name      - InceptionResnetV1/Repeat_1/block17_4/add_207_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[112], VSI_NN_OP_RESHAPE, 1, 1, 156);
    node[112]->nn_param.reshape.size = shape_18;
    node[112]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/convolution_239_InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/Relu_221
      var       - node[113]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[113], VSI_NN_OP_CONV_RELU, 2, 1, 221);
    node[113]->nn_param.conv2d.ksize[0] = 1;
    node[113]->nn_param.conv2d.ksize[1] = 1;
    node[113]->nn_param.conv2d.weights = 128;
    node[113]->nn_param.conv2d.stride[0] = 1;
    node[113]->nn_param.conv2d.stride[1] = 1;
    node[113]->nn_param.conv2d.pad[0] = 0;
    node[113]->nn_param.conv2d.pad[1] = 0;
    node[113]->nn_param.conv2d.pad[2] = 0;
    node[113]->nn_param.conv2d.pad[3] = 0;
    node[113]->nn_param.conv2d.group = 1;
    node[113]->nn_param.conv2d.dilation[0] = 1;
    node[113]->nn_param.conv2d.dilation[1] = 1;
    node[113]->nn_param.conv2d.multiplier = 0;
    node[113]->vx_param.has_relu = TRUE;
    node[113]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[113]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[113]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/convolution_294_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/Relu_273
      var       - node[114]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[114], VSI_NN_OP_CONV_RELU, 2, 1, 273);
    node[114]->nn_param.conv2d.ksize[0] = 1;
    node[114]->nn_param.conv2d.ksize[1] = 1;
    node[114]->nn_param.conv2d.weights = 128;
    node[114]->nn_param.conv2d.stride[0] = 1;
    node[114]->nn_param.conv2d.stride[1] = 1;
    node[114]->nn_param.conv2d.pad[0] = 0;
    node[114]->nn_param.conv2d.pad[1] = 0;
    node[114]->nn_param.conv2d.pad[2] = 0;
    node[114]->nn_param.conv2d.pad[3] = 0;
    node[114]->nn_param.conv2d.group = 1;
    node[114]->nn_param.conv2d.dilation[0] = 1;
    node[114]->nn_param.conv2d.dilation[1] = 1;
    node[114]->nn_param.conv2d.multiplier = 0;
    node[114]->vx_param.has_relu = TRUE;
    node[114]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[114]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[114]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/convolution_265_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/Relu_248
      var       - node[115]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[115], VSI_NN_OP_CONV_RELU, 2, 1, 248);
    node[115]->nn_param.conv2d.ksize[0] = 7;
    node[115]->nn_param.conv2d.ksize[1] = 7;
    node[115]->nn_param.conv2d.weights = 128;
    node[115]->nn_param.conv2d.stride[0] = 1;
    node[115]->nn_param.conv2d.stride[1] = 1;
    node[115]->nn_param.conv2d.pad[0] = 3;
    node[115]->nn_param.conv2d.pad[1] = 3;
    node[115]->nn_param.conv2d.pad[2] = 3;
    node[115]->nn_param.conv2d.pad[3] = 3;
    node[115]->nn_param.conv2d.group = 1;
    node[115]->nn_param.conv2d.dilation[0] = 1;
    node[115]->nn_param.conv2d.dilation[1] = 1;
    node[115]->nn_param.conv2d.multiplier = 0;
    node[115]->vx_param.has_relu = TRUE;
    node[115]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[115]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[115]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/convolution_240_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/Relu_222
      var       - node[116]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[116], VSI_NN_OP_CONV_RELU, 2, 1, 222);
    node[116]->nn_param.conv2d.ksize[0] = 1;
    node[116]->nn_param.conv2d.ksize[1] = 7;
    node[116]->nn_param.conv2d.weights = 128;
    node[116]->nn_param.conv2d.stride[0] = 1;
    node[116]->nn_param.conv2d.stride[1] = 1;
    node[116]->nn_param.conv2d.pad[0] = 0;
    node[116]->nn_param.conv2d.pad[1] = 0;
    node[116]->nn_param.conv2d.pad[2] = 3;
    node[116]->nn_param.conv2d.pad[3] = 3;
    node[116]->nn_param.conv2d.group = 1;
    node[116]->nn_param.conv2d.dilation[0] = 1;
    node[116]->nn_param.conv2d.dilation[1] = 1;
    node[116]->nn_param.conv2d.multiplier = 0;
    node[116]->vx_param.has_relu = TRUE;
    node[116]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[116]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[116]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/concat_213
      var       - node[117]
      name      - InceptionResnetV1/Repeat_1/block17_5/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[117], VSI_NN_OP_CONCAT, 2, 1, 213);
    node[117]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_5/Conv2d_1x1/convolution_205
      var       - node[118]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[118], VSI_NN_OP_CONV_RELU, 2, 1, 205);
    node[118]->nn_param.conv2d.ksize[0] = 1;
    node[118]->nn_param.conv2d.ksize[1] = 1;
    node[118]->nn_param.conv2d.weights = 896;
    node[118]->nn_param.conv2d.stride[0] = 1;
    node[118]->nn_param.conv2d.stride[1] = 1;
    node[118]->nn_param.conv2d.pad[0] = 0;
    node[118]->nn_param.conv2d.pad[1] = 0;
    node[118]->nn_param.conv2d.pad[2] = 0;
    node[118]->nn_param.conv2d.pad[3] = 0;
    node[118]->nn_param.conv2d.group = 1;
    node[118]->nn_param.conv2d.dilation[0] = 1;
    node[118]->nn_param.conv2d.dilation[1] = 1;
    node[118]->nn_param.conv2d.multiplier = 0;
    node[118]->vx_param.has_relu = FALSE;
    node[118]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[118]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[118]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/add_189_concat_135
      var       - node[119]
      name      - InceptionResnetV1/Repeat_1/block17_5/add_189_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[119], VSI_NN_OP_CONCAT, 2, 1, 135);
    node[119]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_141
      var       - node[120]
      name      - InceptionResnetV1/Repeat_1/block17_5/add_189_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[120], VSI_NN_OP_RESHAPE, 1, 1, 141);
    node[120]->nn_param.reshape.size = shape_19;
    node[120]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/add_189_conv_144_InceptionResnetV1/Repeat_1/block17_5/Relu_181
      var       - node[121]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[121], VSI_NN_OP_CONV_RELU, 2, 1, 181);
    node[121]->nn_param.conv2d.ksize[0] = 1;
    node[121]->nn_param.conv2d.ksize[1] = 1;
    node[121]->nn_param.conv2d.weights = 1;
    node[121]->nn_param.conv2d.stride[0] = 1;
    node[121]->nn_param.conv2d.stride[1] = 1;
    node[121]->nn_param.conv2d.pad[0] = 0;
    node[121]->nn_param.conv2d.pad[1] = 0;
    node[121]->nn_param.conv2d.pad[2] = 0;
    node[121]->nn_param.conv2d.pad[3] = 0;
    node[121]->nn_param.conv2d.group = 1;
    node[121]->nn_param.conv2d.dilation[0] = 1;
    node[121]->nn_param.conv2d.dilation[1] = 1;
    node[121]->nn_param.conv2d.multiplier = 0;
    node[121]->vx_param.has_relu = TRUE;
    node[121]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[121]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[121]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_145
      var       - node[122]
      name      - InceptionResnetV1/Repeat_1/block17_5/add_189_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[122], VSI_NN_OP_RESHAPE, 1, 1, 145);
    node[122]->nn_param.reshape.size = shape_20;
    node[122]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/convolution_227_InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/Relu_209
      var       - node[123]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[123], VSI_NN_OP_CONV_RELU, 2, 1, 209);
    node[123]->nn_param.conv2d.ksize[0] = 1;
    node[123]->nn_param.conv2d.ksize[1] = 1;
    node[123]->nn_param.conv2d.weights = 128;
    node[123]->nn_param.conv2d.stride[0] = 1;
    node[123]->nn_param.conv2d.stride[1] = 1;
    node[123]->nn_param.conv2d.pad[0] = 0;
    node[123]->nn_param.conv2d.pad[1] = 0;
    node[123]->nn_param.conv2d.pad[2] = 0;
    node[123]->nn_param.conv2d.pad[3] = 0;
    node[123]->nn_param.conv2d.group = 1;
    node[123]->nn_param.conv2d.dilation[0] = 1;
    node[123]->nn_param.conv2d.dilation[1] = 1;
    node[123]->nn_param.conv2d.multiplier = 0;
    node[123]->vx_param.has_relu = TRUE;
    node[123]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[123]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[123]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/convolution_284_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/Relu_264
      var       - node[124]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[124], VSI_NN_OP_CONV_RELU, 2, 1, 264);
    node[124]->nn_param.conv2d.ksize[0] = 1;
    node[124]->nn_param.conv2d.ksize[1] = 1;
    node[124]->nn_param.conv2d.weights = 128;
    node[124]->nn_param.conv2d.stride[0] = 1;
    node[124]->nn_param.conv2d.stride[1] = 1;
    node[124]->nn_param.conv2d.pad[0] = 0;
    node[124]->nn_param.conv2d.pad[1] = 0;
    node[124]->nn_param.conv2d.pad[2] = 0;
    node[124]->nn_param.conv2d.pad[3] = 0;
    node[124]->nn_param.conv2d.group = 1;
    node[124]->nn_param.conv2d.dilation[0] = 1;
    node[124]->nn_param.conv2d.dilation[1] = 1;
    node[124]->nn_param.conv2d.multiplier = 0;
    node[124]->vx_param.has_relu = TRUE;
    node[124]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[124]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[124]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/convolution_254_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/Relu_236
      var       - node[125]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[125], VSI_NN_OP_CONV_RELU, 2, 1, 236);
    node[125]->nn_param.conv2d.ksize[0] = 7;
    node[125]->nn_param.conv2d.ksize[1] = 7;
    node[125]->nn_param.conv2d.weights = 128;
    node[125]->nn_param.conv2d.stride[0] = 1;
    node[125]->nn_param.conv2d.stride[1] = 1;
    node[125]->nn_param.conv2d.pad[0] = 3;
    node[125]->nn_param.conv2d.pad[1] = 3;
    node[125]->nn_param.conv2d.pad[2] = 3;
    node[125]->nn_param.conv2d.pad[3] = 3;
    node[125]->nn_param.conv2d.group = 1;
    node[125]->nn_param.conv2d.dilation[0] = 1;
    node[125]->nn_param.conv2d.dilation[1] = 1;
    node[125]->nn_param.conv2d.multiplier = 0;
    node[125]->vx_param.has_relu = TRUE;
    node[125]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[125]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[125]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/convolution_226_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/Relu_208
      var       - node[126]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[126], VSI_NN_OP_CONV_RELU, 2, 1, 208);
    node[126]->nn_param.conv2d.ksize[0] = 1;
    node[126]->nn_param.conv2d.ksize[1] = 7;
    node[126]->nn_param.conv2d.weights = 128;
    node[126]->nn_param.conv2d.stride[0] = 1;
    node[126]->nn_param.conv2d.stride[1] = 1;
    node[126]->nn_param.conv2d.pad[0] = 0;
    node[126]->nn_param.conv2d.pad[1] = 0;
    node[126]->nn_param.conv2d.pad[2] = 3;
    node[126]->nn_param.conv2d.pad[3] = 3;
    node[126]->nn_param.conv2d.group = 1;
    node[126]->nn_param.conv2d.dilation[0] = 1;
    node[126]->nn_param.conv2d.dilation[1] = 1;
    node[126]->nn_param.conv2d.multiplier = 0;
    node[126]->vx_param.has_relu = TRUE;
    node[126]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[126]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[126]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/concat_199
      var       - node[127]
      name      - InceptionResnetV1/Repeat_1/block17_6/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[127], VSI_NN_OP_CONCAT, 2, 1, 199);
    node[127]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_6/Conv2d_1x1/convolution_191
      var       - node[128]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[128], VSI_NN_OP_CONV_RELU, 2, 1, 191);
    node[128]->nn_param.conv2d.ksize[0] = 1;
    node[128]->nn_param.conv2d.ksize[1] = 1;
    node[128]->nn_param.conv2d.weights = 896;
    node[128]->nn_param.conv2d.stride[0] = 1;
    node[128]->nn_param.conv2d.stride[1] = 1;
    node[128]->nn_param.conv2d.pad[0] = 0;
    node[128]->nn_param.conv2d.pad[1] = 0;
    node[128]->nn_param.conv2d.pad[2] = 0;
    node[128]->nn_param.conv2d.pad[3] = 0;
    node[128]->nn_param.conv2d.group = 1;
    node[128]->nn_param.conv2d.dilation[0] = 1;
    node[128]->nn_param.conv2d.dilation[1] = 1;
    node[128]->nn_param.conv2d.multiplier = 0;
    node[128]->vx_param.has_relu = FALSE;
    node[128]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[128]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[128]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/add_174_concat_122
      var       - node[129]
      name      - InceptionResnetV1/Repeat_1/block17_6/add_174_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[129], VSI_NN_OP_CONCAT, 2, 1, 122);
    node[129]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_123
      var       - node[130]
      name      - InceptionResnetV1/Repeat_1/block17_6/add_174_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[130], VSI_NN_OP_RESHAPE, 1, 1, 123);
    node[130]->nn_param.reshape.size = shape_21;
    node[130]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/add_174_conv_125_InceptionResnetV1/Repeat_1/block17_6/Relu_165
      var       - node[131]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[131], VSI_NN_OP_CONV_RELU, 2, 1, 165);
    node[131]->nn_param.conv2d.ksize[0] = 1;
    node[131]->nn_param.conv2d.ksize[1] = 1;
    node[131]->nn_param.conv2d.weights = 1;
    node[131]->nn_param.conv2d.stride[0] = 1;
    node[131]->nn_param.conv2d.stride[1] = 1;
    node[131]->nn_param.conv2d.pad[0] = 0;
    node[131]->nn_param.conv2d.pad[1] = 0;
    node[131]->nn_param.conv2d.pad[2] = 0;
    node[131]->nn_param.conv2d.pad[3] = 0;
    node[131]->nn_param.conv2d.group = 1;
    node[131]->nn_param.conv2d.dilation[0] = 1;
    node[131]->nn_param.conv2d.dilation[1] = 1;
    node[131]->nn_param.conv2d.multiplier = 0;
    node[131]->vx_param.has_relu = TRUE;
    node[131]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[131]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[131]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_130
      var       - node[132]
      name      - InceptionResnetV1/Repeat_1/block17_6/add_174_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[132], VSI_NN_OP_RESHAPE, 1, 1, 130);
    node[132]->nn_param.reshape.size = shape_22;
    node[132]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/convolution_204_InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/Relu_188
      var       - node[133]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[133], VSI_NN_OP_CONV_RELU, 2, 1, 188);
    node[133]->nn_param.conv2d.ksize[0] = 1;
    node[133]->nn_param.conv2d.ksize[1] = 1;
    node[133]->nn_param.conv2d.weights = 128;
    node[133]->nn_param.conv2d.stride[0] = 1;
    node[133]->nn_param.conv2d.stride[1] = 1;
    node[133]->nn_param.conv2d.pad[0] = 0;
    node[133]->nn_param.conv2d.pad[1] = 0;
    node[133]->nn_param.conv2d.pad[2] = 0;
    node[133]->nn_param.conv2d.pad[3] = 0;
    node[133]->nn_param.conv2d.group = 1;
    node[133]->nn_param.conv2d.dilation[0] = 1;
    node[133]->nn_param.conv2d.dilation[1] = 1;
    node[133]->nn_param.conv2d.multiplier = 0;
    node[133]->vx_param.has_relu = TRUE;
    node[133]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[133]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[133]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/convolution_255_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/Relu_238
      var       - node[134]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[134], VSI_NN_OP_CONV_RELU, 2, 1, 238);
    node[134]->nn_param.conv2d.ksize[0] = 1;
    node[134]->nn_param.conv2d.ksize[1] = 1;
    node[134]->nn_param.conv2d.weights = 128;
    node[134]->nn_param.conv2d.stride[0] = 1;
    node[134]->nn_param.conv2d.stride[1] = 1;
    node[134]->nn_param.conv2d.pad[0] = 0;
    node[134]->nn_param.conv2d.pad[1] = 0;
    node[134]->nn_param.conv2d.pad[2] = 0;
    node[134]->nn_param.conv2d.pad[3] = 0;
    node[134]->nn_param.conv2d.group = 1;
    node[134]->nn_param.conv2d.dilation[0] = 1;
    node[134]->nn_param.conv2d.dilation[1] = 1;
    node[134]->nn_param.conv2d.multiplier = 0;
    node[134]->vx_param.has_relu = TRUE;
    node[134]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[134]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[134]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/convolution_230_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/Relu_212
      var       - node[135]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[135], VSI_NN_OP_CONV_RELU, 2, 1, 212);
    node[135]->nn_param.conv2d.ksize[0] = 7;
    node[135]->nn_param.conv2d.ksize[1] = 7;
    node[135]->nn_param.conv2d.weights = 128;
    node[135]->nn_param.conv2d.stride[0] = 1;
    node[135]->nn_param.conv2d.stride[1] = 1;
    node[135]->nn_param.conv2d.pad[0] = 3;
    node[135]->nn_param.conv2d.pad[1] = 3;
    node[135]->nn_param.conv2d.pad[2] = 3;
    node[135]->nn_param.conv2d.pad[3] = 3;
    node[135]->nn_param.conv2d.group = 1;
    node[135]->nn_param.conv2d.dilation[0] = 1;
    node[135]->nn_param.conv2d.dilation[1] = 1;
    node[135]->nn_param.conv2d.multiplier = 0;
    node[135]->vx_param.has_relu = TRUE;
    node[135]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[135]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[135]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/convolution_203_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/Relu_187
      var       - node[136]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[136], VSI_NN_OP_CONV_RELU, 2, 1, 187);
    node[136]->nn_param.conv2d.ksize[0] = 1;
    node[136]->nn_param.conv2d.ksize[1] = 7;
    node[136]->nn_param.conv2d.weights = 128;
    node[136]->nn_param.conv2d.stride[0] = 1;
    node[136]->nn_param.conv2d.stride[1] = 1;
    node[136]->nn_param.conv2d.pad[0] = 0;
    node[136]->nn_param.conv2d.pad[1] = 0;
    node[136]->nn_param.conv2d.pad[2] = 3;
    node[136]->nn_param.conv2d.pad[3] = 3;
    node[136]->nn_param.conv2d.group = 1;
    node[136]->nn_param.conv2d.dilation[0] = 1;
    node[136]->nn_param.conv2d.dilation[1] = 1;
    node[136]->nn_param.conv2d.multiplier = 0;
    node[136]->vx_param.has_relu = TRUE;
    node[136]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[136]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[136]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/concat_180
      var       - node[137]
      name      - InceptionResnetV1/Repeat_1/block17_7/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[137], VSI_NN_OP_CONCAT, 2, 1, 180);
    node[137]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_7/Conv2d_1x1/convolution_172
      var       - node[138]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[138], VSI_NN_OP_CONV_RELU, 2, 1, 172);
    node[138]->nn_param.conv2d.ksize[0] = 1;
    node[138]->nn_param.conv2d.ksize[1] = 1;
    node[138]->nn_param.conv2d.weights = 896;
    node[138]->nn_param.conv2d.stride[0] = 1;
    node[138]->nn_param.conv2d.stride[1] = 1;
    node[138]->nn_param.conv2d.pad[0] = 0;
    node[138]->nn_param.conv2d.pad[1] = 0;
    node[138]->nn_param.conv2d.pad[2] = 0;
    node[138]->nn_param.conv2d.pad[3] = 0;
    node[138]->nn_param.conv2d.group = 1;
    node[138]->nn_param.conv2d.dilation[0] = 1;
    node[138]->nn_param.conv2d.dilation[1] = 1;
    node[138]->nn_param.conv2d.multiplier = 0;
    node[138]->vx_param.has_relu = FALSE;
    node[138]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[138]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[138]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/add_157_concat_113
      var       - node[139]
      name      - InceptionResnetV1/Repeat_1/block17_7/add_157_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[139], VSI_NN_OP_CONCAT, 2, 1, 113);
    node[139]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_117
      var       - node[140]
      name      - InceptionResnetV1/Repeat_1/block17_7/add_157_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[140], VSI_NN_OP_RESHAPE, 1, 1, 117);
    node[140]->nn_param.reshape.size = shape_23;
    node[140]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/add_157_conv_118_InceptionResnetV1/Repeat_1/block17_7/Relu_148
      var       - node[141]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[141], VSI_NN_OP_CONV_RELU, 2, 1, 148);
    node[141]->nn_param.conv2d.ksize[0] = 1;
    node[141]->nn_param.conv2d.ksize[1] = 1;
    node[141]->nn_param.conv2d.weights = 1;
    node[141]->nn_param.conv2d.stride[0] = 1;
    node[141]->nn_param.conv2d.stride[1] = 1;
    node[141]->nn_param.conv2d.pad[0] = 0;
    node[141]->nn_param.conv2d.pad[1] = 0;
    node[141]->nn_param.conv2d.pad[2] = 0;
    node[141]->nn_param.conv2d.pad[3] = 0;
    node[141]->nn_param.conv2d.group = 1;
    node[141]->nn_param.conv2d.dilation[0] = 1;
    node[141]->nn_param.conv2d.dilation[1] = 1;
    node[141]->nn_param.conv2d.multiplier = 0;
    node[141]->vx_param.has_relu = TRUE;
    node[141]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[141]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[141]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_121
      var       - node[142]
      name      - InceptionResnetV1/Repeat_1/block17_7/add_157_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[142], VSI_NN_OP_RESHAPE, 1, 1, 121);
    node[142]->nn_param.reshape.size = shape_24;
    node[142]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/convolution_185_InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/Relu_170
      var       - node[143]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[143], VSI_NN_OP_CONV_RELU, 2, 1, 170);
    node[143]->nn_param.conv2d.ksize[0] = 1;
    node[143]->nn_param.conv2d.ksize[1] = 1;
    node[143]->nn_param.conv2d.weights = 128;
    node[143]->nn_param.conv2d.stride[0] = 1;
    node[143]->nn_param.conv2d.stride[1] = 1;
    node[143]->nn_param.conv2d.pad[0] = 0;
    node[143]->nn_param.conv2d.pad[1] = 0;
    node[143]->nn_param.conv2d.pad[2] = 0;
    node[143]->nn_param.conv2d.pad[3] = 0;
    node[143]->nn_param.conv2d.group = 1;
    node[143]->nn_param.conv2d.dilation[0] = 1;
    node[143]->nn_param.conv2d.dilation[1] = 1;
    node[143]->nn_param.conv2d.multiplier = 0;
    node[143]->vx_param.has_relu = TRUE;
    node[143]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[143]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[143]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/convolution_237_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/Relu_219
      var       - node[144]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[144], VSI_NN_OP_CONV_RELU, 2, 1, 219);
    node[144]->nn_param.conv2d.ksize[0] = 1;
    node[144]->nn_param.conv2d.ksize[1] = 1;
    node[144]->nn_param.conv2d.weights = 128;
    node[144]->nn_param.conv2d.stride[0] = 1;
    node[144]->nn_param.conv2d.stride[1] = 1;
    node[144]->nn_param.conv2d.pad[0] = 0;
    node[144]->nn_param.conv2d.pad[1] = 0;
    node[144]->nn_param.conv2d.pad[2] = 0;
    node[144]->nn_param.conv2d.pad[3] = 0;
    node[144]->nn_param.conv2d.group = 1;
    node[144]->nn_param.conv2d.dilation[0] = 1;
    node[144]->nn_param.conv2d.dilation[1] = 1;
    node[144]->nn_param.conv2d.multiplier = 0;
    node[144]->vx_param.has_relu = TRUE;
    node[144]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[144]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[144]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/convolution_211_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/Relu_194
      var       - node[145]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[145], VSI_NN_OP_CONV_RELU, 2, 1, 194);
    node[145]->nn_param.conv2d.ksize[0] = 7;
    node[145]->nn_param.conv2d.ksize[1] = 7;
    node[145]->nn_param.conv2d.weights = 128;
    node[145]->nn_param.conv2d.stride[0] = 1;
    node[145]->nn_param.conv2d.stride[1] = 1;
    node[145]->nn_param.conv2d.pad[0] = 3;
    node[145]->nn_param.conv2d.pad[1] = 3;
    node[145]->nn_param.conv2d.pad[2] = 3;
    node[145]->nn_param.conv2d.pad[3] = 3;
    node[145]->nn_param.conv2d.group = 1;
    node[145]->nn_param.conv2d.dilation[0] = 1;
    node[145]->nn_param.conv2d.dilation[1] = 1;
    node[145]->nn_param.conv2d.multiplier = 0;
    node[145]->vx_param.has_relu = TRUE;
    node[145]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[145]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[145]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/convolution_186_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/Relu_171
      var       - node[146]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[146], VSI_NN_OP_CONV_RELU, 2, 1, 171);
    node[146]->nn_param.conv2d.ksize[0] = 1;
    node[146]->nn_param.conv2d.ksize[1] = 7;
    node[146]->nn_param.conv2d.weights = 128;
    node[146]->nn_param.conv2d.stride[0] = 1;
    node[146]->nn_param.conv2d.stride[1] = 1;
    node[146]->nn_param.conv2d.pad[0] = 0;
    node[146]->nn_param.conv2d.pad[1] = 0;
    node[146]->nn_param.conv2d.pad[2] = 3;
    node[146]->nn_param.conv2d.pad[3] = 3;
    node[146]->nn_param.conv2d.group = 1;
    node[146]->nn_param.conv2d.dilation[0] = 1;
    node[146]->nn_param.conv2d.dilation[1] = 1;
    node[146]->nn_param.conv2d.multiplier = 0;
    node[146]->vx_param.has_relu = TRUE;
    node[146]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[146]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[146]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/concat_163
      var       - node[147]
      name      - InceptionResnetV1/Repeat_1/block17_8/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[147], VSI_NN_OP_CONCAT, 2, 1, 163);
    node[147]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_8/Conv2d_1x1/convolution_155
      var       - node[148]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[148], VSI_NN_OP_CONV_RELU, 2, 1, 155);
    node[148]->nn_param.conv2d.ksize[0] = 1;
    node[148]->nn_param.conv2d.ksize[1] = 1;
    node[148]->nn_param.conv2d.weights = 896;
    node[148]->nn_param.conv2d.stride[0] = 1;
    node[148]->nn_param.conv2d.stride[1] = 1;
    node[148]->nn_param.conv2d.pad[0] = 0;
    node[148]->nn_param.conv2d.pad[1] = 0;
    node[148]->nn_param.conv2d.pad[2] = 0;
    node[148]->nn_param.conv2d.pad[3] = 0;
    node[148]->nn_param.conv2d.group = 1;
    node[148]->nn_param.conv2d.dilation[0] = 1;
    node[148]->nn_param.conv2d.dilation[1] = 1;
    node[148]->nn_param.conv2d.multiplier = 0;
    node[148]->vx_param.has_relu = FALSE;
    node[148]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[148]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[148]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/add_139_concat_94
      var       - node[149]
      name      - InceptionResnetV1/Repeat_1/block17_8/add_139_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[149], VSI_NN_OP_CONCAT, 2, 1, 94);
    node[149]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_105
      var       - node[150]
      name      - InceptionResnetV1/Repeat_1/block17_8/add_139_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[150], VSI_NN_OP_RESHAPE, 1, 1, 105);
    node[150]->nn_param.reshape.size = shape_25;
    node[150]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/add_139_conv_106_InceptionResnetV1/Repeat_1/block17_8/Relu_129
      var       - node[151]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[151], VSI_NN_OP_CONV_RELU, 2, 1, 129);
    node[151]->nn_param.conv2d.ksize[0] = 1;
    node[151]->nn_param.conv2d.ksize[1] = 1;
    node[151]->nn_param.conv2d.weights = 1;
    node[151]->nn_param.conv2d.stride[0] = 1;
    node[151]->nn_param.conv2d.stride[1] = 1;
    node[151]->nn_param.conv2d.pad[0] = 0;
    node[151]->nn_param.conv2d.pad[1] = 0;
    node[151]->nn_param.conv2d.pad[2] = 0;
    node[151]->nn_param.conv2d.pad[3] = 0;
    node[151]->nn_param.conv2d.group = 1;
    node[151]->nn_param.conv2d.dilation[0] = 1;
    node[151]->nn_param.conv2d.dilation[1] = 1;
    node[151]->nn_param.conv2d.multiplier = 0;
    node[151]->vx_param.has_relu = TRUE;
    node[151]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[151]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[151]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_108
      var       - node[152]
      name      - InceptionResnetV1/Repeat_1/block17_8/add_139_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[152], VSI_NN_OP_RESHAPE, 1, 1, 108);
    node[152]->nn_param.reshape.size = shape_26;
    node[152]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/convolution_176_InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/Relu_159
      var       - node[153]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[153], VSI_NN_OP_CONV_RELU, 2, 1, 159);
    node[153]->nn_param.conv2d.ksize[0] = 1;
    node[153]->nn_param.conv2d.ksize[1] = 1;
    node[153]->nn_param.conv2d.weights = 128;
    node[153]->nn_param.conv2d.stride[0] = 1;
    node[153]->nn_param.conv2d.stride[1] = 1;
    node[153]->nn_param.conv2d.pad[0] = 0;
    node[153]->nn_param.conv2d.pad[1] = 0;
    node[153]->nn_param.conv2d.pad[2] = 0;
    node[153]->nn_param.conv2d.pad[3] = 0;
    node[153]->nn_param.conv2d.group = 1;
    node[153]->nn_param.conv2d.dilation[0] = 1;
    node[153]->nn_param.conv2d.dilation[1] = 1;
    node[153]->nn_param.conv2d.multiplier = 0;
    node[153]->vx_param.has_relu = TRUE;
    node[153]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[153]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[153]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/convolution_228_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/Relu_210
      var       - node[154]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[154], VSI_NN_OP_CONV_RELU, 2, 1, 210);
    node[154]->nn_param.conv2d.ksize[0] = 1;
    node[154]->nn_param.conv2d.ksize[1] = 1;
    node[154]->nn_param.conv2d.weights = 128;
    node[154]->nn_param.conv2d.stride[0] = 1;
    node[154]->nn_param.conv2d.stride[1] = 1;
    node[154]->nn_param.conv2d.pad[0] = 0;
    node[154]->nn_param.conv2d.pad[1] = 0;
    node[154]->nn_param.conv2d.pad[2] = 0;
    node[154]->nn_param.conv2d.pad[3] = 0;
    node[154]->nn_param.conv2d.group = 1;
    node[154]->nn_param.conv2d.dilation[0] = 1;
    node[154]->nn_param.conv2d.dilation[1] = 1;
    node[154]->nn_param.conv2d.multiplier = 0;
    node[154]->vx_param.has_relu = TRUE;
    node[154]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[154]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[154]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/convolution_200_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/Relu_183
      var       - node[155]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[155], VSI_NN_OP_CONV_RELU, 2, 1, 183);
    node[155]->nn_param.conv2d.ksize[0] = 7;
    node[155]->nn_param.conv2d.ksize[1] = 7;
    node[155]->nn_param.conv2d.weights = 128;
    node[155]->nn_param.conv2d.stride[0] = 1;
    node[155]->nn_param.conv2d.stride[1] = 1;
    node[155]->nn_param.conv2d.pad[0] = 3;
    node[155]->nn_param.conv2d.pad[1] = 3;
    node[155]->nn_param.conv2d.pad[2] = 3;
    node[155]->nn_param.conv2d.pad[3] = 3;
    node[155]->nn_param.conv2d.group = 1;
    node[155]->nn_param.conv2d.dilation[0] = 1;
    node[155]->nn_param.conv2d.dilation[1] = 1;
    node[155]->nn_param.conv2d.multiplier = 0;
    node[155]->vx_param.has_relu = TRUE;
    node[155]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[155]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[155]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/convolution_175_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/Relu_158
      var       - node[156]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[156], VSI_NN_OP_CONV_RELU, 2, 1, 158);
    node[156]->nn_param.conv2d.ksize[0] = 1;
    node[156]->nn_param.conv2d.ksize[1] = 7;
    node[156]->nn_param.conv2d.weights = 128;
    node[156]->nn_param.conv2d.stride[0] = 1;
    node[156]->nn_param.conv2d.stride[1] = 1;
    node[156]->nn_param.conv2d.pad[0] = 0;
    node[156]->nn_param.conv2d.pad[1] = 0;
    node[156]->nn_param.conv2d.pad[2] = 3;
    node[156]->nn_param.conv2d.pad[3] = 3;
    node[156]->nn_param.conv2d.group = 1;
    node[156]->nn_param.conv2d.dilation[0] = 1;
    node[156]->nn_param.conv2d.dilation[1] = 1;
    node[156]->nn_param.conv2d.multiplier = 0;
    node[156]->vx_param.has_relu = TRUE;
    node[156]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[156]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[156]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/concat_149
      var       - node[157]
      name      - InceptionResnetV1/Repeat_1/block17_9/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[157], VSI_NN_OP_CONCAT, 2, 1, 149);
    node[157]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_9/Conv2d_1x1/convolution_140
      var       - node[158]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[158], VSI_NN_OP_CONV_RELU, 2, 1, 140);
    node[158]->nn_param.conv2d.ksize[0] = 1;
    node[158]->nn_param.conv2d.ksize[1] = 1;
    node[158]->nn_param.conv2d.weights = 896;
    node[158]->nn_param.conv2d.stride[0] = 1;
    node[158]->nn_param.conv2d.stride[1] = 1;
    node[158]->nn_param.conv2d.pad[0] = 0;
    node[158]->nn_param.conv2d.pad[1] = 0;
    node[158]->nn_param.conv2d.pad[2] = 0;
    node[158]->nn_param.conv2d.pad[3] = 0;
    node[158]->nn_param.conv2d.group = 1;
    node[158]->nn_param.conv2d.dilation[0] = 1;
    node[158]->nn_param.conv2d.dilation[1] = 1;
    node[158]->nn_param.conv2d.multiplier = 0;
    node[158]->vx_param.has_relu = FALSE;
    node[158]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[158]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[158]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/add_120_concat_88
      var       - node[159]
      name      - InceptionResnetV1/Repeat_1/block17_9/add_120_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[159], VSI_NN_OP_CONCAT, 2, 1, 88);
    node[159]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_89
      var       - node[160]
      name      - InceptionResnetV1/Repeat_1/block17_9/add_120_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[160], VSI_NN_OP_RESHAPE, 1, 1, 89);
    node[160]->nn_param.reshape.size = shape_27;
    node[160]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/add_120_conv_90_InceptionResnetV1/Repeat_1/block17_9/Relu_109
      var       - node[161]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[161], VSI_NN_OP_CONV_RELU, 2, 1, 109);
    node[161]->nn_param.conv2d.ksize[0] = 1;
    node[161]->nn_param.conv2d.ksize[1] = 1;
    node[161]->nn_param.conv2d.weights = 1;
    node[161]->nn_param.conv2d.stride[0] = 1;
    node[161]->nn_param.conv2d.stride[1] = 1;
    node[161]->nn_param.conv2d.pad[0] = 0;
    node[161]->nn_param.conv2d.pad[1] = 0;
    node[161]->nn_param.conv2d.pad[2] = 0;
    node[161]->nn_param.conv2d.pad[3] = 0;
    node[161]->nn_param.conv2d.group = 1;
    node[161]->nn_param.conv2d.dilation[0] = 1;
    node[161]->nn_param.conv2d.dilation[1] = 1;
    node[161]->nn_param.conv2d.multiplier = 0;
    node[161]->vx_param.has_relu = TRUE;
    node[161]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[161]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[161]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_91
      var       - node[162]
      name      - InceptionResnetV1/Repeat_1/block17_9/add_120_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[162], VSI_NN_OP_RESHAPE, 1, 1, 91);
    node[162]->nn_param.reshape.size = shape_28;
    node[162]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/convolution_154_InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/Relu_138
      var       - node[163]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[163], VSI_NN_OP_CONV_RELU, 2, 1, 138);
    node[163]->nn_param.conv2d.ksize[0] = 1;
    node[163]->nn_param.conv2d.ksize[1] = 1;
    node[163]->nn_param.conv2d.weights = 128;
    node[163]->nn_param.conv2d.stride[0] = 1;
    node[163]->nn_param.conv2d.stride[1] = 1;
    node[163]->nn_param.conv2d.pad[0] = 0;
    node[163]->nn_param.conv2d.pad[1] = 0;
    node[163]->nn_param.conv2d.pad[2] = 0;
    node[163]->nn_param.conv2d.pad[3] = 0;
    node[163]->nn_param.conv2d.group = 1;
    node[163]->nn_param.conv2d.dilation[0] = 1;
    node[163]->nn_param.conv2d.dilation[1] = 1;
    node[163]->nn_param.conv2d.multiplier = 0;
    node[163]->vx_param.has_relu = TRUE;
    node[163]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[163]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[163]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/convolution_201_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/Relu_184
      var       - node[164]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[164], VSI_NN_OP_CONV_RELU, 2, 1, 184);
    node[164]->nn_param.conv2d.ksize[0] = 1;
    node[164]->nn_param.conv2d.ksize[1] = 1;
    node[164]->nn_param.conv2d.weights = 128;
    node[164]->nn_param.conv2d.stride[0] = 1;
    node[164]->nn_param.conv2d.stride[1] = 1;
    node[164]->nn_param.conv2d.pad[0] = 0;
    node[164]->nn_param.conv2d.pad[1] = 0;
    node[164]->nn_param.conv2d.pad[2] = 0;
    node[164]->nn_param.conv2d.pad[3] = 0;
    node[164]->nn_param.conv2d.group = 1;
    node[164]->nn_param.conv2d.dilation[0] = 1;
    node[164]->nn_param.conv2d.dilation[1] = 1;
    node[164]->nn_param.conv2d.multiplier = 0;
    node[164]->vx_param.has_relu = TRUE;
    node[164]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[164]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[164]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/convolution_177_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/Relu_162
      var       - node[165]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[165], VSI_NN_OP_CONV_RELU, 2, 1, 162);
    node[165]->nn_param.conv2d.ksize[0] = 7;
    node[165]->nn_param.conv2d.ksize[1] = 7;
    node[165]->nn_param.conv2d.weights = 128;
    node[165]->nn_param.conv2d.stride[0] = 1;
    node[165]->nn_param.conv2d.stride[1] = 1;
    node[165]->nn_param.conv2d.pad[0] = 3;
    node[165]->nn_param.conv2d.pad[1] = 3;
    node[165]->nn_param.conv2d.pad[2] = 3;
    node[165]->nn_param.conv2d.pad[3] = 3;
    node[165]->nn_param.conv2d.group = 1;
    node[165]->nn_param.conv2d.dilation[0] = 1;
    node[165]->nn_param.conv2d.dilation[1] = 1;
    node[165]->nn_param.conv2d.multiplier = 0;
    node[165]->vx_param.has_relu = TRUE;
    node[165]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[165]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[165]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/convolution_153_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/Relu_137
      var       - node[166]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 128, 1]]
      out_shape - [[8, 8, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[166], VSI_NN_OP_CONV_RELU, 2, 1, 137);
    node[166]->nn_param.conv2d.ksize[0] = 1;
    node[166]->nn_param.conv2d.ksize[1] = 7;
    node[166]->nn_param.conv2d.weights = 128;
    node[166]->nn_param.conv2d.stride[0] = 1;
    node[166]->nn_param.conv2d.stride[1] = 1;
    node[166]->nn_param.conv2d.pad[0] = 0;
    node[166]->nn_param.conv2d.pad[1] = 0;
    node[166]->nn_param.conv2d.pad[2] = 3;
    node[166]->nn_param.conv2d.pad[3] = 3;
    node[166]->nn_param.conv2d.group = 1;
    node[166]->nn_param.conv2d.dilation[0] = 1;
    node[166]->nn_param.conv2d.dilation[1] = 1;
    node[166]->nn_param.conv2d.multiplier = 0;
    node[166]->vx_param.has_relu = TRUE;
    node[166]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[166]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[166]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/concat_128
      var       - node[167]
      name      - InceptionResnetV1/Repeat_1/block17_10/concat
      operation - concat
      in_shape  - [[8, 8, 128, 1]]
                  [[8, 8, 128, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[167], VSI_NN_OP_CONCAT, 2, 1, 128);
    node[167]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_1/block17_10/Conv2d_1x1/convolution_119
      var       - node[168]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[168], VSI_NN_OP_CONV_RELU, 2, 1, 119);
    node[168]->nn_param.conv2d.ksize[0] = 1;
    node[168]->nn_param.conv2d.ksize[1] = 1;
    node[168]->nn_param.conv2d.weights = 896;
    node[168]->nn_param.conv2d.stride[0] = 1;
    node[168]->nn_param.conv2d.stride[1] = 1;
    node[168]->nn_param.conv2d.pad[0] = 0;
    node[168]->nn_param.conv2d.pad[1] = 0;
    node[168]->nn_param.conv2d.pad[2] = 0;
    node[168]->nn_param.conv2d.pad[3] = 0;
    node[168]->nn_param.conv2d.group = 1;
    node[168]->nn_param.conv2d.dilation[0] = 1;
    node[168]->nn_param.conv2d.dilation[1] = 1;
    node[168]->nn_param.conv2d.multiplier = 0;
    node[168]->vx_param.has_relu = FALSE;
    node[168]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[168]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[168]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/add_98_concat_74
      var       - node[169]
      name      - InceptionResnetV1/Repeat_1/block17_10/add_98_concat
      operation - concat
      in_shape  - [[8, 8, 896, 1]]
                  [[8, 8, 896, 1]]
      out_shape - [[8, 8, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[169], VSI_NN_OP_CONCAT, 2, 1, 74);
    node[169]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_81
      var       - node[170]
      name      - InceptionResnetV1/Repeat_1/block17_10/add_98_reshape
      operation - reshape
      in_shape  - [[8, 8, 1792, 1]]
      out_shape - [[64, 896, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[170], VSI_NN_OP_RESHAPE, 1, 1, 81);
    node[170]->nn_param.reshape.size = shape_29;
    node[170]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/add_98_conv_83_InceptionResnetV1/Repeat_1/block17_10/Relu_87
      var       - node[171]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 896, 2, 1]]
      out_shape - [[64, 896, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[171], VSI_NN_OP_CONV_RELU, 2, 1, 87);
    node[171]->nn_param.conv2d.ksize[0] = 1;
    node[171]->nn_param.conv2d.ksize[1] = 1;
    node[171]->nn_param.conv2d.weights = 1;
    node[171]->nn_param.conv2d.stride[0] = 1;
    node[171]->nn_param.conv2d.stride[1] = 1;
    node[171]->nn_param.conv2d.pad[0] = 0;
    node[171]->nn_param.conv2d.pad[1] = 0;
    node[171]->nn_param.conv2d.pad[2] = 0;
    node[171]->nn_param.conv2d.pad[3] = 0;
    node[171]->nn_param.conv2d.group = 1;
    node[171]->nn_param.conv2d.dilation[0] = 1;
    node[171]->nn_param.conv2d.dilation[1] = 1;
    node[171]->nn_param.conv2d.multiplier = 0;
    node[171]->vx_param.has_relu = TRUE;
    node[171]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[171]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[171]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_86
      var       - node[172]
      name      - InceptionResnetV1/Repeat_1/block17_10/add_98_reshape
      operation - reshape
      in_shape  - [[64, 896, 1, 1]]
      out_shape - [[8, 8, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[172], VSI_NN_OP_RESHAPE, 1, 1, 86);
    node[172]->nn_param.reshape.size = shape_30;
    node[172]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_3/MaxPool_1a_3x3/MaxPool_76
      var       - node[173]
      name      - InceptionResnetV1/Mixed_7a/Branch_3/MaxPool_1a_3x3/MaxPool
      operation - pooling
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[3, 3, 896, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[173], VSI_NN_OP_POOL, 1, 1, 76);
    node[173]->nn_param.pool.ksize[0] = 3;
    node[173]->nn_param.pool.ksize[1] = 3;
    node[173]->nn_param.pool.stride[0] = 2;
    node[173]->nn_param.pool.stride[1] = 2;
    node[173]->nn_param.pool.pad[0] = 0;
    node[173]->nn_param.pool.pad[1] = 0;
    node[173]->nn_param.pool.pad[2] = 0;
    node[173]->nn_param.pool.pad[3] = 0;
    node[173]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[173]->nn_param.pool.round_type = VSI_NN_ROUND_FLOOR;
    node[173]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/convolution_131_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/Relu_110
      var       - node[174]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[174], VSI_NN_OP_CONV_RELU, 2, 1, 110);
    node[174]->nn_param.conv2d.ksize[0] = 1;
    node[174]->nn_param.conv2d.ksize[1] = 1;
    node[174]->nn_param.conv2d.weights = 256;
    node[174]->nn_param.conv2d.stride[0] = 1;
    node[174]->nn_param.conv2d.stride[1] = 1;
    node[174]->nn_param.conv2d.pad[0] = 0;
    node[174]->nn_param.conv2d.pad[1] = 0;
    node[174]->nn_param.conv2d.pad[2] = 0;
    node[174]->nn_param.conv2d.pad[3] = 0;
    node[174]->nn_param.conv2d.group = 1;
    node[174]->nn_param.conv2d.dilation[0] = 1;
    node[174]->nn_param.conv2d.dilation[1] = 1;
    node[174]->nn_param.conv2d.multiplier = 0;
    node[174]->vx_param.has_relu = TRUE;
    node[174]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[174]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[174]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/convolution_132_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/Relu_111
      var       - node[175]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[175], VSI_NN_OP_CONV_RELU, 2, 1, 111);
    node[175]->nn_param.conv2d.ksize[0] = 1;
    node[175]->nn_param.conv2d.ksize[1] = 1;
    node[175]->nn_param.conv2d.weights = 256;
    node[175]->nn_param.conv2d.stride[0] = 1;
    node[175]->nn_param.conv2d.stride[1] = 1;
    node[175]->nn_param.conv2d.pad[0] = 0;
    node[175]->nn_param.conv2d.pad[1] = 0;
    node[175]->nn_param.conv2d.pad[2] = 0;
    node[175]->nn_param.conv2d.pad[3] = 0;
    node[175]->nn_param.conv2d.group = 1;
    node[175]->nn_param.conv2d.dilation[0] = 1;
    node[175]->nn_param.conv2d.dilation[1] = 1;
    node[175]->nn_param.conv2d.multiplier = 0;
    node[175]->vx_param.has_relu = TRUE;
    node[175]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[175]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[175]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/convolution_152_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/Relu_136
      var       - node[176]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 896, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[176], VSI_NN_OP_CONV_RELU, 2, 1, 136);
    node[176]->nn_param.conv2d.ksize[0] = 1;
    node[176]->nn_param.conv2d.ksize[1] = 1;
    node[176]->nn_param.conv2d.weights = 256;
    node[176]->nn_param.conv2d.stride[0] = 1;
    node[176]->nn_param.conv2d.stride[1] = 1;
    node[176]->nn_param.conv2d.pad[0] = 0;
    node[176]->nn_param.conv2d.pad[1] = 0;
    node[176]->nn_param.conv2d.pad[2] = 0;
    node[176]->nn_param.conv2d.pad[3] = 0;
    node[176]->nn_param.conv2d.group = 1;
    node[176]->nn_param.conv2d.dilation[0] = 1;
    node[176]->nn_param.conv2d.dilation[1] = 1;
    node[176]->nn_param.conv2d.multiplier = 0;
    node[176]->vx_param.has_relu = TRUE;
    node[176]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[176]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[176]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/convolution_99_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/Relu_77
      var       - node[177]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[177], VSI_NN_OP_CONV_RELU, 2, 1, 77);
    node[177]->nn_param.conv2d.ksize[0] = 3;
    node[177]->nn_param.conv2d.ksize[1] = 3;
    node[177]->nn_param.conv2d.weights = 384;
    node[177]->nn_param.conv2d.stride[0] = 2;
    node[177]->nn_param.conv2d.stride[1] = 2;
    node[177]->nn_param.conv2d.pad[0] = 0;
    node[177]->nn_param.conv2d.pad[1] = 0;
    node[177]->nn_param.conv2d.pad[2] = 0;
    node[177]->nn_param.conv2d.pad[3] = 0;
    node[177]->nn_param.conv2d.group = 1;
    node[177]->nn_param.conv2d.dilation[0] = 1;
    node[177]->nn_param.conv2d.dilation[1] = 1;
    node[177]->nn_param.conv2d.multiplier = 0;
    node[177]->vx_param.has_relu = TRUE;
    node[177]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[177]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[177]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/convolution_100_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/Relu_78
      var       - node[178]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[3, 3, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[178], VSI_NN_OP_CONV_RELU, 2, 1, 78);
    node[178]->nn_param.conv2d.ksize[0] = 3;
    node[178]->nn_param.conv2d.ksize[1] = 3;
    node[178]->nn_param.conv2d.weights = 256;
    node[178]->nn_param.conv2d.stride[0] = 2;
    node[178]->nn_param.conv2d.stride[1] = 2;
    node[178]->nn_param.conv2d.pad[0] = 0;
    node[178]->nn_param.conv2d.pad[1] = 0;
    node[178]->nn_param.conv2d.pad[2] = 0;
    node[178]->nn_param.conv2d.pad[3] = 0;
    node[178]->nn_param.conv2d.group = 1;
    node[178]->nn_param.conv2d.dilation[0] = 1;
    node[178]->nn_param.conv2d.dilation[1] = 1;
    node[178]->nn_param.conv2d.multiplier = 0;
    node[178]->vx_param.has_relu = TRUE;
    node[178]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[178]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[178]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/convolution_127_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/Relu_107
      var       - node[179]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[8, 8, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[179], VSI_NN_OP_CONV_RELU, 2, 1, 107);
    node[179]->nn_param.conv2d.ksize[0] = 3;
    node[179]->nn_param.conv2d.ksize[1] = 3;
    node[179]->nn_param.conv2d.weights = 256;
    node[179]->nn_param.conv2d.stride[0] = 1;
    node[179]->nn_param.conv2d.stride[1] = 1;
    node[179]->nn_param.conv2d.pad[0] = 1;
    node[179]->nn_param.conv2d.pad[1] = 1;
    node[179]->nn_param.conv2d.pad[2] = 1;
    node[179]->nn_param.conv2d.pad[3] = 1;
    node[179]->nn_param.conv2d.group = 1;
    node[179]->nn_param.conv2d.dilation[0] = 1;
    node[179]->nn_param.conv2d.dilation[1] = 1;
    node[179]->nn_param.conv2d.multiplier = 0;
    node[179]->vx_param.has_relu = TRUE;
    node[179]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[179]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[179]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/convolution_97_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/Relu_75
      var       - node[180]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[8, 8, 256, 1]]
      out_shape - [[3, 3, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[180], VSI_NN_OP_CONV_RELU, 2, 1, 75);
    node[180]->nn_param.conv2d.ksize[0] = 3;
    node[180]->nn_param.conv2d.ksize[1] = 3;
    node[180]->nn_param.conv2d.weights = 256;
    node[180]->nn_param.conv2d.stride[0] = 2;
    node[180]->nn_param.conv2d.stride[1] = 2;
    node[180]->nn_param.conv2d.pad[0] = 0;
    node[180]->nn_param.conv2d.pad[1] = 0;
    node[180]->nn_param.conv2d.pad[2] = 0;
    node[180]->nn_param.conv2d.pad[3] = 0;
    node[180]->nn_param.conv2d.group = 1;
    node[180]->nn_param.conv2d.dilation[0] = 1;
    node[180]->nn_param.conv2d.dilation[1] = 1;
    node[180]->nn_param.conv2d.multiplier = 0;
    node[180]->vx_param.has_relu = TRUE;
    node[180]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[180]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[180]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Mixed_7a/concat/axis_66
      var       - node[181]
      name      - InceptionResnetV1/Mixed_7a/concat/axis
      operation - concat
      in_shape  - [[3, 3, 384, 1]]
                  [[3, 3, 256, 1]]
                  [[3, 3, 256, 1]]
                  [[3, 3, 896, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[181], VSI_NN_OP_CONCAT, 4, 1, 66);
    node[181]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/convolution_116_InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/Relu_96
      var       - node[182]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[182], VSI_NN_OP_CONV_RELU, 2, 1, 96);
    node[182]->nn_param.conv2d.ksize[0] = 1;
    node[182]->nn_param.conv2d.ksize[1] = 1;
    node[182]->nn_param.conv2d.weights = 192;
    node[182]->nn_param.conv2d.stride[0] = 1;
    node[182]->nn_param.conv2d.stride[1] = 1;
    node[182]->nn_param.conv2d.pad[0] = 0;
    node[182]->nn_param.conv2d.pad[1] = 0;
    node[182]->nn_param.conv2d.pad[2] = 0;
    node[182]->nn_param.conv2d.pad[3] = 0;
    node[182]->nn_param.conv2d.group = 1;
    node[182]->nn_param.conv2d.dilation[0] = 1;
    node[182]->nn_param.conv2d.dilation[1] = 1;
    node[182]->nn_param.conv2d.multiplier = 0;
    node[182]->vx_param.has_relu = TRUE;
    node[182]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[182]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[182]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/convolution_168_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/Relu_151
      var       - node[183]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[183], VSI_NN_OP_CONV_RELU, 2, 1, 151);
    node[183]->nn_param.conv2d.ksize[0] = 1;
    node[183]->nn_param.conv2d.ksize[1] = 1;
    node[183]->nn_param.conv2d.weights = 192;
    node[183]->nn_param.conv2d.stride[0] = 1;
    node[183]->nn_param.conv2d.stride[1] = 1;
    node[183]->nn_param.conv2d.pad[0] = 0;
    node[183]->nn_param.conv2d.pad[1] = 0;
    node[183]->nn_param.conv2d.pad[2] = 0;
    node[183]->nn_param.conv2d.pad[3] = 0;
    node[183]->nn_param.conv2d.group = 1;
    node[183]->nn_param.conv2d.dilation[0] = 1;
    node[183]->nn_param.conv2d.dilation[1] = 1;
    node[183]->nn_param.conv2d.multiplier = 0;
    node[183]->vx_param.has_relu = TRUE;
    node[183]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[183]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[183]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/convolution_143_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/Relu_126
      var       - node[184]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[184], VSI_NN_OP_CONV_RELU, 2, 1, 126);
    node[184]->nn_param.conv2d.ksize[0] = 3;
    node[184]->nn_param.conv2d.ksize[1] = 3;
    node[184]->nn_param.conv2d.weights = 192;
    node[184]->nn_param.conv2d.stride[0] = 1;
    node[184]->nn_param.conv2d.stride[1] = 1;
    node[184]->nn_param.conv2d.pad[0] = 1;
    node[184]->nn_param.conv2d.pad[1] = 1;
    node[184]->nn_param.conv2d.pad[2] = 1;
    node[184]->nn_param.conv2d.pad[3] = 1;
    node[184]->nn_param.conv2d.group = 1;
    node[184]->nn_param.conv2d.dilation[0] = 1;
    node[184]->nn_param.conv2d.dilation[1] = 1;
    node[184]->nn_param.conv2d.multiplier = 0;
    node[184]->vx_param.has_relu = TRUE;
    node[184]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[184]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[184]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/convolution_115_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/Relu_95
      var       - node[185]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[185], VSI_NN_OP_CONV_RELU, 2, 1, 95);
    node[185]->nn_param.conv2d.ksize[0] = 1;
    node[185]->nn_param.conv2d.ksize[1] = 3;
    node[185]->nn_param.conv2d.weights = 192;
    node[185]->nn_param.conv2d.stride[0] = 1;
    node[185]->nn_param.conv2d.stride[1] = 1;
    node[185]->nn_param.conv2d.pad[0] = 0;
    node[185]->nn_param.conv2d.pad[1] = 0;
    node[185]->nn_param.conv2d.pad[2] = 1;
    node[185]->nn_param.conv2d.pad[3] = 1;
    node[185]->nn_param.conv2d.group = 1;
    node[185]->nn_param.conv2d.dilation[0] = 1;
    node[185]->nn_param.conv2d.dilation[1] = 1;
    node[185]->nn_param.conv2d.multiplier = 0;
    node[185]->vx_param.has_relu = TRUE;
    node[185]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[185]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[185]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/concat_85
      var       - node[186]
      name      - InceptionResnetV1/Repeat_2/block8_1/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[186], VSI_NN_OP_CONCAT, 2, 1, 85);
    node[186]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_2/block8_1/Conv2d_1x1/convolution_73
      var       - node[187]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[187], VSI_NN_OP_CONV_RELU, 2, 1, 73);
    node[187]->nn_param.conv2d.ksize[0] = 1;
    node[187]->nn_param.conv2d.ksize[1] = 1;
    node[187]->nn_param.conv2d.weights = 1792;
    node[187]->nn_param.conv2d.stride[0] = 1;
    node[187]->nn_param.conv2d.stride[1] = 1;
    node[187]->nn_param.conv2d.pad[0] = 0;
    node[187]->nn_param.conv2d.pad[1] = 0;
    node[187]->nn_param.conv2d.pad[2] = 0;
    node[187]->nn_param.conv2d.pad[3] = 0;
    node[187]->nn_param.conv2d.group = 1;
    node[187]->nn_param.conv2d.dilation[0] = 1;
    node[187]->nn_param.conv2d.dilation[1] = 1;
    node[187]->nn_param.conv2d.multiplier = 0;
    node[187]->vx_param.has_relu = FALSE;
    node[187]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[187]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[187]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/add_56_concat_63
      var       - node[188]
      name      - InceptionResnetV1/Repeat_2/block8_1/add_56_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[188], VSI_NN_OP_CONCAT, 2, 1, 63);
    node[188]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_64
      var       - node[189]
      name      - InceptionResnetV1/Repeat_2/block8_1/add_56_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[189], VSI_NN_OP_RESHAPE, 1, 1, 64);
    node[189]->nn_param.reshape.size = shape_31;
    node[189]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/add_56_conv_65_InceptionResnetV1/Repeat_2/block8_1/Relu_48
      var       - node[190]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[190], VSI_NN_OP_CONV_RELU, 2, 1, 48);
    node[190]->nn_param.conv2d.ksize[0] = 1;
    node[190]->nn_param.conv2d.ksize[1] = 1;
    node[190]->nn_param.conv2d.weights = 1;
    node[190]->nn_param.conv2d.stride[0] = 1;
    node[190]->nn_param.conv2d.stride[1] = 1;
    node[190]->nn_param.conv2d.pad[0] = 0;
    node[190]->nn_param.conv2d.pad[1] = 0;
    node[190]->nn_param.conv2d.pad[2] = 0;
    node[190]->nn_param.conv2d.pad[3] = 0;
    node[190]->nn_param.conv2d.group = 1;
    node[190]->nn_param.conv2d.dilation[0] = 1;
    node[190]->nn_param.conv2d.dilation[1] = 1;
    node[190]->nn_param.conv2d.multiplier = 0;
    node[190]->vx_param.has_relu = TRUE;
    node[190]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[190]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[190]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_69
      var       - node[191]
      name      - InceptionResnetV1/Repeat_2/block8_1/add_56_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[191], VSI_NN_OP_RESHAPE, 1, 1, 69);
    node[191]->nn_param.reshape.size = shape_32;
    node[191]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/convolution_101_InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/Relu_79
      var       - node[192]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[192], VSI_NN_OP_CONV_RELU, 2, 1, 79);
    node[192]->nn_param.conv2d.ksize[0] = 1;
    node[192]->nn_param.conv2d.ksize[1] = 1;
    node[192]->nn_param.conv2d.weights = 192;
    node[192]->nn_param.conv2d.stride[0] = 1;
    node[192]->nn_param.conv2d.stride[1] = 1;
    node[192]->nn_param.conv2d.pad[0] = 0;
    node[192]->nn_param.conv2d.pad[1] = 0;
    node[192]->nn_param.conv2d.pad[2] = 0;
    node[192]->nn_param.conv2d.pad[3] = 0;
    node[192]->nn_param.conv2d.group = 1;
    node[192]->nn_param.conv2d.dilation[0] = 1;
    node[192]->nn_param.conv2d.dilation[1] = 1;
    node[192]->nn_param.conv2d.multiplier = 0;
    node[192]->vx_param.has_relu = TRUE;
    node[192]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[192]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[192]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/convolution_160_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/Relu_142
      var       - node[193]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[193], VSI_NN_OP_CONV_RELU, 2, 1, 142);
    node[193]->nn_param.conv2d.ksize[0] = 1;
    node[193]->nn_param.conv2d.ksize[1] = 1;
    node[193]->nn_param.conv2d.weights = 192;
    node[193]->nn_param.conv2d.stride[0] = 1;
    node[193]->nn_param.conv2d.stride[1] = 1;
    node[193]->nn_param.conv2d.pad[0] = 0;
    node[193]->nn_param.conv2d.pad[1] = 0;
    node[193]->nn_param.conv2d.pad[2] = 0;
    node[193]->nn_param.conv2d.pad[3] = 0;
    node[193]->nn_param.conv2d.group = 1;
    node[193]->nn_param.conv2d.dilation[0] = 1;
    node[193]->nn_param.conv2d.dilation[1] = 1;
    node[193]->nn_param.conv2d.multiplier = 0;
    node[193]->vx_param.has_relu = TRUE;
    node[193]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[193]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[193]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/convolution_133_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/Relu_112
      var       - node[194]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[194], VSI_NN_OP_CONV_RELU, 2, 1, 112);
    node[194]->nn_param.conv2d.ksize[0] = 3;
    node[194]->nn_param.conv2d.ksize[1] = 3;
    node[194]->nn_param.conv2d.weights = 192;
    node[194]->nn_param.conv2d.stride[0] = 1;
    node[194]->nn_param.conv2d.stride[1] = 1;
    node[194]->nn_param.conv2d.pad[0] = 1;
    node[194]->nn_param.conv2d.pad[1] = 1;
    node[194]->nn_param.conv2d.pad[2] = 1;
    node[194]->nn_param.conv2d.pad[3] = 1;
    node[194]->nn_param.conv2d.group = 1;
    node[194]->nn_param.conv2d.dilation[0] = 1;
    node[194]->nn_param.conv2d.dilation[1] = 1;
    node[194]->nn_param.conv2d.multiplier = 0;
    node[194]->vx_param.has_relu = TRUE;
    node[194]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[194]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[194]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/convolution_102_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/Relu_80
      var       - node[195]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[195], VSI_NN_OP_CONV_RELU, 2, 1, 80);
    node[195]->nn_param.conv2d.ksize[0] = 1;
    node[195]->nn_param.conv2d.ksize[1] = 3;
    node[195]->nn_param.conv2d.weights = 192;
    node[195]->nn_param.conv2d.stride[0] = 1;
    node[195]->nn_param.conv2d.stride[1] = 1;
    node[195]->nn_param.conv2d.pad[0] = 0;
    node[195]->nn_param.conv2d.pad[1] = 0;
    node[195]->nn_param.conv2d.pad[2] = 1;
    node[195]->nn_param.conv2d.pad[3] = 1;
    node[195]->nn_param.conv2d.group = 1;
    node[195]->nn_param.conv2d.dilation[0] = 1;
    node[195]->nn_param.conv2d.dilation[1] = 1;
    node[195]->nn_param.conv2d.multiplier = 0;
    node[195]->vx_param.has_relu = TRUE;
    node[195]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[195]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[195]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/concat_67
      var       - node[196]
      name      - InceptionResnetV1/Repeat_2/block8_2/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[196], VSI_NN_OP_CONCAT, 2, 1, 67);
    node[196]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_2/block8_2/Conv2d_1x1/convolution_57
      var       - node[197]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[197], VSI_NN_OP_CONV_RELU, 2, 1, 57);
    node[197]->nn_param.conv2d.ksize[0] = 1;
    node[197]->nn_param.conv2d.ksize[1] = 1;
    node[197]->nn_param.conv2d.weights = 1792;
    node[197]->nn_param.conv2d.stride[0] = 1;
    node[197]->nn_param.conv2d.stride[1] = 1;
    node[197]->nn_param.conv2d.pad[0] = 0;
    node[197]->nn_param.conv2d.pad[1] = 0;
    node[197]->nn_param.conv2d.pad[2] = 0;
    node[197]->nn_param.conv2d.pad[3] = 0;
    node[197]->nn_param.conv2d.group = 1;
    node[197]->nn_param.conv2d.dilation[0] = 1;
    node[197]->nn_param.conv2d.dilation[1] = 1;
    node[197]->nn_param.conv2d.multiplier = 0;
    node[197]->vx_param.has_relu = FALSE;
    node[197]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[197]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[197]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/add_42_concat_50
      var       - node[198]
      name      - InceptionResnetV1/Repeat_2/block8_2/add_42_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[198], VSI_NN_OP_CONCAT, 2, 1, 50);
    node[198]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_51
      var       - node[199]
      name      - InceptionResnetV1/Repeat_2/block8_2/add_42_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[199], VSI_NN_OP_RESHAPE, 1, 1, 51);
    node[199]->nn_param.reshape.size = shape_33;
    node[199]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/add_42_conv_53_InceptionResnetV1/Repeat_2/block8_2/Relu_35
      var       - node[200]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[200], VSI_NN_OP_CONV_RELU, 2, 1, 35);
    node[200]->nn_param.conv2d.ksize[0] = 1;
    node[200]->nn_param.conv2d.ksize[1] = 1;
    node[200]->nn_param.conv2d.weights = 1;
    node[200]->nn_param.conv2d.stride[0] = 1;
    node[200]->nn_param.conv2d.stride[1] = 1;
    node[200]->nn_param.conv2d.pad[0] = 0;
    node[200]->nn_param.conv2d.pad[1] = 0;
    node[200]->nn_param.conv2d.pad[2] = 0;
    node[200]->nn_param.conv2d.pad[3] = 0;
    node[200]->nn_param.conv2d.group = 1;
    node[200]->nn_param.conv2d.dilation[0] = 1;
    node[200]->nn_param.conv2d.dilation[1] = 1;
    node[200]->nn_param.conv2d.multiplier = 0;
    node[200]->vx_param.has_relu = TRUE;
    node[200]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[200]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[200]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_58
      var       - node[201]
      name      - InceptionResnetV1/Repeat_2/block8_2/add_42_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[201], VSI_NN_OP_RESHAPE, 1, 1, 58);
    node[201]->nn_param.reshape.size = shape_34;
    node[201]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/convolution_71_InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/Relu_54
      var       - node[202]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[202], VSI_NN_OP_CONV_RELU, 2, 1, 54);
    node[202]->nn_param.conv2d.ksize[0] = 1;
    node[202]->nn_param.conv2d.ksize[1] = 1;
    node[202]->nn_param.conv2d.weights = 192;
    node[202]->nn_param.conv2d.stride[0] = 1;
    node[202]->nn_param.conv2d.stride[1] = 1;
    node[202]->nn_param.conv2d.pad[0] = 0;
    node[202]->nn_param.conv2d.pad[1] = 0;
    node[202]->nn_param.conv2d.pad[2] = 0;
    node[202]->nn_param.conv2d.pad[3] = 0;
    node[202]->nn_param.conv2d.group = 1;
    node[202]->nn_param.conv2d.dilation[0] = 1;
    node[202]->nn_param.conv2d.dilation[1] = 1;
    node[202]->nn_param.conv2d.multiplier = 0;
    node[202]->vx_param.has_relu = TRUE;
    node[202]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[202]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[202]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/convolution_134_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/Relu_114
      var       - node[203]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[203], VSI_NN_OP_CONV_RELU, 2, 1, 114);
    node[203]->nn_param.conv2d.ksize[0] = 1;
    node[203]->nn_param.conv2d.ksize[1] = 1;
    node[203]->nn_param.conv2d.weights = 192;
    node[203]->nn_param.conv2d.stride[0] = 1;
    node[203]->nn_param.conv2d.stride[1] = 1;
    node[203]->nn_param.conv2d.pad[0] = 0;
    node[203]->nn_param.conv2d.pad[1] = 0;
    node[203]->nn_param.conv2d.pad[2] = 0;
    node[203]->nn_param.conv2d.pad[3] = 0;
    node[203]->nn_param.conv2d.group = 1;
    node[203]->nn_param.conv2d.dilation[0] = 1;
    node[203]->nn_param.conv2d.dilation[1] = 1;
    node[203]->nn_param.conv2d.multiplier = 0;
    node[203]->vx_param.has_relu = TRUE;
    node[203]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[203]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[203]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/convolution_104_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/Relu_84
      var       - node[204]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[204], VSI_NN_OP_CONV_RELU, 2, 1, 84);
    node[204]->nn_param.conv2d.ksize[0] = 3;
    node[204]->nn_param.conv2d.ksize[1] = 3;
    node[204]->nn_param.conv2d.weights = 192;
    node[204]->nn_param.conv2d.stride[0] = 1;
    node[204]->nn_param.conv2d.stride[1] = 1;
    node[204]->nn_param.conv2d.pad[0] = 1;
    node[204]->nn_param.conv2d.pad[1] = 1;
    node[204]->nn_param.conv2d.pad[2] = 1;
    node[204]->nn_param.conv2d.pad[3] = 1;
    node[204]->nn_param.conv2d.group = 1;
    node[204]->nn_param.conv2d.dilation[0] = 1;
    node[204]->nn_param.conv2d.dilation[1] = 1;
    node[204]->nn_param.conv2d.multiplier = 0;
    node[204]->vx_param.has_relu = TRUE;
    node[204]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[204]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[204]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/convolution_72_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/Relu_55
      var       - node[205]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[205], VSI_NN_OP_CONV_RELU, 2, 1, 55);
    node[205]->nn_param.conv2d.ksize[0] = 1;
    node[205]->nn_param.conv2d.ksize[1] = 3;
    node[205]->nn_param.conv2d.weights = 192;
    node[205]->nn_param.conv2d.stride[0] = 1;
    node[205]->nn_param.conv2d.stride[1] = 1;
    node[205]->nn_param.conv2d.pad[0] = 0;
    node[205]->nn_param.conv2d.pad[1] = 0;
    node[205]->nn_param.conv2d.pad[2] = 1;
    node[205]->nn_param.conv2d.pad[3] = 1;
    node[205]->nn_param.conv2d.group = 1;
    node[205]->nn_param.conv2d.dilation[0] = 1;
    node[205]->nn_param.conv2d.dilation[1] = 1;
    node[205]->nn_param.conv2d.multiplier = 0;
    node[205]->vx_param.has_relu = TRUE;
    node[205]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[205]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[205]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/concat_47
      var       - node[206]
      name      - InceptionResnetV1/Repeat_2/block8_3/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[206], VSI_NN_OP_CONCAT, 2, 1, 47);
    node[206]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_2/block8_3/Conv2d_1x1/convolution_41
      var       - node[207]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[207], VSI_NN_OP_CONV_RELU, 2, 1, 41);
    node[207]->nn_param.conv2d.ksize[0] = 1;
    node[207]->nn_param.conv2d.ksize[1] = 1;
    node[207]->nn_param.conv2d.weights = 1792;
    node[207]->nn_param.conv2d.stride[0] = 1;
    node[207]->nn_param.conv2d.stride[1] = 1;
    node[207]->nn_param.conv2d.pad[0] = 0;
    node[207]->nn_param.conv2d.pad[1] = 0;
    node[207]->nn_param.conv2d.pad[2] = 0;
    node[207]->nn_param.conv2d.pad[3] = 0;
    node[207]->nn_param.conv2d.group = 1;
    node[207]->nn_param.conv2d.dilation[0] = 1;
    node[207]->nn_param.conv2d.dilation[1] = 1;
    node[207]->nn_param.conv2d.multiplier = 0;
    node[207]->vx_param.has_relu = FALSE;
    node[207]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[207]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[207]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/add_27_concat_34
      var       - node[208]
      name      - InceptionResnetV1/Repeat_2/block8_3/add_27_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[208], VSI_NN_OP_CONCAT, 2, 1, 34);
    node[208]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_40
      var       - node[209]
      name      - InceptionResnetV1/Repeat_2/block8_3/add_27_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[209], VSI_NN_OP_RESHAPE, 1, 1, 40);
    node[209]->nn_param.reshape.size = shape_35;
    node[209]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/add_27_conv_45_InceptionResnetV1/Repeat_2/block8_3/Relu_21
      var       - node[210]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[210], VSI_NN_OP_CONV_RELU, 2, 1, 21);
    node[210]->nn_param.conv2d.ksize[0] = 1;
    node[210]->nn_param.conv2d.ksize[1] = 1;
    node[210]->nn_param.conv2d.weights = 1;
    node[210]->nn_param.conv2d.stride[0] = 1;
    node[210]->nn_param.conv2d.stride[1] = 1;
    node[210]->nn_param.conv2d.pad[0] = 0;
    node[210]->nn_param.conv2d.pad[1] = 0;
    node[210]->nn_param.conv2d.pad[2] = 0;
    node[210]->nn_param.conv2d.pad[3] = 0;
    node[210]->nn_param.conv2d.group = 1;
    node[210]->nn_param.conv2d.dilation[0] = 1;
    node[210]->nn_param.conv2d.dilation[1] = 1;
    node[210]->nn_param.conv2d.multiplier = 0;
    node[210]->vx_param.has_relu = TRUE;
    node[210]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[210]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[210]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_49
      var       - node[211]
      name      - InceptionResnetV1/Repeat_2/block8_3/add_27_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[211], VSI_NN_OP_RESHAPE, 1, 1, 49);
    node[211]->nn_param.reshape.size = shape_36;
    node[211]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/convolution_60_InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/Relu_44
      var       - node[212]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[212], VSI_NN_OP_CONV_RELU, 2, 1, 44);
    node[212]->nn_param.conv2d.ksize[0] = 1;
    node[212]->nn_param.conv2d.ksize[1] = 1;
    node[212]->nn_param.conv2d.weights = 192;
    node[212]->nn_param.conv2d.stride[0] = 1;
    node[212]->nn_param.conv2d.stride[1] = 1;
    node[212]->nn_param.conv2d.pad[0] = 0;
    node[212]->nn_param.conv2d.pad[1] = 0;
    node[212]->nn_param.conv2d.pad[2] = 0;
    node[212]->nn_param.conv2d.pad[3] = 0;
    node[212]->nn_param.conv2d.group = 1;
    node[212]->nn_param.conv2d.dilation[0] = 1;
    node[212]->nn_param.conv2d.dilation[1] = 1;
    node[212]->nn_param.conv2d.multiplier = 0;
    node[212]->vx_param.has_relu = TRUE;
    node[212]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[212]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[212]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/convolution_124_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/Relu_103
      var       - node[213]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[213], VSI_NN_OP_CONV_RELU, 2, 1, 103);
    node[213]->nn_param.conv2d.ksize[0] = 1;
    node[213]->nn_param.conv2d.ksize[1] = 1;
    node[213]->nn_param.conv2d.weights = 192;
    node[213]->nn_param.conv2d.stride[0] = 1;
    node[213]->nn_param.conv2d.stride[1] = 1;
    node[213]->nn_param.conv2d.pad[0] = 0;
    node[213]->nn_param.conv2d.pad[1] = 0;
    node[213]->nn_param.conv2d.pad[2] = 0;
    node[213]->nn_param.conv2d.pad[3] = 0;
    node[213]->nn_param.conv2d.group = 1;
    node[213]->nn_param.conv2d.dilation[0] = 1;
    node[213]->nn_param.conv2d.dilation[1] = 1;
    node[213]->nn_param.conv2d.multiplier = 0;
    node[213]->vx_param.has_relu = TRUE;
    node[213]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[213]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[213]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/convolution_92_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/Relu_68
      var       - node[214]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[214], VSI_NN_OP_CONV_RELU, 2, 1, 68);
    node[214]->nn_param.conv2d.ksize[0] = 3;
    node[214]->nn_param.conv2d.ksize[1] = 3;
    node[214]->nn_param.conv2d.weights = 192;
    node[214]->nn_param.conv2d.stride[0] = 1;
    node[214]->nn_param.conv2d.stride[1] = 1;
    node[214]->nn_param.conv2d.pad[0] = 1;
    node[214]->nn_param.conv2d.pad[1] = 1;
    node[214]->nn_param.conv2d.pad[2] = 1;
    node[214]->nn_param.conv2d.pad[3] = 1;
    node[214]->nn_param.conv2d.group = 1;
    node[214]->nn_param.conv2d.dilation[0] = 1;
    node[214]->nn_param.conv2d.dilation[1] = 1;
    node[214]->nn_param.conv2d.multiplier = 0;
    node[214]->vx_param.has_relu = TRUE;
    node[214]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[214]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[214]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/convolution_59_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/Relu_43
      var       - node[215]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[215], VSI_NN_OP_CONV_RELU, 2, 1, 43);
    node[215]->nn_param.conv2d.ksize[0] = 1;
    node[215]->nn_param.conv2d.ksize[1] = 3;
    node[215]->nn_param.conv2d.weights = 192;
    node[215]->nn_param.conv2d.stride[0] = 1;
    node[215]->nn_param.conv2d.stride[1] = 1;
    node[215]->nn_param.conv2d.pad[0] = 0;
    node[215]->nn_param.conv2d.pad[1] = 0;
    node[215]->nn_param.conv2d.pad[2] = 1;
    node[215]->nn_param.conv2d.pad[3] = 1;
    node[215]->nn_param.conv2d.group = 1;
    node[215]->nn_param.conv2d.dilation[0] = 1;
    node[215]->nn_param.conv2d.dilation[1] = 1;
    node[215]->nn_param.conv2d.multiplier = 0;
    node[215]->vx_param.has_relu = TRUE;
    node[215]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[215]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[215]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/concat_36
      var       - node[216]
      name      - InceptionResnetV1/Repeat_2/block8_4/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[216], VSI_NN_OP_CONCAT, 2, 1, 36);
    node[216]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_2/block8_4/Conv2d_1x1/convolution_28
      var       - node[217]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[217], VSI_NN_OP_CONV_RELU, 2, 1, 28);
    node[217]->nn_param.conv2d.ksize[0] = 1;
    node[217]->nn_param.conv2d.ksize[1] = 1;
    node[217]->nn_param.conv2d.weights = 1792;
    node[217]->nn_param.conv2d.stride[0] = 1;
    node[217]->nn_param.conv2d.stride[1] = 1;
    node[217]->nn_param.conv2d.pad[0] = 0;
    node[217]->nn_param.conv2d.pad[1] = 0;
    node[217]->nn_param.conv2d.pad[2] = 0;
    node[217]->nn_param.conv2d.pad[3] = 0;
    node[217]->nn_param.conv2d.group = 1;
    node[217]->nn_param.conv2d.dilation[0] = 1;
    node[217]->nn_param.conv2d.dilation[1] = 1;
    node[217]->nn_param.conv2d.multiplier = 0;
    node[217]->vx_param.has_relu = FALSE;
    node[217]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[217]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[217]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/add_17_concat_24
      var       - node[218]
      name      - InceptionResnetV1/Repeat_2/block8_4/add_17_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[218], VSI_NN_OP_CONCAT, 2, 1, 24);
    node[218]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_29
      var       - node[219]
      name      - InceptionResnetV1/Repeat_2/block8_4/add_17_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[219], VSI_NN_OP_RESHAPE, 1, 1, 29);
    node[219]->nn_param.reshape.size = shape_37;
    node[219]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/add_17_conv_32_InceptionResnetV1/Repeat_2/block8_4/Relu_13
      var       - node[220]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[220], VSI_NN_OP_CONV_RELU, 2, 1, 13);
    node[220]->nn_param.conv2d.ksize[0] = 1;
    node[220]->nn_param.conv2d.ksize[1] = 1;
    node[220]->nn_param.conv2d.weights = 1;
    node[220]->nn_param.conv2d.stride[0] = 1;
    node[220]->nn_param.conv2d.stride[1] = 1;
    node[220]->nn_param.conv2d.pad[0] = 0;
    node[220]->nn_param.conv2d.pad[1] = 0;
    node[220]->nn_param.conv2d.pad[2] = 0;
    node[220]->nn_param.conv2d.pad[3] = 0;
    node[220]->nn_param.conv2d.group = 1;
    node[220]->nn_param.conv2d.dilation[0] = 1;
    node[220]->nn_param.conv2d.dilation[1] = 1;
    node[220]->nn_param.conv2d.multiplier = 0;
    node[220]->vx_param.has_relu = TRUE;
    node[220]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[220]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[220]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_33
      var       - node[221]
      name      - InceptionResnetV1/Repeat_2/block8_4/add_17_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[221], VSI_NN_OP_RESHAPE, 1, 1, 33);
    node[221]->nn_param.reshape.size = shape_38;
    node[221]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/convolution_39_InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/Relu_26
      var       - node[222]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[222], VSI_NN_OP_CONV_RELU, 2, 1, 26);
    node[222]->nn_param.conv2d.ksize[0] = 1;
    node[222]->nn_param.conv2d.ksize[1] = 1;
    node[222]->nn_param.conv2d.weights = 192;
    node[222]->nn_param.conv2d.stride[0] = 1;
    node[222]->nn_param.conv2d.stride[1] = 1;
    node[222]->nn_param.conv2d.pad[0] = 0;
    node[222]->nn_param.conv2d.pad[1] = 0;
    node[222]->nn_param.conv2d.pad[2] = 0;
    node[222]->nn_param.conv2d.pad[3] = 0;
    node[222]->nn_param.conv2d.group = 1;
    node[222]->nn_param.conv2d.dilation[0] = 1;
    node[222]->nn_param.conv2d.dilation[1] = 1;
    node[222]->nn_param.conv2d.multiplier = 0;
    node[222]->vx_param.has_relu = TRUE;
    node[222]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[222]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[222]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/convolution_93_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/Relu_70
      var       - node[223]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[223], VSI_NN_OP_CONV_RELU, 2, 1, 70);
    node[223]->nn_param.conv2d.ksize[0] = 1;
    node[223]->nn_param.conv2d.ksize[1] = 1;
    node[223]->nn_param.conv2d.weights = 192;
    node[223]->nn_param.conv2d.stride[0] = 1;
    node[223]->nn_param.conv2d.stride[1] = 1;
    node[223]->nn_param.conv2d.pad[0] = 0;
    node[223]->nn_param.conv2d.pad[1] = 0;
    node[223]->nn_param.conv2d.pad[2] = 0;
    node[223]->nn_param.conv2d.pad[3] = 0;
    node[223]->nn_param.conv2d.group = 1;
    node[223]->nn_param.conv2d.dilation[0] = 1;
    node[223]->nn_param.conv2d.dilation[1] = 1;
    node[223]->nn_param.conv2d.multiplier = 0;
    node[223]->vx_param.has_relu = TRUE;
    node[223]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[223]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[223]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/convolution_62_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/Relu_46
      var       - node[224]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[224], VSI_NN_OP_CONV_RELU, 2, 1, 46);
    node[224]->nn_param.conv2d.ksize[0] = 3;
    node[224]->nn_param.conv2d.ksize[1] = 3;
    node[224]->nn_param.conv2d.weights = 192;
    node[224]->nn_param.conv2d.stride[0] = 1;
    node[224]->nn_param.conv2d.stride[1] = 1;
    node[224]->nn_param.conv2d.pad[0] = 1;
    node[224]->nn_param.conv2d.pad[1] = 1;
    node[224]->nn_param.conv2d.pad[2] = 1;
    node[224]->nn_param.conv2d.pad[3] = 1;
    node[224]->nn_param.conv2d.group = 1;
    node[224]->nn_param.conv2d.dilation[0] = 1;
    node[224]->nn_param.conv2d.dilation[1] = 1;
    node[224]->nn_param.conv2d.multiplier = 0;
    node[224]->vx_param.has_relu = TRUE;
    node[224]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[224]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[224]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/convolution_38_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/Relu_25
      var       - node[225]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[225], VSI_NN_OP_CONV_RELU, 2, 1, 25);
    node[225]->nn_param.conv2d.ksize[0] = 1;
    node[225]->nn_param.conv2d.ksize[1] = 3;
    node[225]->nn_param.conv2d.weights = 192;
    node[225]->nn_param.conv2d.stride[0] = 1;
    node[225]->nn_param.conv2d.stride[1] = 1;
    node[225]->nn_param.conv2d.pad[0] = 0;
    node[225]->nn_param.conv2d.pad[1] = 0;
    node[225]->nn_param.conv2d.pad[2] = 1;
    node[225]->nn_param.conv2d.pad[3] = 1;
    node[225]->nn_param.conv2d.group = 1;
    node[225]->nn_param.conv2d.dilation[0] = 1;
    node[225]->nn_param.conv2d.dilation[1] = 1;
    node[225]->nn_param.conv2d.multiplier = 0;
    node[225]->vx_param.has_relu = TRUE;
    node[225]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[225]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[225]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/concat_20
      var       - node[226]
      name      - InceptionResnetV1/Repeat_2/block8_5/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[226], VSI_NN_OP_CONCAT, 2, 1, 20);
    node[226]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Repeat_2/block8_5/Conv2d_1x1/convolution_16
      var       - node[227]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[227], VSI_NN_OP_CONV_RELU, 2, 1, 16);
    node[227]->nn_param.conv2d.ksize[0] = 1;
    node[227]->nn_param.conv2d.ksize[1] = 1;
    node[227]->nn_param.conv2d.weights = 1792;
    node[227]->nn_param.conv2d.stride[0] = 1;
    node[227]->nn_param.conv2d.stride[1] = 1;
    node[227]->nn_param.conv2d.pad[0] = 0;
    node[227]->nn_param.conv2d.pad[1] = 0;
    node[227]->nn_param.conv2d.pad[2] = 0;
    node[227]->nn_param.conv2d.pad[3] = 0;
    node[227]->nn_param.conv2d.group = 1;
    node[227]->nn_param.conv2d.dilation[0] = 1;
    node[227]->nn_param.conv2d.dilation[1] = 1;
    node[227]->nn_param.conv2d.multiplier = 0;
    node[227]->vx_param.has_relu = FALSE;
    node[227]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[227]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[227]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/add_9_concat_12
      var       - node[228]
      name      - InceptionResnetV1/Repeat_2/block8_5/add_9_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[228], VSI_NN_OP_CONCAT, 2, 1, 12);
    node[228]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_15
      var       - node[229]
      name      - InceptionResnetV1/Repeat_2/block8_5/add_9_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[229], VSI_NN_OP_RESHAPE, 1, 1, 15);
    node[229]->nn_param.reshape.size = shape_39;
    node[229]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/add_9_conv_22_InceptionResnetV1/Repeat_2/block8_5/Relu_7
      var       - node[230]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[230], VSI_NN_OP_CONV_RELU, 2, 1, 7);
    node[230]->nn_param.conv2d.ksize[0] = 1;
    node[230]->nn_param.conv2d.ksize[1] = 1;
    node[230]->nn_param.conv2d.weights = 1;
    node[230]->nn_param.conv2d.stride[0] = 1;
    node[230]->nn_param.conv2d.stride[1] = 1;
    node[230]->nn_param.conv2d.pad[0] = 0;
    node[230]->nn_param.conv2d.pad[1] = 0;
    node[230]->nn_param.conv2d.pad[2] = 0;
    node[230]->nn_param.conv2d.pad[3] = 0;
    node[230]->nn_param.conv2d.group = 1;
    node[230]->nn_param.conv2d.dilation[0] = 1;
    node[230]->nn_param.conv2d.dilation[1] = 1;
    node[230]->nn_param.conv2d.multiplier = 0;
    node[230]->vx_param.has_relu = TRUE;
    node[230]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[230]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[230]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_23
      var       - node[231]
      name      - InceptionResnetV1/Repeat_2/block8_5/add_9_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[231], VSI_NN_OP_RESHAPE, 1, 1, 23);
    node[231]->nn_param.reshape.size = shape_40;
    node[231]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/convolution_30_InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/Relu_18
      var       - node[232]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[232], VSI_NN_OP_CONV_RELU, 2, 1, 18);
    node[232]->nn_param.conv2d.ksize[0] = 1;
    node[232]->nn_param.conv2d.ksize[1] = 1;
    node[232]->nn_param.conv2d.weights = 192;
    node[232]->nn_param.conv2d.stride[0] = 1;
    node[232]->nn_param.conv2d.stride[1] = 1;
    node[232]->nn_param.conv2d.pad[0] = 0;
    node[232]->nn_param.conv2d.pad[1] = 0;
    node[232]->nn_param.conv2d.pad[2] = 0;
    node[232]->nn_param.conv2d.pad[3] = 0;
    node[232]->nn_param.conv2d.group = 1;
    node[232]->nn_param.conv2d.dilation[0] = 1;
    node[232]->nn_param.conv2d.dilation[1] = 1;
    node[232]->nn_param.conv2d.multiplier = 0;
    node[232]->vx_param.has_relu = TRUE;
    node[232]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[232]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[232]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/convolution_82_InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/Relu_61
      var       - node[233]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[233], VSI_NN_OP_CONV_RELU, 2, 1, 61);
    node[233]->nn_param.conv2d.ksize[0] = 1;
    node[233]->nn_param.conv2d.ksize[1] = 1;
    node[233]->nn_param.conv2d.weights = 192;
    node[233]->nn_param.conv2d.stride[0] = 1;
    node[233]->nn_param.conv2d.stride[1] = 1;
    node[233]->nn_param.conv2d.pad[0] = 0;
    node[233]->nn_param.conv2d.pad[1] = 0;
    node[233]->nn_param.conv2d.pad[2] = 0;
    node[233]->nn_param.conv2d.pad[3] = 0;
    node[233]->nn_param.conv2d.group = 1;
    node[233]->nn_param.conv2d.dilation[0] = 1;
    node[233]->nn_param.conv2d.dilation[1] = 1;
    node[233]->nn_param.conv2d.multiplier = 0;
    node[233]->vx_param.has_relu = TRUE;
    node[233]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[233]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[233]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/convolution_52_InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/Relu_37
      var       - node[234]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[234], VSI_NN_OP_CONV_RELU, 2, 1, 37);
    node[234]->nn_param.conv2d.ksize[0] = 3;
    node[234]->nn_param.conv2d.ksize[1] = 3;
    node[234]->nn_param.conv2d.weights = 192;
    node[234]->nn_param.conv2d.stride[0] = 1;
    node[234]->nn_param.conv2d.stride[1] = 1;
    node[234]->nn_param.conv2d.pad[0] = 1;
    node[234]->nn_param.conv2d.pad[1] = 1;
    node[234]->nn_param.conv2d.pad[2] = 1;
    node[234]->nn_param.conv2d.pad[3] = 1;
    node[234]->nn_param.conv2d.group = 1;
    node[234]->nn_param.conv2d.dilation[0] = 1;
    node[234]->nn_param.conv2d.dilation[1] = 1;
    node[234]->nn_param.conv2d.multiplier = 0;
    node[234]->vx_param.has_relu = TRUE;
    node[234]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[234]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[234]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/convolution_31_InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/Relu_19
      var       - node[235]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 192, 1]]
      out_shape - [[3, 3, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[235], VSI_NN_OP_CONV_RELU, 2, 1, 19);
    node[235]->nn_param.conv2d.ksize[0] = 1;
    node[235]->nn_param.conv2d.ksize[1] = 3;
    node[235]->nn_param.conv2d.weights = 192;
    node[235]->nn_param.conv2d.stride[0] = 1;
    node[235]->nn_param.conv2d.stride[1] = 1;
    node[235]->nn_param.conv2d.pad[0] = 0;
    node[235]->nn_param.conv2d.pad[1] = 0;
    node[235]->nn_param.conv2d.pad[2] = 1;
    node[235]->nn_param.conv2d.pad[3] = 1;
    node[235]->nn_param.conv2d.group = 1;
    node[235]->nn_param.conv2d.dilation[0] = 1;
    node[235]->nn_param.conv2d.dilation[1] = 1;
    node[235]->nn_param.conv2d.multiplier = 0;
    node[235]->vx_param.has_relu = TRUE;
    node[235]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[235]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[235]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/concat_14
      var       - node[236]
      name      - InceptionResnetV1/Block8/concat
      operation - concat
      in_shape  - [[3, 3, 192, 1]]
                  [[3, 3, 192, 1]]
      out_shape - [[3, 3, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[236], VSI_NN_OP_CONCAT, 2, 1, 14);
    node[236]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Block8/Conv2d_1x1/convolution_11
      var       - node[237]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[3, 3, 384, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[237], VSI_NN_OP_CONV_RELU, 2, 1, 11);
    node[237]->nn_param.conv2d.ksize[0] = 1;
    node[237]->nn_param.conv2d.ksize[1] = 1;
    node[237]->nn_param.conv2d.weights = 1792;
    node[237]->nn_param.conv2d.stride[0] = 1;
    node[237]->nn_param.conv2d.stride[1] = 1;
    node[237]->nn_param.conv2d.pad[0] = 0;
    node[237]->nn_param.conv2d.pad[1] = 0;
    node[237]->nn_param.conv2d.pad[2] = 0;
    node[237]->nn_param.conv2d.pad[3] = 0;
    node[237]->nn_param.conv2d.group = 1;
    node[237]->nn_param.conv2d.dilation[0] = 1;
    node[237]->nn_param.conv2d.dilation[1] = 1;
    node[237]->nn_param.conv2d.multiplier = 0;
    node[237]->vx_param.has_relu = FALSE;
    node[237]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[237]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[237]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/add_6_concat_2
      var       - node[238]
      name      - InceptionResnetV1/Block8/add_6_concat
      operation - concat
      in_shape  - [[3, 3, 1792, 1]]
                  [[3, 3, 1792, 1]]
      out_shape - [[3, 3, 3584, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[238], VSI_NN_OP_CONCAT, 2, 1, 2);
    node[238]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/add_6_reshape_5
      var       - node[239]
      name      - InceptionResnetV1/Block8/add_6_reshape
      operation - reshape
      in_shape  - [[3, 3, 3584, 1]]
      out_shape - [[64, 252, 2, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[239], VSI_NN_OP_RESHAPE, 1, 1, 5);
    node[239]->nn_param.reshape.size = shape_41;
    node[239]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Block8/add_6_conv_8
      var       - node[240]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[64, 252, 2, 1]]
      out_shape - [[64, 252, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[240], VSI_NN_OP_CONV_RELU, 2, 1, 8);
    node[240]->nn_param.conv2d.ksize[0] = 1;
    node[240]->nn_param.conv2d.ksize[1] = 1;
    node[240]->nn_param.conv2d.weights = 1;
    node[240]->nn_param.conv2d.stride[0] = 1;
    node[240]->nn_param.conv2d.stride[1] = 1;
    node[240]->nn_param.conv2d.pad[0] = 0;
    node[240]->nn_param.conv2d.pad[1] = 0;
    node[240]->nn_param.conv2d.pad[2] = 0;
    node[240]->nn_param.conv2d.pad[3] = 0;
    node[240]->nn_param.conv2d.group = 1;
    node[240]->nn_param.conv2d.dilation[0] = 1;
    node[240]->nn_param.conv2d.dilation[1] = 1;
    node[240]->nn_param.conv2d.multiplier = 0;
    node[240]->vx_param.has_relu = FALSE;
    node[240]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[240]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[240]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Block8/add_6_reshape_10
      var       - node[241]
      name      - InceptionResnetV1/Block8/add_6_reshape
      operation - reshape
      in_shape  - [[64, 252, 1, 1]]
      out_shape - [[3, 3, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[241], VSI_NN_OP_RESHAPE, 1, 1, 10);
    node[241]->nn_param.reshape.size = shape_42;
    node[241]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470
      var       - node[242]
      name      - fullconnectrelu
      operation - fullconnectrelu
      in_shape  - [[3, 3, 1792, 1]]
      out_shape - [[1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[242], VSI_NN_OP_FCL_RELU, 2, 1, 470);
    node[242]->nn_param.fcl.weights = 1792;
    node[242]->vx_param.has_relu = FALSE;
    node[242]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[242]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[242]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470_reshape_253
      var       - node[243]
      name      - InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470_reshape
      operation - reshape
      in_shape  - [[1792, 1]]
      out_shape - [[1, 1, 1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[243], VSI_NN_OP_RESHAPE, 1, 1, 253);
    node[243]->nn_param.reshape.size = shape_43;
    node[243]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Logits/Flatten/Reshape/shape_4_insert_permute_in_0_256
      var       - node[244]
      name      - InceptionResnetV1/Logits/Flatten/Reshape/shape_4_insert_permute_in_0
      operation - permute
      in_shape  - [[1, 1, 1792, 1]]
      out_shape - [[1792, 1, 1, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[244], VSI_NN_OP_PERMUTE, 1, 1, 256);
    node[244]->nn_param.permute.perm = perm_1;
    node[244]->nn_param.permute.dim_num = 4;

    /*-----------------------------------------
      lid       - InceptionResnetV1/Logits/Flatten/Reshape/shape_4
      var       - node[245]
      name      - InceptionResnetV1/Logits/Flatten/Reshape/shape
      operation - reshape
      in_shape  - [[1792, 1, 1, 1]]
      out_shape - [[1792, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[245], VSI_NN_OP_RESHAPE, 1, 1, 4);
    node[245]->nn_param.reshape.size = shape_44;
    node[245]->nn_param.reshape.dim_num = 2;

    /*-----------------------------------------
      lid       - trans_InceptionResnetV1/Bottleneck/MatMul_3
      var       - node[246]
      name      - fullconnectrelu
      operation - fullconnectrelu
      in_shape  - [[1792, 1]]
      out_shape - [[128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[246], VSI_NN_OP_FCL_RELU, 2, 1, 3);
    node[246]->nn_param.fcl.weights = 128;
    node[246]->vx_param.has_relu = FALSE;
    node[246]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[246]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[246]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @attach_input/in0_1:out0 */
    attr.size[0] = 160;
    attr.size[1] = 160;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_INT8);

    /* @attach_InceptionResnetV1/Bottleneck/BatchNorm/batchnorm/add_1/out0_0:out0 */
    attr.size[0] = 128;
    attr.size[1] = 1;
    attr.dim_num = 2;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_INT8);



    /* @InceptionResnetV1/Conv2d_1a_3x3/convolution_468_InceptionResnetV1/Conv2d_1a_3x3/Relu_466:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 2320;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_VDATA, 951424, 2320);

    /* @InceptionResnetV1/Conv2d_2a_3x3/convolution_465_InceptionResnetV1/Conv2d_2a_3x3/Relu_463:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_VDATA, 953744, 10512);

    /* @InceptionResnetV1/Conv2d_2b_3x3/convolution_462_InceptionResnetV1/Conv2d_2b_3x3/Relu_459:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 20240;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_VDATA, 964256, 20240);

    /* @InceptionResnetV1/Conv2d_3b_1x1/convolution_454_InceptionResnetV1/Conv2d_3b_1x1/Relu_444:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 6928;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_VDATA, 984496, 6928);

    /* @InceptionResnetV1/Conv2d_4a_3x3/convolution_438_InceptionResnetV1/Conv2d_4a_3x3/Relu_420:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 141072;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_VDATA, 991424, 141072);

    /* @InceptionResnetV1/Conv2d_4b_3x3/convolution_409_InceptionResnetV1/Conv2d_4b_3x3/Relu_386:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 521232;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_VDATA, 1132496, 521232);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/convolution_436_InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/Relu_418:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_VDATA, 7329936, 9488);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/convolution_452_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/Relu_442:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_VDATA, 7339424, 9488);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/convolution_460_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/Relu_456:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_VDATA, 7359424, 9488);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/convolution_435_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/Relu_417:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_VDATA, 7348912, 10512);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/convolution_453_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/Relu_443:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_VDATA, 7368912, 10512);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/convolution_437_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/Relu_419:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_VDATA, 7379424, 10512);

    /* @trans_InceptionResnetV1/Repeat/block35_1/Conv2d_1x1/convolution_396:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 27920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_VDATA, 23368928, 27920);

    /* @InceptionResnetV1/Repeat/block35_1/add_375_conv_250_InceptionResnetV1/Repeat/block35_1/Relu_365:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_VDATA, 7389936, 1296);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/convolution_414_InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/Relu_393:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_VDATA, 7391232, 9488);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/convolution_440_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/Relu_424:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_VDATA, 7400720, 9488);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/convolution_455_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/Relu_446:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_VDATA, 7420720, 9488);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/convolution_415_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/Relu_394:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_VDATA, 7410208, 10512);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/convolution_441_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/Relu_425:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_VDATA, 7430208, 10512);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/convolution_416_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/Relu_395:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[19], attr, VSI_NN_TYPE_VDATA, 7440720, 10512);

    /* @trans_InceptionResnetV1/Repeat/block35_2/Conv2d_1x1/convolution_373:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 27920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[20], attr, VSI_NN_TYPE_VDATA, 23396848, 27920);

    /* @InceptionResnetV1/Repeat/block35_2/add_355_conv_233_InceptionResnetV1/Repeat/block35_2/Relu_344:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[21], attr, VSI_NN_TYPE_VDATA, 7451232, 1296);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/convolution_401_InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/Relu_378:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[22], attr, VSI_NN_TYPE_VDATA, 7452528, 9488);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/convolution_431_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/Relu_411:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[23], attr, VSI_NN_TYPE_VDATA, 7462016, 9488);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/convolution_450_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/Relu_439:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[24], attr, VSI_NN_TYPE_VDATA, 7482016, 9488);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/convolution_400_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/Relu_377:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[25], attr, VSI_NN_TYPE_VDATA, 7471504, 10512);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/convolution_430_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/Relu_410:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[26], attr, VSI_NN_TYPE_VDATA, 7491504, 10512);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/convolution_399_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/Relu_376:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[27], attr, VSI_NN_TYPE_VDATA, 7502016, 10512);

    /* @trans_InceptionResnetV1/Repeat/block35_3/Conv2d_1x1/convolution_356:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 27920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[28], attr, VSI_NN_TYPE_VDATA, 23424768, 27920);

    /* @InceptionResnetV1/Repeat/block35_3/add_336_conv_224_InceptionResnetV1/Repeat/block35_3/Relu_326:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[29], attr, VSI_NN_TYPE_VDATA, 7512528, 1296);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/convolution_371_InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/Relu_353:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[30], attr, VSI_NN_TYPE_VDATA, 7513824, 9488);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/convolution_403_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/Relu_382:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[31], attr, VSI_NN_TYPE_VDATA, 7523312, 9488);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/convolution_432_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/Relu_413:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[32], attr, VSI_NN_TYPE_VDATA, 7543312, 9488);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/convolution_370_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/Relu_352:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[33], attr, VSI_NN_TYPE_VDATA, 7532800, 10512);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/convolution_404_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/Relu_383:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[34], attr, VSI_NN_TYPE_VDATA, 7552800, 10512);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/convolution_372_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/Relu_354:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[35], attr, VSI_NN_TYPE_VDATA, 7563312, 10512);

    /* @trans_InceptionResnetV1/Repeat/block35_4/Conv2d_1x1/convolution_334:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 27920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[36], attr, VSI_NN_TYPE_VDATA, 23452688, 27920);

    /* @InceptionResnetV1/Repeat/block35_4/add_317_conv_216_InceptionResnetV1/Repeat/block35_4/Relu_308:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[37], attr, VSI_NN_TYPE_VDATA, 7573824, 1296);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/convolution_350_InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/Relu_332:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[38], attr, VSI_NN_TYPE_VDATA, 7575120, 9488);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/convolution_380_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/Relu_359:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[39], attr, VSI_NN_TYPE_VDATA, 7584608, 9488);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/convolution_412_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/Relu_390:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9488;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[40], attr, VSI_NN_TYPE_VDATA, 7604608, 9488);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/convolution_349_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/Relu_331:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[41], attr, VSI_NN_TYPE_VDATA, 7594096, 10512);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/convolution_381_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/Relu_360:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[42], attr, VSI_NN_TYPE_VDATA, 7614096, 10512);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/convolution_351_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/Relu_333:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10512;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[43], attr, VSI_NN_TYPE_VDATA, 7624608, 10512);

    /* @trans_InceptionResnetV1/Repeat/block35_5/Conv2d_1x1/convolution_316:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 27920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[44], attr, VSI_NN_TYPE_VDATA, 23480608, 27920);

    /* @InceptionResnetV1/Repeat/block35_5/add_300_conv_197_InceptionResnetV1/Repeat/block35_5/Relu_291:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[45], attr, VSI_NN_TYPE_VDATA, 7635120, 1296);

    /* @InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/convolution_299_InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/Relu_279:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1018832;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[46], attr, VSI_NN_TYPE_VDATA, 1653728, 1018832);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/convolution_348_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/Relu_330:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 51984;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[47], attr, VSI_NN_TYPE_VDATA, 2672560, 51984);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/convolution_323_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/Relu_306:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 334608;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[48], attr, VSI_NN_TYPE_VDATA, 2724544, 334608);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/convolution_298_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/Relu_278:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529872;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[49], attr, VSI_NN_TYPE_VDATA, 3059152, 529872);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/convolution_319_InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/Relu_302:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[50], attr, VSI_NN_TYPE_VDATA, 7636416, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/convolution_379_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/Relu_358:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[51], attr, VSI_NN_TYPE_VDATA, 7753424, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/convolution_346_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/Relu_327:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 206608;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[52], attr, VSI_NN_TYPE_VDATA, 7870432, 206608);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/convolution_318_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/Relu_301:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[53], attr, VSI_NN_TYPE_VDATA, 8077040, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_1/Conv2d_1x1/convolution_281:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[54], attr, VSI_NN_TYPE_VDATA, 23508528, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_1/add_261_conv_192_InceptionResnetV1/Repeat_1/block17_1/Relu_251:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[55], attr, VSI_NN_TYPE_VDATA, 8194048, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/convolution_296_InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/Relu_276:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[56], attr, VSI_NN_TYPE_VDATA, 8751584, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/convolution_347_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/Relu_329:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[57], attr, VSI_NN_TYPE_VDATA, 8868592, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/convolution_322_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/Relu_305:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 206800;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[58], attr, VSI_NN_TYPE_VDATA, 8985600, 206800);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/convolution_297_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/Relu_277:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[59], attr, VSI_NN_TYPE_VDATA, 9192400, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_2/Conv2d_1x1/convolution_260:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[60], attr, VSI_NN_TYPE_VDATA, 23984208, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_2/add_243_conv_178_InceptionResnetV1/Repeat_1/block17_2/Relu_234:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[61], attr, VSI_NN_TYPE_VDATA, 9309408, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/convolution_275_InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/Relu_258:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[62], attr, VSI_NN_TYPE_VDATA, 9310704, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/convolution_328_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/Relu_312:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[63], attr, VSI_NN_TYPE_VDATA, 9427712, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/convolution_304_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/Relu_286:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 207568;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[64], attr, VSI_NN_TYPE_VDATA, 9544720, 207568);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/convolution_274_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/Relu_257:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[65], attr, VSI_NN_TYPE_VDATA, 9752288, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_3/Conv2d_1x1/convolution_241:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[66], attr, VSI_NN_TYPE_VDATA, 24222048, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_3/add_223_conv_166_InceptionResnetV1/Repeat_1/block17_3/Relu_214:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[67], attr, VSI_NN_TYPE_VDATA, 9869296, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/convolution_263_InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/Relu_245:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[68], attr, VSI_NN_TYPE_VDATA, 9870592, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/convolution_320_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/Relu_303:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[69], attr, VSI_NN_TYPE_VDATA, 9987600, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/convolution_293_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/Relu_271:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 207632;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[70], attr, VSI_NN_TYPE_VDATA, 10104608, 207632);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/convolution_262_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/Relu_244:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[71], attr, VSI_NN_TYPE_VDATA, 10312240, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_4/Conv2d_1x1/convolution_225:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[72], attr, VSI_NN_TYPE_VDATA, 24459888, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_4/add_207_conv_150_InceptionResnetV1/Repeat_1/block17_4/Relu_198:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[73], attr, VSI_NN_TYPE_VDATA, 10429248, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/convolution_239_InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/Relu_221:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[74], attr, VSI_NN_TYPE_VDATA, 10430544, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/convolution_294_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/Relu_273:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[75], attr, VSI_NN_TYPE_VDATA, 10547552, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/convolution_265_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/Relu_248:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 207440;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[76], attr, VSI_NN_TYPE_VDATA, 10664560, 207440);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/convolution_240_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/Relu_222:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[77], attr, VSI_NN_TYPE_VDATA, 10872000, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_5/Conv2d_1x1/convolution_205:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[78], attr, VSI_NN_TYPE_VDATA, 24697728, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_5/add_189_conv_144_InceptionResnetV1/Repeat_1/block17_5/Relu_181:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[79], attr, VSI_NN_TYPE_VDATA, 10989008, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/convolution_227_InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/Relu_209:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[80], attr, VSI_NN_TYPE_VDATA, 10990304, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/convolution_284_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/Relu_264:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[81], attr, VSI_NN_TYPE_VDATA, 11107312, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/convolution_254_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/Relu_236:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 208272;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[82], attr, VSI_NN_TYPE_VDATA, 11224320, 208272);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/convolution_226_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/Relu_208:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[83], attr, VSI_NN_TYPE_VDATA, 11432592, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_6/Conv2d_1x1/convolution_191:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[84], attr, VSI_NN_TYPE_VDATA, 24935568, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_6/add_174_conv_125_InceptionResnetV1/Repeat_1/block17_6/Relu_165:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[85], attr, VSI_NN_TYPE_VDATA, 11549600, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/convolution_204_InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/Relu_188:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[86], attr, VSI_NN_TYPE_VDATA, 11550896, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/convolution_255_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/Relu_238:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[87], attr, VSI_NN_TYPE_VDATA, 11667904, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/convolution_230_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/Relu_212:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 208336;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[88], attr, VSI_NN_TYPE_VDATA, 11784912, 208336);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/convolution_203_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/Relu_187:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[89], attr, VSI_NN_TYPE_VDATA, 11993248, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_7/Conv2d_1x1/convolution_172:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[90], attr, VSI_NN_TYPE_VDATA, 25173408, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_7/add_157_conv_118_InceptionResnetV1/Repeat_1/block17_7/Relu_148:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[91], attr, VSI_NN_TYPE_VDATA, 12110256, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/convolution_185_InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/Relu_170:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[92], attr, VSI_NN_TYPE_VDATA, 12111552, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/convolution_237_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/Relu_219:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[93], attr, VSI_NN_TYPE_VDATA, 12228560, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/convolution_211_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/Relu_194:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 209040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[94], attr, VSI_NN_TYPE_VDATA, 12345568, 209040);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/convolution_186_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/Relu_171:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[95], attr, VSI_NN_TYPE_VDATA, 12554608, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_8/Conv2d_1x1/convolution_155:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[96], attr, VSI_NN_TYPE_VDATA, 25411248, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_8/add_139_conv_106_InceptionResnetV1/Repeat_1/block17_8/Relu_129:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[97], attr, VSI_NN_TYPE_VDATA, 12671616, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/convolution_176_InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/Relu_159:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[98], attr, VSI_NN_TYPE_VDATA, 12672912, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/convolution_228_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/Relu_210:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[99], attr, VSI_NN_TYPE_VDATA, 12789920, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/convolution_200_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/Relu_183:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 207248;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[100], attr, VSI_NN_TYPE_VDATA, 12906928, 207248);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/convolution_175_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/Relu_158:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[101], attr, VSI_NN_TYPE_VDATA, 13114176, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_9/Conv2d_1x1/convolution_140:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 234320;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[102], attr, VSI_NN_TYPE_VDATA, 25649088, 234320);

    /* @InceptionResnetV1/Repeat_1/block17_9/add_120_conv_90_InceptionResnetV1/Repeat_1/block17_9/Relu_109:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[103], attr, VSI_NN_TYPE_VDATA, 13231184, 1296);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/convolution_154_InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/Relu_138:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[104], attr, VSI_NN_TYPE_VDATA, 8195344, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/convolution_201_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/Relu_184:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[105], attr, VSI_NN_TYPE_VDATA, 8312352, 117008);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/convolution_177_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/Relu_162:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 203920;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[106], attr, VSI_NN_TYPE_VDATA, 8429360, 203920);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/convolution_153_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/Relu_137:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 117008;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[107], attr, VSI_NN_TYPE_VDATA, 8633280, 117008);

    /* @trans_InceptionResnetV1/Repeat_1/block17_10/Conv2d_1x1/convolution_119:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 237840;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[108], attr, VSI_NN_TYPE_VDATA, 23746368, 237840);

    /* @InceptionResnetV1/Repeat_1/block17_10/add_98_conv_83_InceptionResnetV1/Repeat_1/block17_10/Relu_87:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[109], attr, VSI_NN_TYPE_VDATA, 8750288, 1296);

    /* @InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/convolution_131_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/Relu_110:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 232720;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[110], attr, VSI_NN_TYPE_VDATA, 3589024, 232720);

    /* @InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/convolution_132_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/Relu_111:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 232720;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[111], attr, VSI_NN_TYPE_VDATA, 4851712, 232720);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/convolution_152_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/Relu_136:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 232720;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[112], attr, VSI_NN_TYPE_VDATA, 5793504, 232720);

    /* @InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/convolution_99_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/Relu_77:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1029968;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[113], attr, VSI_NN_TYPE_VDATA, 3821744, 1029968);

    /* @InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/convolution_100_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/Relu_78:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 709072;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[114], attr, VSI_NN_TYPE_VDATA, 5084432, 709072);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/convolution_127_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/Relu_107:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 593168;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[115], attr, VSI_NN_TYPE_VDATA, 6026224, 593168);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/convolution_97_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/Relu_75:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 710544;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[116], attr, VSI_NN_TYPE_VDATA, 6619392, 710544);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/convolution_116_InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/Relu_96:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[117], attr, VSI_NN_TYPE_VDATA, 13232480, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/convolution_168_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/Relu_151:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[118], attr, VSI_NN_TYPE_VDATA, 13579376, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/convolution_143_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/Relu_126:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 150608;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[119], attr, VSI_NN_TYPE_VDATA, 13926272, 150608);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/convolution_115_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/Relu_95:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[120], attr, VSI_NN_TYPE_VDATA, 14076880, 113424);

    /* @trans_InceptionResnetV1/Repeat_2/block8_1/Conv2d_1x1/convolution_73:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 670608;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[121], attr, VSI_NN_TYPE_VDATA, 25883408, 670608);

    /* @InceptionResnetV1/Repeat_2/block8_1/add_56_conv_65_InceptionResnetV1/Repeat_2/block8_1/Relu_48:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[122], attr, VSI_NN_TYPE_VDATA, 14190304, 1296);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/convolution_101_InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/Relu_79:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[123], attr, VSI_NN_TYPE_VDATA, 14191600, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/convolution_160_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/Relu_142:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[124], attr, VSI_NN_TYPE_VDATA, 14538496, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/convolution_133_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/Relu_112:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 150736;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[125], attr, VSI_NN_TYPE_VDATA, 14885392, 150736);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/convolution_102_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/Relu_80:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[126], attr, VSI_NN_TYPE_VDATA, 15036128, 113424);

    /* @trans_InceptionResnetV1/Repeat_2/block8_2/Conv2d_1x1/convolution_57:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 671504;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[127], attr, VSI_NN_TYPE_VDATA, 26554016, 671504);

    /* @InceptionResnetV1/Repeat_2/block8_2/add_42_conv_53_InceptionResnetV1/Repeat_2/block8_2/Relu_35:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[128], attr, VSI_NN_TYPE_VDATA, 15149552, 1296);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/convolution_71_InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/Relu_54:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[129], attr, VSI_NN_TYPE_VDATA, 15150848, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/convolution_134_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/Relu_114:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[130], attr, VSI_NN_TYPE_VDATA, 15497744, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/convolution_104_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/Relu_84:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 147408;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[131], attr, VSI_NN_TYPE_VDATA, 15844640, 147408);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/convolution_72_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/Relu_55:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[132], attr, VSI_NN_TYPE_VDATA, 15992048, 113424);

    /* @trans_InceptionResnetV1/Repeat_2/block8_3/Conv2d_1x1/convolution_41:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 703760;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[133], attr, VSI_NN_TYPE_VDATA, 27225520, 703760);

    /* @InceptionResnetV1/Repeat_2/block8_3/add_27_conv_45_InceptionResnetV1/Repeat_2/block8_3/Relu_21:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[134], attr, VSI_NN_TYPE_VDATA, 16105472, 1296);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/convolution_60_InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/Relu_44:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[135], attr, VSI_NN_TYPE_VDATA, 16106768, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/convolution_124_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/Relu_103:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[136], attr, VSI_NN_TYPE_VDATA, 16453664, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/convolution_92_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/Relu_68:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 150864;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[137], attr, VSI_NN_TYPE_VDATA, 16800560, 150864);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/convolution_59_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/Relu_43:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[138], attr, VSI_NN_TYPE_VDATA, 16951424, 113424);

    /* @trans_InceptionResnetV1/Repeat_2/block8_4/Conv2d_1x1/convolution_28:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 680592;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[139], attr, VSI_NN_TYPE_VDATA, 27929280, 680592);

    /* @InceptionResnetV1/Repeat_2/block8_4/add_17_conv_32_InceptionResnetV1/Repeat_2/block8_4/Relu_13:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[140], attr, VSI_NN_TYPE_VDATA, 17064848, 1296);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/convolution_39_InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/Relu_26:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[141], attr, VSI_NN_TYPE_VDATA, 17066144, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/convolution_93_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/Relu_70:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[142], attr, VSI_NN_TYPE_VDATA, 17413040, 346896);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/convolution_62_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/Relu_46:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 146896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[143], attr, VSI_NN_TYPE_VDATA, 17759936, 146896);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/convolution_38_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/Relu_25:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[144], attr, VSI_NN_TYPE_VDATA, 17906832, 113424);

    /* @trans_InceptionResnetV1/Repeat_2/block8_5/Conv2d_1x1/convolution_16:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 703760;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[145], attr, VSI_NN_TYPE_VDATA, 28609872, 703760);

    /* @InceptionResnetV1/Repeat_2/block8_5/add_9_conv_22_InceptionResnetV1/Repeat_2/block8_5/Relu_7:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[146], attr, VSI_NN_TYPE_VDATA, 18020256, 1296);

    /* @InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/convolution_30_InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/Relu_18:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[147], attr, VSI_NN_TYPE_VDATA, 0, 346896);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/convolution_82_InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/Relu_61:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 346896;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[148], attr, VSI_NN_TYPE_VDATA, 346896, 346896);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/convolution_52_InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/Relu_37:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 144208;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[149], attr, VSI_NN_TYPE_VDATA, 693792, 144208);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/convolution_31_InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/Relu_19:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 113424;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[150], attr, VSI_NN_TYPE_VDATA, 838000, 113424);

    /* @trans_InceptionResnetV1/Block8/Conv2d_1x1/convolution_11:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 703760;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[151], attr, VSI_NN_TYPE_VDATA, 18021552, 703760);

    /* @trans_InceptionResnetV1/Block8/add_6_conv_8:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1296;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[152], attr, VSI_NN_TYPE_VDATA, 18725312, 1296);

    /* @trans_InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4177416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[153], attr, VSI_NN_TYPE_VDATA, 19191512, 4177416);

    /* @trans_InceptionResnetV1/Bottleneck/MatMul_3:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 464904;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[154], attr, VSI_NN_TYPE_VDATA, 18726608, 464904);



    /* @InceptionResnetV1/Conv2d_1a_3x3/convolution_468_InceptionResnetV1/Conv2d_1a_3x3/Relu_466:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Conv2d_2a_3x3/convolution_465_InceptionResnetV1/Conv2d_2a_3x3/Relu_463:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Conv2d_2b_3x3/convolution_462_InceptionResnetV1/Conv2d_2b_3x3/Relu_459:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/MaxPool_3a_3x3/MaxPool_457:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Conv2d_3b_1x1/convolution_454_InceptionResnetV1/Conv2d_3b_1x1/Relu_444:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Conv2d_4a_3x3/convolution_438_InceptionResnetV1/Conv2d_4a_3x3/Relu_420:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Conv2d_4b_3x3/convolution_409_InceptionResnetV1/Conv2d_4b_3x3/Relu_386:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/convolution_436_InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/Relu_418:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/convolution_452_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/Relu_442:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/convolution_460_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/Relu_456:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/convolution_435_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/Relu_417:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/convolution_453_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/Relu_443:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/convolution_437_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/Relu_419:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/concat_408:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat/block35_1/Conv2d_1x1/convolution_396:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/add_375_concat_246:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/add_375_reshape_247:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/add_375_conv_250_InceptionResnetV1/Repeat/block35_1/Relu_365:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_1/add_375_reshape_252:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/convolution_414_InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/Relu_393:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/convolution_440_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/Relu_424:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/convolution_455_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/Relu_446:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[21]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/convolution_415_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/Relu_394:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[22]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/convolution_441_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/Relu_425:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[23]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/convolution_416_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/Relu_395:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[24]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/concat_384:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[25]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat/block35_2/Conv2d_1x1/convolution_373:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[26]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/add_355_concat_231:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[27]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/add_355_reshape_232:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[28]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/add_355_conv_233_InceptionResnetV1/Repeat/block35_2/Relu_344:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[29]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_2/add_355_reshape_242:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[30]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/convolution_401_InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/Relu_378:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[31]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/convolution_431_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/Relu_411:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[32]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/convolution_450_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/Relu_439:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[33]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/convolution_400_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/Relu_377:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[34]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/convolution_430_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/Relu_410:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[35]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/convolution_399_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/Relu_376:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[36]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/concat_366:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[37]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat/block35_3/Conv2d_1x1/convolution_356:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[38]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/add_336_concat_218:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[39]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/add_336_reshape_220:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[40]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/add_336_conv_224_InceptionResnetV1/Repeat/block35_3/Relu_326:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[41]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_3/add_336_reshape_229:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[42]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/convolution_371_InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/Relu_353:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[43]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/convolution_403_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/Relu_382:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[44]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/convolution_432_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/Relu_413:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[45]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/convolution_370_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/Relu_352:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[46]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/convolution_404_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/Relu_383:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[47]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/convolution_372_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/Relu_354:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[48]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/concat_343:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[49]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat/block35_4/Conv2d_1x1/convolution_334:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[50]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/add_317_concat_206:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[51]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/add_317_reshape_215:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[52]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/add_317_conv_216_InceptionResnetV1/Repeat/block35_4/Relu_308:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[53]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_4/add_317_reshape_217:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[54]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/convolution_350_InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/Relu_332:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[55]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/convolution_380_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/Relu_359:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[56]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/convolution_412_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/Relu_390:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[57]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/convolution_349_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/Relu_331:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[58]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/convolution_381_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/Relu_360:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[59]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/convolution_351_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/Relu_333:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[60]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/concat_324:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[61]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat/block35_5/Conv2d_1x1/convolution_316:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[62]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/add_300_concat_195:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[63]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/add_300_reshape_196:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[64]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/add_300_conv_197_InceptionResnetV1/Repeat/block35_5/Relu_291:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[65]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat/block35_5/add_300_reshape_202:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[66]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool_280:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[67]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/convolution_299_InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/Relu_279:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[68]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/convolution_348_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/Relu_330:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[69]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/convolution_323_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/Relu_306:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[70]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/convolution_298_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/Relu_278:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[71]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_6a/concat_269:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[72]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/convolution_319_InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/Relu_302:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[73]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/convolution_379_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/Relu_358:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[74]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/convolution_346_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/Relu_327:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[75]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/convolution_318_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/Relu_301:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[76]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/concat_292:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[77]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_1/Conv2d_1x1/convolution_281:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[78]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/add_261_concat_182:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[79]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_190:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[80]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/add_261_conv_192_InceptionResnetV1/Repeat_1/block17_1/Relu_251:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[81]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_193:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[82]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/convolution_296_InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/Relu_276:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[83]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/convolution_347_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/Relu_329:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[84]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/convolution_322_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/Relu_305:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[85]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/convolution_297_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/Relu_277:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[86]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/concat_268:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[87]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_2/Conv2d_1x1/convolution_260:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[88]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/add_243_concat_169:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[89]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_173:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[90]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/add_243_conv_178_InceptionResnetV1/Repeat_1/block17_2/Relu_234:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[91]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_179:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[92]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/convolution_275_InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/Relu_258:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[93]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/convolution_328_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/Relu_312:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[94]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/convolution_304_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/Relu_286:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[95]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/convolution_274_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/Relu_257:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[96]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/concat_249:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[97]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_3/Conv2d_1x1/convolution_241:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[98]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/add_223_concat_161:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[99]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_164:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[100]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/add_223_conv_166_InceptionResnetV1/Repeat_1/block17_3/Relu_214:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[101]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_167:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[102]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/convolution_263_InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/Relu_245:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[103]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/convolution_320_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/Relu_303:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[104]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/convolution_293_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/Relu_271:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[105]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/convolution_262_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/Relu_244:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[106]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/concat_235:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[107]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_4/Conv2d_1x1/convolution_225:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[108]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/add_207_concat_146:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[109]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_147:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[110]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/add_207_conv_150_InceptionResnetV1/Repeat_1/block17_4/Relu_198:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[111]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_156:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[112]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/convolution_239_InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/Relu_221:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[113]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/convolution_294_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/Relu_273:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[114]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/convolution_265_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/Relu_248:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[115]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/convolution_240_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/Relu_222:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[116]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/concat_213:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[117]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_5/Conv2d_1x1/convolution_205:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[118]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/add_189_concat_135:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[119]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_141:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[120]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/add_189_conv_144_InceptionResnetV1/Repeat_1/block17_5/Relu_181:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[121]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_145:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[122]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/convolution_227_InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/Relu_209:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[123]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/convolution_284_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/Relu_264:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[124]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/convolution_254_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/Relu_236:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[125]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/convolution_226_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/Relu_208:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[126]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/concat_199:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[127]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_6/Conv2d_1x1/convolution_191:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[128]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/add_174_concat_122:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[129]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_123:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[130]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/add_174_conv_125_InceptionResnetV1/Repeat_1/block17_6/Relu_165:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[131]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_130:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[132]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/convolution_204_InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/Relu_188:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[133]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/convolution_255_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/Relu_238:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[134]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/convolution_230_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/Relu_212:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[135]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/convolution_203_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/Relu_187:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[136]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/concat_180:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[137]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_7/Conv2d_1x1/convolution_172:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[138]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/add_157_concat_113:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[139]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_117:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[140]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/add_157_conv_118_InceptionResnetV1/Repeat_1/block17_7/Relu_148:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[141]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_121:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[142]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/convolution_185_InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/Relu_170:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[143]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/convolution_237_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/Relu_219:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[144]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/convolution_211_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/Relu_194:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[145]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/convolution_186_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/Relu_171:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[146]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/concat_163:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[147]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_8/Conv2d_1x1/convolution_155:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[148]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/add_139_concat_94:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[149]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_105:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[150]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/add_139_conv_106_InceptionResnetV1/Repeat_1/block17_8/Relu_129:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[151]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_108:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[152]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/convolution_176_InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/Relu_159:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[153]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/convolution_228_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/Relu_210:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[154]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/convolution_200_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/Relu_183:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[155]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/convolution_175_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/Relu_158:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[156]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/concat_149:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[157]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_9/Conv2d_1x1/convolution_140:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[158]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/add_120_concat_88:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[159]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_89:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[160]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/add_120_conv_90_InceptionResnetV1/Repeat_1/block17_9/Relu_109:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[161]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_91:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[162]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/convolution_154_InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/Relu_138:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[163]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/convolution_201_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/Relu_184:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[164]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/convolution_177_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/Relu_162:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[165]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/convolution_153_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/Relu_137:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[166]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/concat_128:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[167]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_1/block17_10/Conv2d_1x1/convolution_119:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[168]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/add_98_concat_74:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[169]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_81:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[170]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/add_98_conv_83_InceptionResnetV1/Repeat_1/block17_10/Relu_87:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[171]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_86:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[172]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_3/MaxPool_1a_3x3/MaxPool_76:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[173]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/convolution_131_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/Relu_110:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[174]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/convolution_132_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/Relu_111:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[175]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/convolution_152_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/Relu_136:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[176]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/convolution_99_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/Relu_77:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[177]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/convolution_100_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/Relu_78:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[178]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/convolution_127_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/Relu_107:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[179]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/convolution_97_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/Relu_75:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[180]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Mixed_7a/concat/axis_66:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[181]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/convolution_116_InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/Relu_96:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[182]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/convolution_168_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/Relu_151:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[183]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/convolution_143_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/Relu_126:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[184]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/convolution_115_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/Relu_95:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[185]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/concat_85:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[186]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_2/block8_1/Conv2d_1x1/convolution_73:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[187]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/add_56_concat_63:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[188]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_64:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[189]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/add_56_conv_65_InceptionResnetV1/Repeat_2/block8_1/Relu_48:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[190]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_69:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[191]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/convolution_101_InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/Relu_79:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[192]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/convolution_160_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/Relu_142:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[193]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/convolution_133_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/Relu_112:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[194]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/convolution_102_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/Relu_80:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[195]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/concat_67:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[196]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_2/block8_2/Conv2d_1x1/convolution_57:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[197]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/add_42_concat_50:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[198]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_51:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[199]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/add_42_conv_53_InceptionResnetV1/Repeat_2/block8_2/Relu_35:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[200]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_58:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[201]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/convolution_71_InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/Relu_54:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[202]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/convolution_134_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/Relu_114:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[203]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/convolution_104_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/Relu_84:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[204]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/convolution_72_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/Relu_55:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[205]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/concat_47:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[206]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_2/block8_3/Conv2d_1x1/convolution_41:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[207]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/add_27_concat_34:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[208]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_40:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[209]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/add_27_conv_45_InceptionResnetV1/Repeat_2/block8_3/Relu_21:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[210]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_49:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[211]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/convolution_60_InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/Relu_44:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[212]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/convolution_124_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/Relu_103:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[213]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/convolution_92_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/Relu_68:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[214]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/convolution_59_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/Relu_43:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[215]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/concat_36:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[216]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_2/block8_4/Conv2d_1x1/convolution_28:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[217]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/add_17_concat_24:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[218]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_29:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[219]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/add_17_conv_32_InceptionResnetV1/Repeat_2/block8_4/Relu_13:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[220]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_33:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[221]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/convolution_39_InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/Relu_26:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[222]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/convolution_93_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/Relu_70:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[223]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/convolution_62_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/Relu_46:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[224]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/convolution_38_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/Relu_25:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[225]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/concat_20:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[226]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Repeat_2/block8_5/Conv2d_1x1/convolution_16:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[227]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/add_9_concat_12:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[228]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_15:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[229]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/add_9_conv_22_InceptionResnetV1/Repeat_2/block8_5/Relu_7:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[230]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_23:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[231]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/convolution_30_InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/Relu_18:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[232]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/convolution_82_InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/Relu_61:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[233]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/convolution_52_InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/Relu_37:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[234]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/convolution_31_InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/Relu_19:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[235]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/concat_14:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[236]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Block8/Conv2d_1x1/convolution_11:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[237]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/add_6_concat_2:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[238]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/add_6_reshape_5:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[239]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Block8/add_6_conv_8:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[240]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Block8/add_6_reshape_10:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[241]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470:out0 */
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[242]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470_reshape_253:out0 */
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[243]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Logits/Flatten/Reshape/shape_4_insert_permute_in_0_256:out0 */
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[244]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @InceptionResnetV1/Logits/Flatten/Reshape/shape_4:out0 */
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[245]->output.tensors[0], attr, VSI_NN_TYPE_INT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[0]->input.tensors[0] = norm_tensor[0];
    node[246]->output.tensors[0] = norm_tensor[1];

    /* InceptionResnetV1/Conv2d_1a_3x3/convolution_468_InceptionResnetV1/Conv2d_1a_3x3/Relu_466 */
    node[0]->input.tensors[1] = const_tensor[0]; /* data_vdata */

    /* InceptionResnetV1/Conv2d_2a_3x3/convolution_465_InceptionResnetV1/Conv2d_2a_3x3/Relu_463 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];
    node[1]->input.tensors[1] = const_tensor[1]; /* data_vdata */

    /* InceptionResnetV1/Conv2d_2b_3x3/convolution_462_InceptionResnetV1/Conv2d_2b_3x3/Relu_459 */
    node[2]->input.tensors[0] = node[1]->output.tensors[0];
    node[2]->input.tensors[1] = const_tensor[2]; /* data_vdata */

    /* InceptionResnetV1/MaxPool_3a_3x3/MaxPool_457 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];

    /* InceptionResnetV1/Conv2d_3b_1x1/convolution_454_InceptionResnetV1/Conv2d_3b_1x1/Relu_444 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[3]; /* data_vdata */

    /* InceptionResnetV1/Conv2d_4a_3x3/convolution_438_InceptionResnetV1/Conv2d_4a_3x3/Relu_420 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];
    node[5]->input.tensors[1] = const_tensor[4]; /* data_vdata */

    /* InceptionResnetV1/Conv2d_4b_3x3/convolution_409_InceptionResnetV1/Conv2d_4b_3x3/Relu_386 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];
    node[6]->input.tensors[1] = const_tensor[5]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/convolution_436_InceptionResnetV1/Repeat/block35_1/Branch_0/Conv2d_1x1/Relu_418 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];
    node[7]->input.tensors[1] = const_tensor[6]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/convolution_452_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/Relu_442 */
    node[8]->input.tensors[0] = node[6]->output.tensors[0];
    node[8]->input.tensors[1] = const_tensor[7]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/convolution_460_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/Relu_456 */
    node[9]->input.tensors[0] = node[6]->output.tensors[0];
    node[9]->input.tensors[1] = const_tensor[8]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/convolution_435_InceptionResnetV1/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/Relu_417 */
    node[10]->input.tensors[0] = node[8]->output.tensors[0];
    node[10]->input.tensors[1] = const_tensor[9]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/convolution_453_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/Relu_443 */
    node[11]->input.tensors[0] = node[9]->output.tensors[0];
    node[11]->input.tensors[1] = const_tensor[10]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/convolution_437_InceptionResnetV1/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/Relu_419 */
    node[12]->input.tensors[0] = node[11]->output.tensors[0];
    node[12]->input.tensors[1] = const_tensor[11]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/concat_408 */
    node[13]->input.tensors[0] = node[7]->output.tensors[0];
    node[13]->input.tensors[1] = node[10]->output.tensors[0];
    node[13]->input.tensors[2] = node[12]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat/block35_1/Conv2d_1x1/convolution_396 */
    node[14]->input.tensors[0] = node[13]->output.tensors[0];
    node[14]->input.tensors[1] = const_tensor[12]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/add_375_concat_246 */
    node[15]->input.tensors[0] = node[6]->output.tensors[0];
    node[15]->input.tensors[1] = node[14]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_1/add_375_reshape_247 */
    node[16]->input.tensors[0] = node[15]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_1/add_375_conv_250_InceptionResnetV1/Repeat/block35_1/Relu_365 */
    node[17]->input.tensors[0] = node[16]->output.tensors[0];
    node[17]->input.tensors[1] = const_tensor[13]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_1/add_375_reshape_252 */
    node[18]->input.tensors[0] = node[17]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/convolution_414_InceptionResnetV1/Repeat/block35_2/Branch_0/Conv2d_1x1/Relu_393 */
    node[19]->input.tensors[0] = node[18]->output.tensors[0];
    node[19]->input.tensors[1] = const_tensor[14]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/convolution_440_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/Relu_424 */
    node[20]->input.tensors[0] = node[18]->output.tensors[0];
    node[20]->input.tensors[1] = const_tensor[15]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/convolution_455_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/Relu_446 */
    node[21]->input.tensors[0] = node[18]->output.tensors[0];
    node[21]->input.tensors[1] = const_tensor[16]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/convolution_415_InceptionResnetV1/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/Relu_394 */
    node[22]->input.tensors[0] = node[20]->output.tensors[0];
    node[22]->input.tensors[1] = const_tensor[17]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/convolution_441_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/Relu_425 */
    node[23]->input.tensors[0] = node[21]->output.tensors[0];
    node[23]->input.tensors[1] = const_tensor[18]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/convolution_416_InceptionResnetV1/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/Relu_395 */
    node[24]->input.tensors[0] = node[23]->output.tensors[0];
    node[24]->input.tensors[1] = const_tensor[19]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/concat_384 */
    node[25]->input.tensors[0] = node[19]->output.tensors[0];
    node[25]->input.tensors[1] = node[22]->output.tensors[0];
    node[25]->input.tensors[2] = node[24]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat/block35_2/Conv2d_1x1/convolution_373 */
    node[26]->input.tensors[0] = node[25]->output.tensors[0];
    node[26]->input.tensors[1] = const_tensor[20]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/add_355_concat_231 */
    node[27]->input.tensors[0] = node[18]->output.tensors[0];
    node[27]->input.tensors[1] = node[26]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_2/add_355_reshape_232 */
    node[28]->input.tensors[0] = node[27]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_2/add_355_conv_233_InceptionResnetV1/Repeat/block35_2/Relu_344 */
    node[29]->input.tensors[0] = node[28]->output.tensors[0];
    node[29]->input.tensors[1] = const_tensor[21]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_2/add_355_reshape_242 */
    node[30]->input.tensors[0] = node[29]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/convolution_401_InceptionResnetV1/Repeat/block35_3/Branch_0/Conv2d_1x1/Relu_378 */
    node[31]->input.tensors[0] = node[30]->output.tensors[0];
    node[31]->input.tensors[1] = const_tensor[22]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/convolution_431_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/Relu_411 */
    node[32]->input.tensors[0] = node[30]->output.tensors[0];
    node[32]->input.tensors[1] = const_tensor[23]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/convolution_450_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/Relu_439 */
    node[33]->input.tensors[0] = node[30]->output.tensors[0];
    node[33]->input.tensors[1] = const_tensor[24]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/convolution_400_InceptionResnetV1/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/Relu_377 */
    node[34]->input.tensors[0] = node[32]->output.tensors[0];
    node[34]->input.tensors[1] = const_tensor[25]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/convolution_430_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/Relu_410 */
    node[35]->input.tensors[0] = node[33]->output.tensors[0];
    node[35]->input.tensors[1] = const_tensor[26]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/convolution_399_InceptionResnetV1/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/Relu_376 */
    node[36]->input.tensors[0] = node[35]->output.tensors[0];
    node[36]->input.tensors[1] = const_tensor[27]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/concat_366 */
    node[37]->input.tensors[0] = node[31]->output.tensors[0];
    node[37]->input.tensors[1] = node[34]->output.tensors[0];
    node[37]->input.tensors[2] = node[36]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat/block35_3/Conv2d_1x1/convolution_356 */
    node[38]->input.tensors[0] = node[37]->output.tensors[0];
    node[38]->input.tensors[1] = const_tensor[28]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/add_336_concat_218 */
    node[39]->input.tensors[0] = node[30]->output.tensors[0];
    node[39]->input.tensors[1] = node[38]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_3/add_336_reshape_220 */
    node[40]->input.tensors[0] = node[39]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_3/add_336_conv_224_InceptionResnetV1/Repeat/block35_3/Relu_326 */
    node[41]->input.tensors[0] = node[40]->output.tensors[0];
    node[41]->input.tensors[1] = const_tensor[29]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_3/add_336_reshape_229 */
    node[42]->input.tensors[0] = node[41]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/convolution_371_InceptionResnetV1/Repeat/block35_4/Branch_0/Conv2d_1x1/Relu_353 */
    node[43]->input.tensors[0] = node[42]->output.tensors[0];
    node[43]->input.tensors[1] = const_tensor[30]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/convolution_403_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/Relu_382 */
    node[44]->input.tensors[0] = node[42]->output.tensors[0];
    node[44]->input.tensors[1] = const_tensor[31]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/convolution_432_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/Relu_413 */
    node[45]->input.tensors[0] = node[42]->output.tensors[0];
    node[45]->input.tensors[1] = const_tensor[32]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/convolution_370_InceptionResnetV1/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/Relu_352 */
    node[46]->input.tensors[0] = node[44]->output.tensors[0];
    node[46]->input.tensors[1] = const_tensor[33]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/convolution_404_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/Relu_383 */
    node[47]->input.tensors[0] = node[45]->output.tensors[0];
    node[47]->input.tensors[1] = const_tensor[34]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/convolution_372_InceptionResnetV1/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/Relu_354 */
    node[48]->input.tensors[0] = node[47]->output.tensors[0];
    node[48]->input.tensors[1] = const_tensor[35]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/concat_343 */
    node[49]->input.tensors[0] = node[43]->output.tensors[0];
    node[49]->input.tensors[1] = node[46]->output.tensors[0];
    node[49]->input.tensors[2] = node[48]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat/block35_4/Conv2d_1x1/convolution_334 */
    node[50]->input.tensors[0] = node[49]->output.tensors[0];
    node[50]->input.tensors[1] = const_tensor[36]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/add_317_concat_206 */
    node[51]->input.tensors[0] = node[42]->output.tensors[0];
    node[51]->input.tensors[1] = node[50]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_4/add_317_reshape_215 */
    node[52]->input.tensors[0] = node[51]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_4/add_317_conv_216_InceptionResnetV1/Repeat/block35_4/Relu_308 */
    node[53]->input.tensors[0] = node[52]->output.tensors[0];
    node[53]->input.tensors[1] = const_tensor[37]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_4/add_317_reshape_217 */
    node[54]->input.tensors[0] = node[53]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/convolution_350_InceptionResnetV1/Repeat/block35_5/Branch_0/Conv2d_1x1/Relu_332 */
    node[55]->input.tensors[0] = node[54]->output.tensors[0];
    node[55]->input.tensors[1] = const_tensor[38]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/convolution_380_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/Relu_359 */
    node[56]->input.tensors[0] = node[54]->output.tensors[0];
    node[56]->input.tensors[1] = const_tensor[39]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/convolution_412_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/Relu_390 */
    node[57]->input.tensors[0] = node[54]->output.tensors[0];
    node[57]->input.tensors[1] = const_tensor[40]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/convolution_349_InceptionResnetV1/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/Relu_331 */
    node[58]->input.tensors[0] = node[56]->output.tensors[0];
    node[58]->input.tensors[1] = const_tensor[41]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/convolution_381_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/Relu_360 */
    node[59]->input.tensors[0] = node[57]->output.tensors[0];
    node[59]->input.tensors[1] = const_tensor[42]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/convolution_351_InceptionResnetV1/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/Relu_333 */
    node[60]->input.tensors[0] = node[59]->output.tensors[0];
    node[60]->input.tensors[1] = const_tensor[43]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/concat_324 */
    node[61]->input.tensors[0] = node[55]->output.tensors[0];
    node[61]->input.tensors[1] = node[58]->output.tensors[0];
    node[61]->input.tensors[2] = node[60]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat/block35_5/Conv2d_1x1/convolution_316 */
    node[62]->input.tensors[0] = node[61]->output.tensors[0];
    node[62]->input.tensors[1] = const_tensor[44]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/add_300_concat_195 */
    node[63]->input.tensors[0] = node[54]->output.tensors[0];
    node[63]->input.tensors[1] = node[62]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_5/add_300_reshape_196 */
    node[64]->input.tensors[0] = node[63]->output.tensors[0];

    /* InceptionResnetV1/Repeat/block35_5/add_300_conv_197_InceptionResnetV1/Repeat/block35_5/Relu_291 */
    node[65]->input.tensors[0] = node[64]->output.tensors[0];
    node[65]->input.tensors[1] = const_tensor[45]; /* data_vdata */

    /* InceptionResnetV1/Repeat/block35_5/add_300_reshape_202 */
    node[66]->input.tensors[0] = node[65]->output.tensors[0];

    /* InceptionResnetV1/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool_280 */
    node[67]->input.tensors[0] = node[66]->output.tensors[0];

    /* InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/convolution_299_InceptionResnetV1/Mixed_6a/Branch_0/Conv2d_1a_3x3/Relu_279 */
    node[68]->input.tensors[0] = node[66]->output.tensors[0];
    node[68]->input.tensors[1] = const_tensor[46]; /* data_vdata */

    /* InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/convolution_348_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0a_1x1/Relu_330 */
    node[69]->input.tensors[0] = node[66]->output.tensors[0];
    node[69]->input.tensors[1] = const_tensor[47]; /* data_vdata */

    /* InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/convolution_323_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_0b_3x3/Relu_306 */
    node[70]->input.tensors[0] = node[69]->output.tensors[0];
    node[70]->input.tensors[1] = const_tensor[48]; /* data_vdata */

    /* InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/convolution_298_InceptionResnetV1/Mixed_6a/Branch_1/Conv2d_1a_3x3/Relu_278 */
    node[71]->input.tensors[0] = node[70]->output.tensors[0];
    node[71]->input.tensors[1] = const_tensor[49]; /* data_vdata */

    /* InceptionResnetV1/Mixed_6a/concat_269 */
    node[72]->input.tensors[0] = node[68]->output.tensors[0];
    node[72]->input.tensors[1] = node[71]->output.tensors[0];
    node[72]->input.tensors[2] = node[67]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/convolution_319_InceptionResnetV1/Repeat_1/block17_1/Branch_0/Conv2d_1x1/Relu_302 */
    node[73]->input.tensors[0] = node[72]->output.tensors[0];
    node[73]->input.tensors[1] = const_tensor[50]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/convolution_379_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/Relu_358 */
    node[74]->input.tensors[0] = node[72]->output.tensors[0];
    node[74]->input.tensors[1] = const_tensor[51]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/convolution_346_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/Relu_327 */
    node[75]->input.tensors[0] = node[74]->output.tensors[0];
    node[75]->input.tensors[1] = const_tensor[52]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/convolution_318_InceptionResnetV1/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/Relu_301 */
    node[76]->input.tensors[0] = node[75]->output.tensors[0];
    node[76]->input.tensors[1] = const_tensor[53]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/concat_292 */
    node[77]->input.tensors[0] = node[73]->output.tensors[0];
    node[77]->input.tensors[1] = node[76]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_1/Conv2d_1x1/convolution_281 */
    node[78]->input.tensors[0] = node[77]->output.tensors[0];
    node[78]->input.tensors[1] = const_tensor[54]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/add_261_concat_182 */
    node[79]->input.tensors[0] = node[72]->output.tensors[0];
    node[79]->input.tensors[1] = node[78]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_190 */
    node[80]->input.tensors[0] = node[79]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_1/add_261_conv_192_InceptionResnetV1/Repeat_1/block17_1/Relu_251 */
    node[81]->input.tensors[0] = node[80]->output.tensors[0];
    node[81]->input.tensors[1] = const_tensor[55]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_1/add_261_reshape_193 */
    node[82]->input.tensors[0] = node[81]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/convolution_296_InceptionResnetV1/Repeat_1/block17_2/Branch_0/Conv2d_1x1/Relu_276 */
    node[83]->input.tensors[0] = node[82]->output.tensors[0];
    node[83]->input.tensors[1] = const_tensor[56]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/convolution_347_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/Relu_329 */
    node[84]->input.tensors[0] = node[82]->output.tensors[0];
    node[84]->input.tensors[1] = const_tensor[57]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/convolution_322_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/Relu_305 */
    node[85]->input.tensors[0] = node[84]->output.tensors[0];
    node[85]->input.tensors[1] = const_tensor[58]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/convolution_297_InceptionResnetV1/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/Relu_277 */
    node[86]->input.tensors[0] = node[85]->output.tensors[0];
    node[86]->input.tensors[1] = const_tensor[59]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/concat_268 */
    node[87]->input.tensors[0] = node[83]->output.tensors[0];
    node[87]->input.tensors[1] = node[86]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_2/Conv2d_1x1/convolution_260 */
    node[88]->input.tensors[0] = node[87]->output.tensors[0];
    node[88]->input.tensors[1] = const_tensor[60]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/add_243_concat_169 */
    node[89]->input.tensors[0] = node[82]->output.tensors[0];
    node[89]->input.tensors[1] = node[88]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_173 */
    node[90]->input.tensors[0] = node[89]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_2/add_243_conv_178_InceptionResnetV1/Repeat_1/block17_2/Relu_234 */
    node[91]->input.tensors[0] = node[90]->output.tensors[0];
    node[91]->input.tensors[1] = const_tensor[61]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_2/add_243_reshape_179 */
    node[92]->input.tensors[0] = node[91]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/convolution_275_InceptionResnetV1/Repeat_1/block17_3/Branch_0/Conv2d_1x1/Relu_258 */
    node[93]->input.tensors[0] = node[92]->output.tensors[0];
    node[93]->input.tensors[1] = const_tensor[62]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/convolution_328_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/Relu_312 */
    node[94]->input.tensors[0] = node[92]->output.tensors[0];
    node[94]->input.tensors[1] = const_tensor[63]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/convolution_304_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/Relu_286 */
    node[95]->input.tensors[0] = node[94]->output.tensors[0];
    node[95]->input.tensors[1] = const_tensor[64]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/convolution_274_InceptionResnetV1/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/Relu_257 */
    node[96]->input.tensors[0] = node[95]->output.tensors[0];
    node[96]->input.tensors[1] = const_tensor[65]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/concat_249 */
    node[97]->input.tensors[0] = node[93]->output.tensors[0];
    node[97]->input.tensors[1] = node[96]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_3/Conv2d_1x1/convolution_241 */
    node[98]->input.tensors[0] = node[97]->output.tensors[0];
    node[98]->input.tensors[1] = const_tensor[66]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/add_223_concat_161 */
    node[99]->input.tensors[0] = node[92]->output.tensors[0];
    node[99]->input.tensors[1] = node[98]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_164 */
    node[100]->input.tensors[0] = node[99]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_3/add_223_conv_166_InceptionResnetV1/Repeat_1/block17_3/Relu_214 */
    node[101]->input.tensors[0] = node[100]->output.tensors[0];
    node[101]->input.tensors[1] = const_tensor[67]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_3/add_223_reshape_167 */
    node[102]->input.tensors[0] = node[101]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/convolution_263_InceptionResnetV1/Repeat_1/block17_4/Branch_0/Conv2d_1x1/Relu_245 */
    node[103]->input.tensors[0] = node[102]->output.tensors[0];
    node[103]->input.tensors[1] = const_tensor[68]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/convolution_320_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/Relu_303 */
    node[104]->input.tensors[0] = node[102]->output.tensors[0];
    node[104]->input.tensors[1] = const_tensor[69]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/convolution_293_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/Relu_271 */
    node[105]->input.tensors[0] = node[104]->output.tensors[0];
    node[105]->input.tensors[1] = const_tensor[70]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/convolution_262_InceptionResnetV1/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/Relu_244 */
    node[106]->input.tensors[0] = node[105]->output.tensors[0];
    node[106]->input.tensors[1] = const_tensor[71]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/concat_235 */
    node[107]->input.tensors[0] = node[103]->output.tensors[0];
    node[107]->input.tensors[1] = node[106]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_4/Conv2d_1x1/convolution_225 */
    node[108]->input.tensors[0] = node[107]->output.tensors[0];
    node[108]->input.tensors[1] = const_tensor[72]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/add_207_concat_146 */
    node[109]->input.tensors[0] = node[102]->output.tensors[0];
    node[109]->input.tensors[1] = node[108]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_147 */
    node[110]->input.tensors[0] = node[109]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_4/add_207_conv_150_InceptionResnetV1/Repeat_1/block17_4/Relu_198 */
    node[111]->input.tensors[0] = node[110]->output.tensors[0];
    node[111]->input.tensors[1] = const_tensor[73]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_4/add_207_reshape_156 */
    node[112]->input.tensors[0] = node[111]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/convolution_239_InceptionResnetV1/Repeat_1/block17_5/Branch_0/Conv2d_1x1/Relu_221 */
    node[113]->input.tensors[0] = node[112]->output.tensors[0];
    node[113]->input.tensors[1] = const_tensor[74]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/convolution_294_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/Relu_273 */
    node[114]->input.tensors[0] = node[112]->output.tensors[0];
    node[114]->input.tensors[1] = const_tensor[75]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/convolution_265_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/Relu_248 */
    node[115]->input.tensors[0] = node[114]->output.tensors[0];
    node[115]->input.tensors[1] = const_tensor[76]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/convolution_240_InceptionResnetV1/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/Relu_222 */
    node[116]->input.tensors[0] = node[115]->output.tensors[0];
    node[116]->input.tensors[1] = const_tensor[77]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/concat_213 */
    node[117]->input.tensors[0] = node[113]->output.tensors[0];
    node[117]->input.tensors[1] = node[116]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_5/Conv2d_1x1/convolution_205 */
    node[118]->input.tensors[0] = node[117]->output.tensors[0];
    node[118]->input.tensors[1] = const_tensor[78]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/add_189_concat_135 */
    node[119]->input.tensors[0] = node[112]->output.tensors[0];
    node[119]->input.tensors[1] = node[118]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_141 */
    node[120]->input.tensors[0] = node[119]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_5/add_189_conv_144_InceptionResnetV1/Repeat_1/block17_5/Relu_181 */
    node[121]->input.tensors[0] = node[120]->output.tensors[0];
    node[121]->input.tensors[1] = const_tensor[79]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_5/add_189_reshape_145 */
    node[122]->input.tensors[0] = node[121]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/convolution_227_InceptionResnetV1/Repeat_1/block17_6/Branch_0/Conv2d_1x1/Relu_209 */
    node[123]->input.tensors[0] = node[122]->output.tensors[0];
    node[123]->input.tensors[1] = const_tensor[80]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/convolution_284_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/Relu_264 */
    node[124]->input.tensors[0] = node[122]->output.tensors[0];
    node[124]->input.tensors[1] = const_tensor[81]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/convolution_254_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/Relu_236 */
    node[125]->input.tensors[0] = node[124]->output.tensors[0];
    node[125]->input.tensors[1] = const_tensor[82]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/convolution_226_InceptionResnetV1/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/Relu_208 */
    node[126]->input.tensors[0] = node[125]->output.tensors[0];
    node[126]->input.tensors[1] = const_tensor[83]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/concat_199 */
    node[127]->input.tensors[0] = node[123]->output.tensors[0];
    node[127]->input.tensors[1] = node[126]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_6/Conv2d_1x1/convolution_191 */
    node[128]->input.tensors[0] = node[127]->output.tensors[0];
    node[128]->input.tensors[1] = const_tensor[84]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/add_174_concat_122 */
    node[129]->input.tensors[0] = node[122]->output.tensors[0];
    node[129]->input.tensors[1] = node[128]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_123 */
    node[130]->input.tensors[0] = node[129]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_6/add_174_conv_125_InceptionResnetV1/Repeat_1/block17_6/Relu_165 */
    node[131]->input.tensors[0] = node[130]->output.tensors[0];
    node[131]->input.tensors[1] = const_tensor[85]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_6/add_174_reshape_130 */
    node[132]->input.tensors[0] = node[131]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/convolution_204_InceptionResnetV1/Repeat_1/block17_7/Branch_0/Conv2d_1x1/Relu_188 */
    node[133]->input.tensors[0] = node[132]->output.tensors[0];
    node[133]->input.tensors[1] = const_tensor[86]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/convolution_255_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/Relu_238 */
    node[134]->input.tensors[0] = node[132]->output.tensors[0];
    node[134]->input.tensors[1] = const_tensor[87]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/convolution_230_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/Relu_212 */
    node[135]->input.tensors[0] = node[134]->output.tensors[0];
    node[135]->input.tensors[1] = const_tensor[88]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/convolution_203_InceptionResnetV1/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/Relu_187 */
    node[136]->input.tensors[0] = node[135]->output.tensors[0];
    node[136]->input.tensors[1] = const_tensor[89]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/concat_180 */
    node[137]->input.tensors[0] = node[133]->output.tensors[0];
    node[137]->input.tensors[1] = node[136]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_7/Conv2d_1x1/convolution_172 */
    node[138]->input.tensors[0] = node[137]->output.tensors[0];
    node[138]->input.tensors[1] = const_tensor[90]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/add_157_concat_113 */
    node[139]->input.tensors[0] = node[132]->output.tensors[0];
    node[139]->input.tensors[1] = node[138]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_117 */
    node[140]->input.tensors[0] = node[139]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_7/add_157_conv_118_InceptionResnetV1/Repeat_1/block17_7/Relu_148 */
    node[141]->input.tensors[0] = node[140]->output.tensors[0];
    node[141]->input.tensors[1] = const_tensor[91]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_7/add_157_reshape_121 */
    node[142]->input.tensors[0] = node[141]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/convolution_185_InceptionResnetV1/Repeat_1/block17_8/Branch_0/Conv2d_1x1/Relu_170 */
    node[143]->input.tensors[0] = node[142]->output.tensors[0];
    node[143]->input.tensors[1] = const_tensor[92]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/convolution_237_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/Relu_219 */
    node[144]->input.tensors[0] = node[142]->output.tensors[0];
    node[144]->input.tensors[1] = const_tensor[93]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/convolution_211_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/Relu_194 */
    node[145]->input.tensors[0] = node[144]->output.tensors[0];
    node[145]->input.tensors[1] = const_tensor[94]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/convolution_186_InceptionResnetV1/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/Relu_171 */
    node[146]->input.tensors[0] = node[145]->output.tensors[0];
    node[146]->input.tensors[1] = const_tensor[95]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/concat_163 */
    node[147]->input.tensors[0] = node[143]->output.tensors[0];
    node[147]->input.tensors[1] = node[146]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_8/Conv2d_1x1/convolution_155 */
    node[148]->input.tensors[0] = node[147]->output.tensors[0];
    node[148]->input.tensors[1] = const_tensor[96]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/add_139_concat_94 */
    node[149]->input.tensors[0] = node[142]->output.tensors[0];
    node[149]->input.tensors[1] = node[148]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_105 */
    node[150]->input.tensors[0] = node[149]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_8/add_139_conv_106_InceptionResnetV1/Repeat_1/block17_8/Relu_129 */
    node[151]->input.tensors[0] = node[150]->output.tensors[0];
    node[151]->input.tensors[1] = const_tensor[97]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_8/add_139_reshape_108 */
    node[152]->input.tensors[0] = node[151]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/convolution_176_InceptionResnetV1/Repeat_1/block17_9/Branch_0/Conv2d_1x1/Relu_159 */
    node[153]->input.tensors[0] = node[152]->output.tensors[0];
    node[153]->input.tensors[1] = const_tensor[98]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/convolution_228_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/Relu_210 */
    node[154]->input.tensors[0] = node[152]->output.tensors[0];
    node[154]->input.tensors[1] = const_tensor[99]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/convolution_200_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/Relu_183 */
    node[155]->input.tensors[0] = node[154]->output.tensors[0];
    node[155]->input.tensors[1] = const_tensor[100]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/convolution_175_InceptionResnetV1/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/Relu_158 */
    node[156]->input.tensors[0] = node[155]->output.tensors[0];
    node[156]->input.tensors[1] = const_tensor[101]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/concat_149 */
    node[157]->input.tensors[0] = node[153]->output.tensors[0];
    node[157]->input.tensors[1] = node[156]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_9/Conv2d_1x1/convolution_140 */
    node[158]->input.tensors[0] = node[157]->output.tensors[0];
    node[158]->input.tensors[1] = const_tensor[102]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/add_120_concat_88 */
    node[159]->input.tensors[0] = node[152]->output.tensors[0];
    node[159]->input.tensors[1] = node[158]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_89 */
    node[160]->input.tensors[0] = node[159]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_9/add_120_conv_90_InceptionResnetV1/Repeat_1/block17_9/Relu_109 */
    node[161]->input.tensors[0] = node[160]->output.tensors[0];
    node[161]->input.tensors[1] = const_tensor[103]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_9/add_120_reshape_91 */
    node[162]->input.tensors[0] = node[161]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/convolution_154_InceptionResnetV1/Repeat_1/block17_10/Branch_0/Conv2d_1x1/Relu_138 */
    node[163]->input.tensors[0] = node[162]->output.tensors[0];
    node[163]->input.tensors[1] = const_tensor[104]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/convolution_201_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/Relu_184 */
    node[164]->input.tensors[0] = node[162]->output.tensors[0];
    node[164]->input.tensors[1] = const_tensor[105]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/convolution_177_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/Relu_162 */
    node[165]->input.tensors[0] = node[164]->output.tensors[0];
    node[165]->input.tensors[1] = const_tensor[106]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/convolution_153_InceptionResnetV1/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/Relu_137 */
    node[166]->input.tensors[0] = node[165]->output.tensors[0];
    node[166]->input.tensors[1] = const_tensor[107]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/concat_128 */
    node[167]->input.tensors[0] = node[163]->output.tensors[0];
    node[167]->input.tensors[1] = node[166]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_1/block17_10/Conv2d_1x1/convolution_119 */
    node[168]->input.tensors[0] = node[167]->output.tensors[0];
    node[168]->input.tensors[1] = const_tensor[108]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/add_98_concat_74 */
    node[169]->input.tensors[0] = node[162]->output.tensors[0];
    node[169]->input.tensors[1] = node[168]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_81 */
    node[170]->input.tensors[0] = node[169]->output.tensors[0];

    /* InceptionResnetV1/Repeat_1/block17_10/add_98_conv_83_InceptionResnetV1/Repeat_1/block17_10/Relu_87 */
    node[171]->input.tensors[0] = node[170]->output.tensors[0];
    node[171]->input.tensors[1] = const_tensor[109]; /* data_vdata */

    /* InceptionResnetV1/Repeat_1/block17_10/add_98_reshape_86 */
    node[172]->input.tensors[0] = node[171]->output.tensors[0];

    /* InceptionResnetV1/Mixed_7a/Branch_3/MaxPool_1a_3x3/MaxPool_76 */
    node[173]->input.tensors[0] = node[172]->output.tensors[0];

    /* InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/convolution_131_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_0a_1x1/Relu_110 */
    node[174]->input.tensors[0] = node[172]->output.tensors[0];
    node[174]->input.tensors[1] = const_tensor[110]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/convolution_132_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_0a_1x1/Relu_111 */
    node[175]->input.tensors[0] = node[172]->output.tensors[0];
    node[175]->input.tensors[1] = const_tensor[111]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/convolution_152_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0a_1x1/Relu_136 */
    node[176]->input.tensors[0] = node[172]->output.tensors[0];
    node[176]->input.tensors[1] = const_tensor[112]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/convolution_99_InceptionResnetV1/Mixed_7a/Branch_0/Conv2d_1a_3x3/Relu_77 */
    node[177]->input.tensors[0] = node[174]->output.tensors[0];
    node[177]->input.tensors[1] = const_tensor[113]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/convolution_100_InceptionResnetV1/Mixed_7a/Branch_1/Conv2d_1a_3x3/Relu_78 */
    node[178]->input.tensors[0] = node[175]->output.tensors[0];
    node[178]->input.tensors[1] = const_tensor[114]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/convolution_127_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_0b_3x3/Relu_107 */
    node[179]->input.tensors[0] = node[176]->output.tensors[0];
    node[179]->input.tensors[1] = const_tensor[115]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/convolution_97_InceptionResnetV1/Mixed_7a/Branch_2/Conv2d_1a_3x3/Relu_75 */
    node[180]->input.tensors[0] = node[179]->output.tensors[0];
    node[180]->input.tensors[1] = const_tensor[116]; /* data_vdata */

    /* InceptionResnetV1/Mixed_7a/concat/axis_66 */
    node[181]->input.tensors[0] = node[177]->output.tensors[0];
    node[181]->input.tensors[1] = node[178]->output.tensors[0];
    node[181]->input.tensors[2] = node[180]->output.tensors[0];
    node[181]->input.tensors[3] = node[173]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/convolution_116_InceptionResnetV1/Repeat_2/block8_1/Branch_0/Conv2d_1x1/Relu_96 */
    node[182]->input.tensors[0] = node[181]->output.tensors[0];
    node[182]->input.tensors[1] = const_tensor[117]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/convolution_168_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/Relu_151 */
    node[183]->input.tensors[0] = node[181]->output.tensors[0];
    node[183]->input.tensors[1] = const_tensor[118]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/convolution_143_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/Relu_126 */
    node[184]->input.tensors[0] = node[183]->output.tensors[0];
    node[184]->input.tensors[1] = const_tensor[119]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/convolution_115_InceptionResnetV1/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/Relu_95 */
    node[185]->input.tensors[0] = node[184]->output.tensors[0];
    node[185]->input.tensors[1] = const_tensor[120]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/concat_85 */
    node[186]->input.tensors[0] = node[182]->output.tensors[0];
    node[186]->input.tensors[1] = node[185]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_2/block8_1/Conv2d_1x1/convolution_73 */
    node[187]->input.tensors[0] = node[186]->output.tensors[0];
    node[187]->input.tensors[1] = const_tensor[121]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/add_56_concat_63 */
    node[188]->input.tensors[0] = node[181]->output.tensors[0];
    node[188]->input.tensors[1] = node[187]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_64 */
    node[189]->input.tensors[0] = node[188]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_1/add_56_conv_65_InceptionResnetV1/Repeat_2/block8_1/Relu_48 */
    node[190]->input.tensors[0] = node[189]->output.tensors[0];
    node[190]->input.tensors[1] = const_tensor[122]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_1/add_56_reshape_69 */
    node[191]->input.tensors[0] = node[190]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/convolution_101_InceptionResnetV1/Repeat_2/block8_2/Branch_0/Conv2d_1x1/Relu_79 */
    node[192]->input.tensors[0] = node[191]->output.tensors[0];
    node[192]->input.tensors[1] = const_tensor[123]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/convolution_160_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/Relu_142 */
    node[193]->input.tensors[0] = node[191]->output.tensors[0];
    node[193]->input.tensors[1] = const_tensor[124]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/convolution_133_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/Relu_112 */
    node[194]->input.tensors[0] = node[193]->output.tensors[0];
    node[194]->input.tensors[1] = const_tensor[125]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/convolution_102_InceptionResnetV1/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/Relu_80 */
    node[195]->input.tensors[0] = node[194]->output.tensors[0];
    node[195]->input.tensors[1] = const_tensor[126]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/concat_67 */
    node[196]->input.tensors[0] = node[192]->output.tensors[0];
    node[196]->input.tensors[1] = node[195]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_2/block8_2/Conv2d_1x1/convolution_57 */
    node[197]->input.tensors[0] = node[196]->output.tensors[0];
    node[197]->input.tensors[1] = const_tensor[127]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/add_42_concat_50 */
    node[198]->input.tensors[0] = node[191]->output.tensors[0];
    node[198]->input.tensors[1] = node[197]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_51 */
    node[199]->input.tensors[0] = node[198]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_2/add_42_conv_53_InceptionResnetV1/Repeat_2/block8_2/Relu_35 */
    node[200]->input.tensors[0] = node[199]->output.tensors[0];
    node[200]->input.tensors[1] = const_tensor[128]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_2/add_42_reshape_58 */
    node[201]->input.tensors[0] = node[200]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/convolution_71_InceptionResnetV1/Repeat_2/block8_3/Branch_0/Conv2d_1x1/Relu_54 */
    node[202]->input.tensors[0] = node[201]->output.tensors[0];
    node[202]->input.tensors[1] = const_tensor[129]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/convolution_134_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/Relu_114 */
    node[203]->input.tensors[0] = node[201]->output.tensors[0];
    node[203]->input.tensors[1] = const_tensor[130]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/convolution_104_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/Relu_84 */
    node[204]->input.tensors[0] = node[203]->output.tensors[0];
    node[204]->input.tensors[1] = const_tensor[131]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/convolution_72_InceptionResnetV1/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/Relu_55 */
    node[205]->input.tensors[0] = node[204]->output.tensors[0];
    node[205]->input.tensors[1] = const_tensor[132]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/concat_47 */
    node[206]->input.tensors[0] = node[202]->output.tensors[0];
    node[206]->input.tensors[1] = node[205]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_2/block8_3/Conv2d_1x1/convolution_41 */
    node[207]->input.tensors[0] = node[206]->output.tensors[0];
    node[207]->input.tensors[1] = const_tensor[133]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/add_27_concat_34 */
    node[208]->input.tensors[0] = node[201]->output.tensors[0];
    node[208]->input.tensors[1] = node[207]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_40 */
    node[209]->input.tensors[0] = node[208]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_3/add_27_conv_45_InceptionResnetV1/Repeat_2/block8_3/Relu_21 */
    node[210]->input.tensors[0] = node[209]->output.tensors[0];
    node[210]->input.tensors[1] = const_tensor[134]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_3/add_27_reshape_49 */
    node[211]->input.tensors[0] = node[210]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/convolution_60_InceptionResnetV1/Repeat_2/block8_4/Branch_0/Conv2d_1x1/Relu_44 */
    node[212]->input.tensors[0] = node[211]->output.tensors[0];
    node[212]->input.tensors[1] = const_tensor[135]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/convolution_124_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/Relu_103 */
    node[213]->input.tensors[0] = node[211]->output.tensors[0];
    node[213]->input.tensors[1] = const_tensor[136]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/convolution_92_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/Relu_68 */
    node[214]->input.tensors[0] = node[213]->output.tensors[0];
    node[214]->input.tensors[1] = const_tensor[137]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/convolution_59_InceptionResnetV1/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/Relu_43 */
    node[215]->input.tensors[0] = node[214]->output.tensors[0];
    node[215]->input.tensors[1] = const_tensor[138]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/concat_36 */
    node[216]->input.tensors[0] = node[212]->output.tensors[0];
    node[216]->input.tensors[1] = node[215]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_2/block8_4/Conv2d_1x1/convolution_28 */
    node[217]->input.tensors[0] = node[216]->output.tensors[0];
    node[217]->input.tensors[1] = const_tensor[139]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/add_17_concat_24 */
    node[218]->input.tensors[0] = node[211]->output.tensors[0];
    node[218]->input.tensors[1] = node[217]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_29 */
    node[219]->input.tensors[0] = node[218]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_4/add_17_conv_32_InceptionResnetV1/Repeat_2/block8_4/Relu_13 */
    node[220]->input.tensors[0] = node[219]->output.tensors[0];
    node[220]->input.tensors[1] = const_tensor[140]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_4/add_17_reshape_33 */
    node[221]->input.tensors[0] = node[220]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/convolution_39_InceptionResnetV1/Repeat_2/block8_5/Branch_0/Conv2d_1x1/Relu_26 */
    node[222]->input.tensors[0] = node[221]->output.tensors[0];
    node[222]->input.tensors[1] = const_tensor[141]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/convolution_93_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/Relu_70 */
    node[223]->input.tensors[0] = node[221]->output.tensors[0];
    node[223]->input.tensors[1] = const_tensor[142]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/convolution_62_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/Relu_46 */
    node[224]->input.tensors[0] = node[223]->output.tensors[0];
    node[224]->input.tensors[1] = const_tensor[143]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/convolution_38_InceptionResnetV1/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/Relu_25 */
    node[225]->input.tensors[0] = node[224]->output.tensors[0];
    node[225]->input.tensors[1] = const_tensor[144]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/concat_20 */
    node[226]->input.tensors[0] = node[222]->output.tensors[0];
    node[226]->input.tensors[1] = node[225]->output.tensors[0];

    /* trans_InceptionResnetV1/Repeat_2/block8_5/Conv2d_1x1/convolution_16 */
    node[227]->input.tensors[0] = node[226]->output.tensors[0];
    node[227]->input.tensors[1] = const_tensor[145]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/add_9_concat_12 */
    node[228]->input.tensors[0] = node[221]->output.tensors[0];
    node[228]->input.tensors[1] = node[227]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_15 */
    node[229]->input.tensors[0] = node[228]->output.tensors[0];

    /* InceptionResnetV1/Repeat_2/block8_5/add_9_conv_22_InceptionResnetV1/Repeat_2/block8_5/Relu_7 */
    node[230]->input.tensors[0] = node[229]->output.tensors[0];
    node[230]->input.tensors[1] = const_tensor[146]; /* data_vdata */

    /* InceptionResnetV1/Repeat_2/block8_5/add_9_reshape_23 */
    node[231]->input.tensors[0] = node[230]->output.tensors[0];

    /* InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/convolution_30_InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/Relu_18 */
    node[232]->input.tensors[0] = node[231]->output.tensors[0];
    node[232]->input.tensors[1] = const_tensor[147]; /* data_vdata */

    /* InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/convolution_82_InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/Relu_61 */
    node[233]->input.tensors[0] = node[231]->output.tensors[0];
    node[233]->input.tensors[1] = const_tensor[148]; /* data_vdata */

    /* InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/convolution_52_InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/Relu_37 */
    node[234]->input.tensors[0] = node[233]->output.tensors[0];
    node[234]->input.tensors[1] = const_tensor[149]; /* data_vdata */

    /* InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/convolution_31_InceptionResnetV1/Block8/Branch_1/Conv2d_0c_3x1/Relu_19 */
    node[235]->input.tensors[0] = node[234]->output.tensors[0];
    node[235]->input.tensors[1] = const_tensor[150]; /* data_vdata */

    /* InceptionResnetV1/Block8/concat_14 */
    node[236]->input.tensors[0] = node[232]->output.tensors[0];
    node[236]->input.tensors[1] = node[235]->output.tensors[0];

    /* trans_InceptionResnetV1/Block8/Conv2d_1x1/convolution_11 */
    node[237]->input.tensors[0] = node[236]->output.tensors[0];
    node[237]->input.tensors[1] = const_tensor[151]; /* data_vdata */

    /* InceptionResnetV1/Block8/add_6_concat_2 */
    node[238]->input.tensors[0] = node[231]->output.tensors[0];
    node[238]->input.tensors[1] = node[237]->output.tensors[0];

    /* InceptionResnetV1/Block8/add_6_reshape_5 */
    node[239]->input.tensors[0] = node[238]->output.tensors[0];

    /* trans_InceptionResnetV1/Block8/add_6_conv_8 */
    node[240]->input.tensors[0] = node[239]->output.tensors[0];
    node[240]->input.tensors[1] = const_tensor[152]; /* data_vdata */

    /* InceptionResnetV1/Block8/add_6_reshape_10 */
    node[241]->input.tensors[0] = node[240]->output.tensors[0];

    /* trans_InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470 */
    node[242]->input.tensors[0] = node[241]->output.tensors[0];
    node[242]->input.tensors[1] = const_tensor[153]; /* data_vdata */

    /* InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool_5_conv_470_reshape_253 */
    node[243]->input.tensors[0] = node[242]->output.tensors[0];

    /* InceptionResnetV1/Logits/Flatten/Reshape/shape_4_insert_permute_in_0_256 */
    node[244]->input.tensors[0] = node[243]->output.tensors[0];

    /* InceptionResnetV1/Logits/Flatten/Reshape/shape_4 */
    node[245]->input.tensors[0] = node[244]->output.tensors[0];

    /* trans_InceptionResnetV1/Bottleneck/MatMul_3 */
    node[246]->input.tensors[0] = node[245]->output.tensors[0];
    node[246]->input.tensors[1] = const_tensor[154]; /* data_vdata */



    graph->input.tensors[0] = norm_tensor[0];
    graph->output.tensors[0] = norm_tensor[1];


    status = vsi_nn_SetupGraph( graph, FALSE );
    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vsi_nn_DumpGraphToJson(graph);
    vnn_ReleaseFacenet7d( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateFacenet7d() */

void vnn_ReleaseFacenet7d
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseFacenet7d() */

