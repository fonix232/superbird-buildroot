/****************************************************************************
*   Generated by ACUITY 3.13.0
*   Match ovxlib 1.0.10
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_yoloface7d.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        _node->uid = (uint32_t)_uid; \
        if( NULL == _node ) {\
            goto error;\
        }\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (46)
#define NET_NORM_TENSOR_NUM     (2)
#define NET_CONST_TENSOR_NUM    (22)
#define NET_VIRTUAL_TENSOR_NUM  (46)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM + 32)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static void load_hw_config
    (
    vsi_nn_context_t ctx
    )
{
    ctx->config.evis.ver = VSI_NN_HW_EVIS_2;
    strncpy(ctx->config.target_name, "VIPNANOQI_PID0X7D", VSI_NN_MAX_TARGET_NAME);
}

static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    VSILOGI("Read %d data.", ret);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateYoloFace7d
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx
    )
{
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;



    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
        load_hw_config(ctx);
    }
    else
    {
        ctx = in_ctx;
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 1 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - conv1_1_pool1_5
      var       - node[0]
      name      - convolutionrelupool
      operation - convolutionrelupool
      in_shape  - [[416, 416, 3, 1]]
      out_shape - [[208, 208, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_CONV_RELU_POOL, 2, 1, 5);
    node[0]->nn_param.conv2d.ksize[0] = 3;
    node[0]->nn_param.conv2d.ksize[1] = 3;
    node[0]->nn_param.conv2d.weights = 32;
    node[0]->nn_param.conv2d.stride[0] = 1;
    node[0]->nn_param.conv2d.stride[1] = 1;
    node[0]->nn_param.conv2d.pad[0] = 1;
    node[0]->nn_param.conv2d.pad[1] = 1;
    node[0]->nn_param.conv2d.pad[2] = 1;
    node[0]->nn_param.conv2d.pad[3] = 1;
    node[0]->nn_param.conv2d.group = 1;
    node[0]->nn_param.conv2d.dilation[0] = 1;
    node[0]->nn_param.conv2d.dilation[1] = 1;
    node[0]->nn_param.conv2d.multiplier = 0;
    node[0]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[0]->nn_param.pool.ksize[0] = 2;
    node[0]->nn_param.pool.ksize[1] = 2;
    node[0]->nn_param.pool.stride[0] = 2;
    node[0]->nn_param.pool.stride[1] = 2;
    node[0]->nn_param.pool.pad[0] = 0;
    node[0]->nn_param.pool.pad[1] = 0;
    node[0]->nn_param.pool.pad[2] = 0;
    node[0]->nn_param.pool.pad[3] = 0;
    node[0]->vx_param.has_relu = FALSE;
    node[0]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[0]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[0]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu1_4
      var       - node[1]
      name      - relu1
      operation - leakyrelu
      in_shape  - [[208, 208, 32, 1]]
      out_shape - [[208, 208, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_LEAKY_RELU, 1, 1, 4);
    node[1]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - conv2_6_pool2_10
      var       - node[2]
      name      - convolutionrelupool
      operation - convolutionrelupool
      in_shape  - [[208, 208, 32, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_CONV_RELU_POOL, 2, 1, 10);
    node[2]->nn_param.conv2d.ksize[0] = 3;
    node[2]->nn_param.conv2d.ksize[1] = 3;
    node[2]->nn_param.conv2d.weights = 64;
    node[2]->nn_param.conv2d.stride[0] = 1;
    node[2]->nn_param.conv2d.stride[1] = 1;
    node[2]->nn_param.conv2d.pad[0] = 1;
    node[2]->nn_param.conv2d.pad[1] = 1;
    node[2]->nn_param.conv2d.pad[2] = 1;
    node[2]->nn_param.conv2d.pad[3] = 1;
    node[2]->nn_param.conv2d.group = 1;
    node[2]->nn_param.conv2d.dilation[0] = 1;
    node[2]->nn_param.conv2d.dilation[1] = 1;
    node[2]->nn_param.conv2d.multiplier = 0;
    node[2]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[2]->nn_param.pool.ksize[0] = 2;
    node[2]->nn_param.pool.ksize[1] = 2;
    node[2]->nn_param.pool.stride[0] = 2;
    node[2]->nn_param.pool.stride[1] = 2;
    node[2]->nn_param.pool.pad[0] = 0;
    node[2]->nn_param.pool.pad[1] = 0;
    node[2]->nn_param.pool.pad[2] = 0;
    node[2]->nn_param.pool.pad[3] = 0;
    node[2]->vx_param.has_relu = FALSE;
    node[2]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[2]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[2]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu2_9
      var       - node[3]
      name      - relu2
      operation - leakyrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_LEAKY_RELU, 1, 1, 9);
    node[3]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv3_11
      var       - node[4]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV_RELU, 2, 1, 11);
    node[4]->nn_param.conv2d.ksize[0] = 3;
    node[4]->nn_param.conv2d.ksize[1] = 3;
    node[4]->nn_param.conv2d.weights = 128;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 1;
    node[4]->nn_param.conv2d.pad[1] = 1;
    node[4]->nn_param.conv2d.pad[2] = 1;
    node[4]->nn_param.conv2d.pad[3] = 1;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->nn_param.conv2d.multiplier = 0;
    node[4]->vx_param.has_relu = FALSE;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu3_14
      var       - node[5]
      name      - relu3
      operation - leakyrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_LEAKY_RELU, 1, 1, 14);
    node[5]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv4_15
      var       - node[6]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[104, 104, 128, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_CONV_RELU, 2, 1, 15);
    node[6]->nn_param.conv2d.ksize[0] = 1;
    node[6]->nn_param.conv2d.ksize[1] = 1;
    node[6]->nn_param.conv2d.weights = 64;
    node[6]->nn_param.conv2d.stride[0] = 1;
    node[6]->nn_param.conv2d.stride[1] = 1;
    node[6]->nn_param.conv2d.pad[0] = 0;
    node[6]->nn_param.conv2d.pad[1] = 0;
    node[6]->nn_param.conv2d.pad[2] = 0;
    node[6]->nn_param.conv2d.pad[3] = 0;
    node[6]->nn_param.conv2d.group = 1;
    node[6]->nn_param.conv2d.dilation[0] = 1;
    node[6]->nn_param.conv2d.dilation[1] = 1;
    node[6]->nn_param.conv2d.multiplier = 0;
    node[6]->vx_param.has_relu = FALSE;
    node[6]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[6]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu4_18
      var       - node[7]
      name      - relu4
      operation - leakyrelu
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[104, 104, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_LEAKY_RELU, 1, 1, 18);
    node[7]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - conv5_19_pool5_23
      var       - node[8]
      name      - convolutionrelupool
      operation - convolutionrelupool
      in_shape  - [[104, 104, 64, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_CONV_RELU_POOL, 2, 1, 23);
    node[8]->nn_param.conv2d.ksize[0] = 3;
    node[8]->nn_param.conv2d.ksize[1] = 3;
    node[8]->nn_param.conv2d.weights = 128;
    node[8]->nn_param.conv2d.stride[0] = 1;
    node[8]->nn_param.conv2d.stride[1] = 1;
    node[8]->nn_param.conv2d.pad[0] = 1;
    node[8]->nn_param.conv2d.pad[1] = 1;
    node[8]->nn_param.conv2d.pad[2] = 1;
    node[8]->nn_param.conv2d.pad[3] = 1;
    node[8]->nn_param.conv2d.group = 1;
    node[8]->nn_param.conv2d.dilation[0] = 1;
    node[8]->nn_param.conv2d.dilation[1] = 1;
    node[8]->nn_param.conv2d.multiplier = 0;
    node[8]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[8]->nn_param.pool.ksize[0] = 2;
    node[8]->nn_param.pool.ksize[1] = 2;
    node[8]->nn_param.pool.stride[0] = 2;
    node[8]->nn_param.pool.stride[1] = 2;
    node[8]->nn_param.pool.pad[0] = 0;
    node[8]->nn_param.pool.pad[1] = 0;
    node[8]->nn_param.pool.pad[2] = 0;
    node[8]->nn_param.pool.pad[3] = 0;
    node[8]->vx_param.has_relu = FALSE;
    node[8]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[8]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[8]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu5_22
      var       - node[9]
      name      - relu5
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_LEAKY_RELU, 1, 1, 22);
    node[9]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv6_24
      var       - node[10]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_CONV_RELU, 2, 1, 24);
    node[10]->nn_param.conv2d.ksize[0] = 3;
    node[10]->nn_param.conv2d.ksize[1] = 3;
    node[10]->nn_param.conv2d.weights = 256;
    node[10]->nn_param.conv2d.stride[0] = 1;
    node[10]->nn_param.conv2d.stride[1] = 1;
    node[10]->nn_param.conv2d.pad[0] = 1;
    node[10]->nn_param.conv2d.pad[1] = 1;
    node[10]->nn_param.conv2d.pad[2] = 1;
    node[10]->nn_param.conv2d.pad[3] = 1;
    node[10]->nn_param.conv2d.group = 1;
    node[10]->nn_param.conv2d.dilation[0] = 1;
    node[10]->nn_param.conv2d.dilation[1] = 1;
    node[10]->nn_param.conv2d.multiplier = 0;
    node[10]->vx_param.has_relu = FALSE;
    node[10]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[10]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[10]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu6_27
      var       - node[11]
      name      - relu6
      operation - leakyrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_LEAKY_RELU, 1, 1, 27);
    node[11]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv7_28
      var       - node[12]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[52, 52, 256, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_CONV_RELU, 2, 1, 28);
    node[12]->nn_param.conv2d.ksize[0] = 1;
    node[12]->nn_param.conv2d.ksize[1] = 1;
    node[12]->nn_param.conv2d.weights = 128;
    node[12]->nn_param.conv2d.stride[0] = 1;
    node[12]->nn_param.conv2d.stride[1] = 1;
    node[12]->nn_param.conv2d.pad[0] = 0;
    node[12]->nn_param.conv2d.pad[1] = 0;
    node[12]->nn_param.conv2d.pad[2] = 0;
    node[12]->nn_param.conv2d.pad[3] = 0;
    node[12]->nn_param.conv2d.group = 1;
    node[12]->nn_param.conv2d.dilation[0] = 1;
    node[12]->nn_param.conv2d.dilation[1] = 1;
    node[12]->nn_param.conv2d.multiplier = 0;
    node[12]->vx_param.has_relu = FALSE;
    node[12]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[12]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[12]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu7_31
      var       - node[13]
      name      - relu7
      operation - leakyrelu
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[52, 52, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_LEAKY_RELU, 1, 1, 31);
    node[13]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - conv8_32_pool8_36
      var       - node[14]
      name      - convolutionrelupool
      operation - convolutionrelupool
      in_shape  - [[52, 52, 128, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_CONV_RELU_POOL, 2, 1, 36);
    node[14]->nn_param.conv2d.ksize[0] = 3;
    node[14]->nn_param.conv2d.ksize[1] = 3;
    node[14]->nn_param.conv2d.weights = 256;
    node[14]->nn_param.conv2d.stride[0] = 1;
    node[14]->nn_param.conv2d.stride[1] = 1;
    node[14]->nn_param.conv2d.pad[0] = 1;
    node[14]->nn_param.conv2d.pad[1] = 1;
    node[14]->nn_param.conv2d.pad[2] = 1;
    node[14]->nn_param.conv2d.pad[3] = 1;
    node[14]->nn_param.conv2d.group = 1;
    node[14]->nn_param.conv2d.dilation[0] = 1;
    node[14]->nn_param.conv2d.dilation[1] = 1;
    node[14]->nn_param.conv2d.multiplier = 0;
    node[14]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[14]->nn_param.pool.ksize[0] = 2;
    node[14]->nn_param.pool.ksize[1] = 2;
    node[14]->nn_param.pool.stride[0] = 2;
    node[14]->nn_param.pool.stride[1] = 2;
    node[14]->nn_param.pool.pad[0] = 0;
    node[14]->nn_param.pool.pad[1] = 0;
    node[14]->nn_param.pool.pad[2] = 0;
    node[14]->nn_param.pool.pad[3] = 0;
    node[14]->vx_param.has_relu = FALSE;
    node[14]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[14]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[14]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu8_35
      var       - node[15]
      name      - relu8
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_LEAKY_RELU, 1, 1, 35);
    node[15]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv9_37
      var       - node[16]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_CONV_RELU, 2, 1, 37);
    node[16]->nn_param.conv2d.ksize[0] = 3;
    node[16]->nn_param.conv2d.ksize[1] = 3;
    node[16]->nn_param.conv2d.weights = 512;
    node[16]->nn_param.conv2d.stride[0] = 1;
    node[16]->nn_param.conv2d.stride[1] = 1;
    node[16]->nn_param.conv2d.pad[0] = 1;
    node[16]->nn_param.conv2d.pad[1] = 1;
    node[16]->nn_param.conv2d.pad[2] = 1;
    node[16]->nn_param.conv2d.pad[3] = 1;
    node[16]->nn_param.conv2d.group = 1;
    node[16]->nn_param.conv2d.dilation[0] = 1;
    node[16]->nn_param.conv2d.dilation[1] = 1;
    node[16]->nn_param.conv2d.multiplier = 0;
    node[16]->vx_param.has_relu = FALSE;
    node[16]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[16]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[16]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu9_40
      var       - node[17]
      name      - relu9
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_LEAKY_RELU, 1, 1, 40);
    node[17]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv10_41
      var       - node[18]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_CONV_RELU, 2, 1, 41);
    node[18]->nn_param.conv2d.ksize[0] = 1;
    node[18]->nn_param.conv2d.ksize[1] = 1;
    node[18]->nn_param.conv2d.weights = 256;
    node[18]->nn_param.conv2d.stride[0] = 1;
    node[18]->nn_param.conv2d.stride[1] = 1;
    node[18]->nn_param.conv2d.pad[0] = 0;
    node[18]->nn_param.conv2d.pad[1] = 0;
    node[18]->nn_param.conv2d.pad[2] = 0;
    node[18]->nn_param.conv2d.pad[3] = 0;
    node[18]->nn_param.conv2d.group = 1;
    node[18]->nn_param.conv2d.dilation[0] = 1;
    node[18]->nn_param.conv2d.dilation[1] = 1;
    node[18]->nn_param.conv2d.multiplier = 0;
    node[18]->vx_param.has_relu = FALSE;
    node[18]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[18]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[18]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu10_44
      var       - node[19]
      name      - relu10
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_LEAKY_RELU, 1, 1, 44);
    node[19]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv11_45
      var       - node[20]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_CONV_RELU, 2, 1, 45);
    node[20]->nn_param.conv2d.ksize[0] = 3;
    node[20]->nn_param.conv2d.ksize[1] = 3;
    node[20]->nn_param.conv2d.weights = 512;
    node[20]->nn_param.conv2d.stride[0] = 1;
    node[20]->nn_param.conv2d.stride[1] = 1;
    node[20]->nn_param.conv2d.pad[0] = 1;
    node[20]->nn_param.conv2d.pad[1] = 1;
    node[20]->nn_param.conv2d.pad[2] = 1;
    node[20]->nn_param.conv2d.pad[3] = 1;
    node[20]->nn_param.conv2d.group = 1;
    node[20]->nn_param.conv2d.dilation[0] = 1;
    node[20]->nn_param.conv2d.dilation[1] = 1;
    node[20]->nn_param.conv2d.multiplier = 0;
    node[20]->vx_param.has_relu = FALSE;
    node[20]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[20]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[20]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu11_48
      var       - node[21]
      name      - relu11
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_LEAKY_RELU, 1, 1, 48);
    node[21]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv12_49
      var       - node[22]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[22], VSI_NN_OP_CONV_RELU, 2, 1, 49);
    node[22]->nn_param.conv2d.ksize[0] = 1;
    node[22]->nn_param.conv2d.ksize[1] = 1;
    node[22]->nn_param.conv2d.weights = 256;
    node[22]->nn_param.conv2d.stride[0] = 1;
    node[22]->nn_param.conv2d.stride[1] = 1;
    node[22]->nn_param.conv2d.pad[0] = 0;
    node[22]->nn_param.conv2d.pad[1] = 0;
    node[22]->nn_param.conv2d.pad[2] = 0;
    node[22]->nn_param.conv2d.pad[3] = 0;
    node[22]->nn_param.conv2d.group = 1;
    node[22]->nn_param.conv2d.dilation[0] = 1;
    node[22]->nn_param.conv2d.dilation[1] = 1;
    node[22]->nn_param.conv2d.multiplier = 0;
    node[22]->vx_param.has_relu = FALSE;
    node[22]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[22]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[22]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu12_52
      var       - node[23]
      name      - relu12
      operation - leakyrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[23], VSI_NN_OP_LEAKY_RELU, 1, 1, 52);
    node[23]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv13_53
      var       - node[24]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[26, 26, 256, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[24], VSI_NN_OP_CONV_RELU, 2, 1, 53);
    node[24]->nn_param.conv2d.ksize[0] = 3;
    node[24]->nn_param.conv2d.ksize[1] = 3;
    node[24]->nn_param.conv2d.weights = 512;
    node[24]->nn_param.conv2d.stride[0] = 1;
    node[24]->nn_param.conv2d.stride[1] = 1;
    node[24]->nn_param.conv2d.pad[0] = 1;
    node[24]->nn_param.conv2d.pad[1] = 1;
    node[24]->nn_param.conv2d.pad[2] = 1;
    node[24]->nn_param.conv2d.pad[3] = 1;
    node[24]->nn_param.conv2d.group = 1;
    node[24]->nn_param.conv2d.dilation[0] = 1;
    node[24]->nn_param.conv2d.dilation[1] = 1;
    node[24]->nn_param.conv2d.multiplier = 0;
    node[24]->vx_param.has_relu = FALSE;
    node[24]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[24]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[24]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu13_56
      var       - node[25]
      name      - relu13
      operation - leakyrelu
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[26, 26, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[25], VSI_NN_OP_LEAKY_RELU, 1, 1, 56);
    node[25]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - pool13_57
      var       - node[26]
      name      - pool13
      operation - pooling
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[26], VSI_NN_OP_POOL, 1, 1, 57);
    node[26]->nn_param.pool.ksize[0] = 2;
    node[26]->nn_param.pool.ksize[1] = 2;
    node[26]->nn_param.pool.stride[0] = 2;
    node[26]->nn_param.pool.stride[1] = 2;
    node[26]->nn_param.pool.pad[0] = 0;
    node[26]->nn_param.pool.pad[1] = 0;
    node[26]->nn_param.pool.pad[2] = 0;
    node[26]->nn_param.pool.pad[3] = 0;
    node[26]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[26]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[26]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - reorg_86
      var       - node[27]
      name      - reorg
      operation - reorg
      in_shape  - [[26, 26, 512, 1]]
      out_shape - [[13, 13, 2048, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[27], VSI_NN_OP_REORG, 1, 1, 86);
    node[27]->nn_param.reorg.stride = 2;

    /*-----------------------------------------
      lid       - trans_conv14_58
      var       - node[28]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[28], VSI_NN_OP_CONV_RELU, 2, 1, 58);
    node[28]->nn_param.conv2d.ksize[0] = 3;
    node[28]->nn_param.conv2d.ksize[1] = 3;
    node[28]->nn_param.conv2d.weights = 1024;
    node[28]->nn_param.conv2d.stride[0] = 1;
    node[28]->nn_param.conv2d.stride[1] = 1;
    node[28]->nn_param.conv2d.pad[0] = 1;
    node[28]->nn_param.conv2d.pad[1] = 1;
    node[28]->nn_param.conv2d.pad[2] = 1;
    node[28]->nn_param.conv2d.pad[3] = 1;
    node[28]->nn_param.conv2d.group = 1;
    node[28]->nn_param.conv2d.dilation[0] = 1;
    node[28]->nn_param.conv2d.dilation[1] = 1;
    node[28]->nn_param.conv2d.multiplier = 0;
    node[28]->vx_param.has_relu = FALSE;
    node[28]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[28]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[28]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu14_61
      var       - node[29]
      name      - relu14
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[29], VSI_NN_OP_LEAKY_RELU, 1, 1, 61);
    node[29]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv15_62
      var       - node[30]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[30], VSI_NN_OP_CONV_RELU, 2, 1, 62);
    node[30]->nn_param.conv2d.ksize[0] = 1;
    node[30]->nn_param.conv2d.ksize[1] = 1;
    node[30]->nn_param.conv2d.weights = 512;
    node[30]->nn_param.conv2d.stride[0] = 1;
    node[30]->nn_param.conv2d.stride[1] = 1;
    node[30]->nn_param.conv2d.pad[0] = 0;
    node[30]->nn_param.conv2d.pad[1] = 0;
    node[30]->nn_param.conv2d.pad[2] = 0;
    node[30]->nn_param.conv2d.pad[3] = 0;
    node[30]->nn_param.conv2d.group = 1;
    node[30]->nn_param.conv2d.dilation[0] = 1;
    node[30]->nn_param.conv2d.dilation[1] = 1;
    node[30]->nn_param.conv2d.multiplier = 0;
    node[30]->vx_param.has_relu = FALSE;
    node[30]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[30]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[30]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu15_65
      var       - node[31]
      name      - relu15
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[31], VSI_NN_OP_LEAKY_RELU, 1, 1, 65);
    node[31]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv16_66
      var       - node[32]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[32], VSI_NN_OP_CONV_RELU, 2, 1, 66);
    node[32]->nn_param.conv2d.ksize[0] = 3;
    node[32]->nn_param.conv2d.ksize[1] = 3;
    node[32]->nn_param.conv2d.weights = 1024;
    node[32]->nn_param.conv2d.stride[0] = 1;
    node[32]->nn_param.conv2d.stride[1] = 1;
    node[32]->nn_param.conv2d.pad[0] = 1;
    node[32]->nn_param.conv2d.pad[1] = 1;
    node[32]->nn_param.conv2d.pad[2] = 1;
    node[32]->nn_param.conv2d.pad[3] = 1;
    node[32]->nn_param.conv2d.group = 1;
    node[32]->nn_param.conv2d.dilation[0] = 1;
    node[32]->nn_param.conv2d.dilation[1] = 1;
    node[32]->nn_param.conv2d.multiplier = 0;
    node[32]->vx_param.has_relu = FALSE;
    node[32]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[32]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[32]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu16_69
      var       - node[33]
      name      - relu16
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[33], VSI_NN_OP_LEAKY_RELU, 1, 1, 69);
    node[33]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv17_70
      var       - node[34]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[34], VSI_NN_OP_CONV_RELU, 2, 1, 70);
    node[34]->nn_param.conv2d.ksize[0] = 1;
    node[34]->nn_param.conv2d.ksize[1] = 1;
    node[34]->nn_param.conv2d.weights = 512;
    node[34]->nn_param.conv2d.stride[0] = 1;
    node[34]->nn_param.conv2d.stride[1] = 1;
    node[34]->nn_param.conv2d.pad[0] = 0;
    node[34]->nn_param.conv2d.pad[1] = 0;
    node[34]->nn_param.conv2d.pad[2] = 0;
    node[34]->nn_param.conv2d.pad[3] = 0;
    node[34]->nn_param.conv2d.group = 1;
    node[34]->nn_param.conv2d.dilation[0] = 1;
    node[34]->nn_param.conv2d.dilation[1] = 1;
    node[34]->nn_param.conv2d.multiplier = 0;
    node[34]->vx_param.has_relu = FALSE;
    node[34]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[34]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[34]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu17_73
      var       - node[35]
      name      - relu17
      operation - leakyrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[35], VSI_NN_OP_LEAKY_RELU, 1, 1, 73);
    node[35]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv18_74
      var       - node[36]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 512, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[36], VSI_NN_OP_CONV_RELU, 2, 1, 74);
    node[36]->nn_param.conv2d.ksize[0] = 3;
    node[36]->nn_param.conv2d.ksize[1] = 3;
    node[36]->nn_param.conv2d.weights = 1024;
    node[36]->nn_param.conv2d.stride[0] = 1;
    node[36]->nn_param.conv2d.stride[1] = 1;
    node[36]->nn_param.conv2d.pad[0] = 1;
    node[36]->nn_param.conv2d.pad[1] = 1;
    node[36]->nn_param.conv2d.pad[2] = 1;
    node[36]->nn_param.conv2d.pad[3] = 1;
    node[36]->nn_param.conv2d.group = 1;
    node[36]->nn_param.conv2d.dilation[0] = 1;
    node[36]->nn_param.conv2d.dilation[1] = 1;
    node[36]->nn_param.conv2d.multiplier = 0;
    node[36]->vx_param.has_relu = FALSE;
    node[36]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[36]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[36]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu18_77
      var       - node[37]
      name      - relu18
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[37], VSI_NN_OP_LEAKY_RELU, 1, 1, 77);
    node[37]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv19_78
      var       - node[38]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[38], VSI_NN_OP_CONV_RELU, 2, 1, 78);
    node[38]->nn_param.conv2d.ksize[0] = 3;
    node[38]->nn_param.conv2d.ksize[1] = 3;
    node[38]->nn_param.conv2d.weights = 1024;
    node[38]->nn_param.conv2d.stride[0] = 1;
    node[38]->nn_param.conv2d.stride[1] = 1;
    node[38]->nn_param.conv2d.pad[0] = 1;
    node[38]->nn_param.conv2d.pad[1] = 1;
    node[38]->nn_param.conv2d.pad[2] = 1;
    node[38]->nn_param.conv2d.pad[3] = 1;
    node[38]->nn_param.conv2d.group = 1;
    node[38]->nn_param.conv2d.dilation[0] = 1;
    node[38]->nn_param.conv2d.dilation[1] = 1;
    node[38]->nn_param.conv2d.multiplier = 0;
    node[38]->vx_param.has_relu = FALSE;
    node[38]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[38]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[38]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu19_81
      var       - node[39]
      name      - relu19
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[39], VSI_NN_OP_LEAKY_RELU, 1, 1, 81);
    node[39]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv20_82
      var       - node[40]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[40], VSI_NN_OP_CONV_RELU, 2, 1, 82);
    node[40]->nn_param.conv2d.ksize[0] = 3;
    node[40]->nn_param.conv2d.ksize[1] = 3;
    node[40]->nn_param.conv2d.weights = 1024;
    node[40]->nn_param.conv2d.stride[0] = 1;
    node[40]->nn_param.conv2d.stride[1] = 1;
    node[40]->nn_param.conv2d.pad[0] = 1;
    node[40]->nn_param.conv2d.pad[1] = 1;
    node[40]->nn_param.conv2d.pad[2] = 1;
    node[40]->nn_param.conv2d.pad[3] = 1;
    node[40]->nn_param.conv2d.group = 1;
    node[40]->nn_param.conv2d.dilation[0] = 1;
    node[40]->nn_param.conv2d.dilation[1] = 1;
    node[40]->nn_param.conv2d.multiplier = 0;
    node[40]->vx_param.has_relu = FALSE;
    node[40]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[40]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[40]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu20_85
      var       - node[41]
      name      - relu20
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[41], VSI_NN_OP_LEAKY_RELU, 1, 1, 85);
    node[41]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - route_87
      var       - node[42]
      name      - route
      operation - concat
      in_shape  - [[13, 13, 2048, 1]]
                  [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 3072, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[42], VSI_NN_OP_CONCAT, 2, 1, 87);
    node[42]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - trans_conv21_88
      var       - node[43]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 3072, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[43], VSI_NN_OP_CONV_RELU, 2, 1, 88);
    node[43]->nn_param.conv2d.ksize[0] = 3;
    node[43]->nn_param.conv2d.ksize[1] = 3;
    node[43]->nn_param.conv2d.weights = 1024;
    node[43]->nn_param.conv2d.stride[0] = 1;
    node[43]->nn_param.conv2d.stride[1] = 1;
    node[43]->nn_param.conv2d.pad[0] = 1;
    node[43]->nn_param.conv2d.pad[1] = 1;
    node[43]->nn_param.conv2d.pad[2] = 1;
    node[43]->nn_param.conv2d.pad[3] = 1;
    node[43]->nn_param.conv2d.group = 1;
    node[43]->nn_param.conv2d.dilation[0] = 1;
    node[43]->nn_param.conv2d.dilation[1] = 1;
    node[43]->nn_param.conv2d.multiplier = 0;
    node[43]->vx_param.has_relu = FALSE;
    node[43]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[43]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[43]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - relu21_91
      var       - node[44]
      name      - relu21
      operation - leakyrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[44], VSI_NN_OP_LEAKY_RELU, 1, 1, 91);
    node[44]->nn_param.activation.leaky_ratio = 0.1;

    /*-----------------------------------------
      lid       - trans_conv22_92
      var       - node[45]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[13, 13, 1024, 1]]
      out_shape - [[13, 13, 30, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[45], VSI_NN_OP_CONV_RELU, 2, 1, 92);
    node[45]->nn_param.conv2d.ksize[0] = 1;
    node[45]->nn_param.conv2d.ksize[1] = 1;
    node[45]->nn_param.conv2d.weights = 30;
    node[45]->nn_param.conv2d.stride[0] = 1;
    node[45]->nn_param.conv2d.stride[1] = 1;
    node[45]->nn_param.conv2d.pad[0] = 0;
    node[45]->nn_param.conv2d.pad[1] = 0;
    node[45]->nn_param.conv2d.pad[2] = 0;
    node[45]->nn_param.conv2d.pad[3] = 0;
    node[45]->nn_param.conv2d.group = 1;
    node[45]->nn_param.conv2d.dilation[0] = 1;
    node[45]->nn_param.conv2d.dilation[1] = 1;
    node[45]->nn_param.conv2d.multiplier = 0;
    node[45]->vx_param.has_relu = FALSE;
    node[45]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[45]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[45]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @data_0:out0 */
    attr.size[0] = 416;
    attr.size[1] = 416;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_INT8);

    /* @output_93:out0 */
    attr.size[0] = 13;
    attr.size[1] = 13;
    attr.size[2] = 30;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_INT8);



    /* @conv1_1_pool1_5:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 2320;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_VDATA, 0, 2320);

    /* @conv2_6_pool2_10:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 20240;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_VDATA, 2320, 20240);

    /* @trans_conv3_11:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 76048;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_VDATA, 65232848, 76048);

    /* @trans_conv4_15:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 10000;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_VDATA, 65308896, 10000);

    /* @conv5_19_pool5_23:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 76048;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_VDATA, 22560, 76048);

    /* @trans_conv6_24:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 278608;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_VDATA, 65318896, 278608);

    /* @trans_conv7_28:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 35088;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_VDATA, 65597504, 35088);

    /* @conv8_32_pool8_36:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 298256;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_VDATA, 98608, 298256);

    /* @trans_conv9_37:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1090320;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_VDATA, 65632592, 1090320);

    /* @trans_conv10_41:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_VDATA, 396864, 134416);

    /* @trans_conv11_45:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1181328;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_VDATA, 531280, 1181328);

    /* @trans_conv12_49:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 134416;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_VDATA, 1712608, 134416);

    /* @trans_conv13_53:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 1185040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_VDATA, 1847024, 1185040);

    /* @trans_conv14_58:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4442576;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_VDATA, 3032064, 4442576);

    /* @trans_conv15_62:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 528656;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_VDATA, 7474640, 528656);

    /* @trans_conv16_66:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4728080;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_VDATA, 8003296, 4728080);

    /* @trans_conv17_70:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 529680;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_VDATA, 12731376, 529680);

    /* @trans_conv18_74:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 4725392;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_VDATA, 13261056, 4725392);

    /* @trans_conv19_78:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9446672;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_VDATA, 17986448, 9446672);

    /* @trans_conv20_82:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 9446672;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[19], attr, VSI_NN_TYPE_VDATA, 27433120, 9446672);

    /* @trans_conv21_88:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 28321040;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[20], attr, VSI_NN_TYPE_VDATA, 36879792, 28321040);

    /* @trans_conv22_92:vdata
    weight: #WEIGHT_SHAPE#
            #WEIGHT_QUANTIZE_PARAMS#
    bias:   #BIAS_SHAPE#
            #BIAS_QUANTIZE_PARAMS#
    */
    attr.size[0] = 32016;
    attr.dim_num = 1;
    NEW_CONST_TENSOR(const_tensor[21], attr, VSI_NN_TYPE_VDATA, 65200832, 32016);



    /* @conv1_1_pool1_5:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu1_4:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2_6_pool2_10:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu2_9:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv3_11:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu3_14:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv4_15:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu4_18:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_19_pool5_23:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu5_22:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv6_24:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu6_27:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv7_28:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu7_31:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv8_32_pool8_36:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu8_35:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv9_37:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu9_40:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv10_41:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu10_44:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv11_45:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu11_48:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[21]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv12_49:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[22]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu12_52:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[23]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv13_53:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[24]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu13_56:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[25]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool13_57:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[26]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @reorg_86:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[27]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv14_58:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[28]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu14_61:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[29]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv15_62:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[30]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu15_65:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[31]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv16_66:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[32]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu16_69:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[33]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv17_70:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[34]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu17_73:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[35]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv18_74:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[36]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu18_77:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[37]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv19_78:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[38]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu19_81:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[39]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv20_82:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[40]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu20_85:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[41]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @route_87:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[42]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_conv21_88:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[43]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @relu21_91:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[44]->output.tensors[0], attr, VSI_NN_TYPE_INT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[0]->input.tensors[0] = norm_tensor[0];
    node[45]->output.tensors[0] = norm_tensor[1];

    /* conv1_1_pool1_5 */
    node[0]->input.tensors[1] = const_tensor[0]; /* data_vdata */

    /* relu1_4 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];

    /* conv2_6_pool2_10 */
    node[2]->input.tensors[0] = node[1]->output.tensors[0];
    node[2]->input.tensors[1] = const_tensor[1]; /* data_vdata */

    /* relu2_9 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];

    /* trans_conv3_11 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[2]; /* data_vdata */

    /* relu3_14 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];

    /* trans_conv4_15 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];
    node[6]->input.tensors[1] = const_tensor[3]; /* data_vdata */

    /* relu4_18 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];

    /* conv5_19_pool5_23 */
    node[8]->input.tensors[0] = node[7]->output.tensors[0];
    node[8]->input.tensors[1] = const_tensor[4]; /* data_vdata */

    /* relu5_22 */
    node[9]->input.tensors[0] = node[8]->output.tensors[0];

    /* trans_conv6_24 */
    node[10]->input.tensors[0] = node[9]->output.tensors[0];
    node[10]->input.tensors[1] = const_tensor[5]; /* data_vdata */

    /* relu6_27 */
    node[11]->input.tensors[0] = node[10]->output.tensors[0];

    /* trans_conv7_28 */
    node[12]->input.tensors[0] = node[11]->output.tensors[0];
    node[12]->input.tensors[1] = const_tensor[6]; /* data_vdata */

    /* relu7_31 */
    node[13]->input.tensors[0] = node[12]->output.tensors[0];

    /* conv8_32_pool8_36 */
    node[14]->input.tensors[0] = node[13]->output.tensors[0];
    node[14]->input.tensors[1] = const_tensor[7]; /* data_vdata */

    /* relu8_35 */
    node[15]->input.tensors[0] = node[14]->output.tensors[0];

    /* trans_conv9_37 */
    node[16]->input.tensors[0] = node[15]->output.tensors[0];
    node[16]->input.tensors[1] = const_tensor[8]; /* data_vdata */

    /* relu9_40 */
    node[17]->input.tensors[0] = node[16]->output.tensors[0];

    /* trans_conv10_41 */
    node[18]->input.tensors[0] = node[17]->output.tensors[0];
    node[18]->input.tensors[1] = const_tensor[9]; /* data_vdata */

    /* relu10_44 */
    node[19]->input.tensors[0] = node[18]->output.tensors[0];

    /* trans_conv11_45 */
    node[20]->input.tensors[0] = node[19]->output.tensors[0];
    node[20]->input.tensors[1] = const_tensor[10]; /* data_vdata */

    /* relu11_48 */
    node[21]->input.tensors[0] = node[20]->output.tensors[0];

    /* trans_conv12_49 */
    node[22]->input.tensors[0] = node[21]->output.tensors[0];
    node[22]->input.tensors[1] = const_tensor[11]; /* data_vdata */

    /* relu12_52 */
    node[23]->input.tensors[0] = node[22]->output.tensors[0];

    /* trans_conv13_53 */
    node[24]->input.tensors[0] = node[23]->output.tensors[0];
    node[24]->input.tensors[1] = const_tensor[12]; /* data_vdata */

    /* relu13_56 */
    node[25]->input.tensors[0] = node[24]->output.tensors[0];

    /* pool13_57 */
    node[26]->input.tensors[0] = node[25]->output.tensors[0];

    /* reorg_86 */
    node[27]->input.tensors[0] = node[25]->output.tensors[0];

    /* trans_conv14_58 */
    node[28]->input.tensors[0] = node[26]->output.tensors[0];
    node[28]->input.tensors[1] = const_tensor[13]; /* data_vdata */

    /* relu14_61 */
    node[29]->input.tensors[0] = node[28]->output.tensors[0];

    /* trans_conv15_62 */
    node[30]->input.tensors[0] = node[29]->output.tensors[0];
    node[30]->input.tensors[1] = const_tensor[14]; /* data_vdata */

    /* relu15_65 */
    node[31]->input.tensors[0] = node[30]->output.tensors[0];

    /* trans_conv16_66 */
    node[32]->input.tensors[0] = node[31]->output.tensors[0];
    node[32]->input.tensors[1] = const_tensor[15]; /* data_vdata */

    /* relu16_69 */
    node[33]->input.tensors[0] = node[32]->output.tensors[0];

    /* trans_conv17_70 */
    node[34]->input.tensors[0] = node[33]->output.tensors[0];
    node[34]->input.tensors[1] = const_tensor[16]; /* data_vdata */

    /* relu17_73 */
    node[35]->input.tensors[0] = node[34]->output.tensors[0];

    /* trans_conv18_74 */
    node[36]->input.tensors[0] = node[35]->output.tensors[0];
    node[36]->input.tensors[1] = const_tensor[17]; /* data_vdata */

    /* relu18_77 */
    node[37]->input.tensors[0] = node[36]->output.tensors[0];

    /* trans_conv19_78 */
    node[38]->input.tensors[0] = node[37]->output.tensors[0];
    node[38]->input.tensors[1] = const_tensor[18]; /* data_vdata */

    /* relu19_81 */
    node[39]->input.tensors[0] = node[38]->output.tensors[0];

    /* trans_conv20_82 */
    node[40]->input.tensors[0] = node[39]->output.tensors[0];
    node[40]->input.tensors[1] = const_tensor[19]; /* data_vdata */

    /* relu20_85 */
    node[41]->input.tensors[0] = node[40]->output.tensors[0];

    /* route_87 */
    node[42]->input.tensors[0] = node[27]->output.tensors[0];
    node[42]->input.tensors[1] = node[41]->output.tensors[0];

    /* trans_conv21_88 */
    node[43]->input.tensors[0] = node[42]->output.tensors[0];
    node[43]->input.tensors[1] = const_tensor[20]; /* data_vdata */

    /* relu21_91 */
    node[44]->input.tensors[0] = node[43]->output.tensors[0];

    /* trans_conv22_92 */
    node[45]->input.tensors[0] = node[44]->output.tensors[0];
    node[45]->input.tensors[1] = const_tensor[21]; /* data_vdata */



    graph->input.tensors[0] = norm_tensor[0];
    graph->output.tensors[0] = norm_tensor[1];


    status = vsi_nn_SetupGraph( graph, FALSE );
    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vsi_nn_DumpGraphToJson(graph);
    vnn_ReleaseYoloFace7d( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateYoloFace7d() */

void vnn_ReleaseYoloFace7d
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseYoloFace7d() */

